id,title,included_tasks,description,flow,updated_at,tags
87,Scrape API in a Python task running in a Docker container and load the JSON document to a MongoDB collection,"[""io.kestra.core.tasks.flows.WorkingDirectory"", ""io.kestra.plugin.scripts.python.Script"", ""io.kestra.core.tasks.storages.LocalFiles"", ""io.kestra.plugin.mongodb.Load""]","This flow will scrape GitHub API in a Python task running in a Docker container, and will output the result to a JSON file. That JSON API payload is then loaded to a MongoDB collection.","id: jsonFromAPItoMongoDB
namespace: blueprint

tasks:
  - id: wdir
    type: io.kestra.core.tasks.flows.WorkingDirectory
    tasks:
      - id: generateJSON
        type: io.kestra.plugin.scripts.python.Script
        runner: DOCKER
        docker:
          image: ghcr.io/kestra-io/pydata:latest
        script: | 
          import requests
          import json
          from kestra import Kestra

          response = requests.get(""https://api.github.com"")
          data = response.json()

          with open(""output.json"", ""w"") as output_file:
              json.dump(data, output_file)
          
          Kestra.outputs({'data': data, 'status': response.status_code})

      - id: jsonFiles
        type: io.kestra.core.tasks.storages.LocalFiles
        outputs:
          - output.json

  - id: loadToMongoDB
    type: io.kestra.plugin.mongodb.Load
    connection:
      uri: mongodb://host.docker.internal:27017/
    database: local
    collection: github
    from: ""{{outputs.jsonFiles.uris['output.json']}}""
",2023-07-25 23:36:57.721,"Outputs,Python,Ingest,Local files"
116,"Scheduled data ingestion to AWS S3 data lake managed by Apache Iceberg, AWS Glue and Amazon Athena (Python script from Git)","[""io.kestra.plugin.aws.s3.List"", ""io.kestra.core.tasks.flows.If"", ""io.kestra.core.tasks.flows.WorkingDirectory"", ""io.kestra.plugin.git.Clone"", ""io.kestra.plugin.scripts.python.Commands"", ""io.kestra.plugin.aws.athena.Query"", ""io.kestra.plugin.aws.cli.AwsCLI"", ""io.kestra.core.models.triggers.types.Schedule""]","This scheduled flow checks for new files in a given S3 bucket every hour. If new files have been detected, the flow will:
- Download the raw CSV files from S3
- Read those CSV files as a dataframe, use Pandas to clean the data and ingest it into the S3 data lake managed by Iceberg and AWS Glue
- Move already processed files to the `archive/` folder in the same S3 bucket","id: ingestToDataLakeGit
namespace: blueprint

variables:
  bucket: kestraio
  prefix: inbox
  database: default

tasks:
  - id: listObjects
    type: io.kestra.plugin.aws.s3.List
    prefix: ""{{vars.prefix}}""
    accessKeyId: ""{{ secret('AWS_ACCESS_KEY_ID') }}""
    secretKeyId: ""{{ secret('AWS_SECRET_ACCESS_KEY') }}""
    region: ""{{ secret('AWS_DEFAULT_REGION') }}""
    bucket: ""{{vars.bucket}}""

  - id: check
    type: io.kestra.core.tasks.flows.If
    condition: ""{{outputs.listObjects.objects}}""
    then:
    - id: processNewObjects
      type: io.kestra.core.tasks.flows.WorkingDirectory
      tasks:
        - id: git
          type: io.kestra.plugin.git.Clone
          url: https://github.com/kestra-io/scripts
          branch: main

        - id: ingestToDataLake
          type: io.kestra.plugin.scripts.python.Commands
          warningOnStdErr: false
          env:
            AWS_ACCESS_KEY_ID: ""{{ secret('AWS_ACCESS_KEY_ID') }}""
            AWS_SECRET_ACCESS_KEY: ""{{ secret('AWS_SECRET_ACCESS_KEY') }}""
            AWS_DEFAULT_REGION: ""{{ secret('AWS_DEFAULT_REGION') }}""
          docker:
            image: ghcr.io/kestra-io/aws:latest
          commands:
            - python etl/aws_iceberg_fruit.py

    - id: mergeQuery
      type: io.kestra.plugin.aws.athena.Query
      accessKeyId: ""{{ secret('AWS_ACCESS_KEY_ID') }}""
      secretKeyId: ""{{ secret('AWS_SECRET_ACCESS_KEY') }}""
      region: ""{{ secret('AWS_DEFAULT_REGION') }}""
      database: ""{{vars.database}}""
      outputLocation: ""s3://{{vars.bucket}}/query_results/""
      query: |
        MERGE INTO fruits f USING raw_fruits r
            ON f.fruit = r.fruit
            WHEN MATCHED
                THEN UPDATE
                    SET id = r.id, berry = r.berry, update_timestamp = current_timestamp
            WHEN NOT MATCHED
                THEN INSERT (id, fruit, berry, update_timestamp)
                      VALUES(r.id, r.fruit, r.berry, current_timestamp);

    - id: optimize
      type: io.kestra.plugin.aws.athena.Query
      accessKeyId: ""{{ secret('AWS_ACCESS_KEY_ID') }}""
      secretKeyId: ""{{ secret('AWS_SECRET_ACCESS_KEY') }}""
      region: ""{{ secret('AWS_DEFAULT_REGION') }}""
      database: ""{{vars.database}}""
      outputLocation: ""s3://{{vars.bucket}}/query_results/""
      query: |
        OPTIMIZE fruits REWRITE DATA USING BIN_PACK;

    - id: moveToArchive
      type: io.kestra.plugin.aws.cli.AwsCLI
      accessKeyId: ""{{ secret('AWS_ACCESS_KEY_ID') }}""
      secretKeyId: ""{{ secret('AWS_SECRET_ACCESS_KEY') }}""
      region: ""{{ secret('AWS_DEFAULT_REGION') }}""
      commands:
        - aws s3 mv s3://{{vars.bucket}}/{{vars.prefix}}/ s3://{{vars.bucket}}/archive/{{vars.prefix}}/ --recursive      

triggers:
  - id: hourlySchedule
    type: io.kestra.core.models.triggers.types.Schedule
    disabled: true
    cron: ""@hourly""",2023-08-04 14:08:21.788,"Python,Git,Iceberg,AWS,CLI,If,S3"
71,Ingest data to and query data from ClickHouse,"[""io.kestra.plugin.jdbc.clickhouse.Query""]","This flow will create a database and a table in ClickHouse if they don't already exist. It will then insert some data into the table and finally query the table to show the data.

To test this flow, you can start ClickHouse in a Docker container:

```
docker run -d -p 8123:8123 -p 9000:9000 --name myclickhouse --ulimit nofile=262144:262144 clickhouse/clickhouse-server
```
","id: queryClickHouse
namespace: blueprint
  
tasks:
  - id: database
    type: io.kestra.plugin.jdbc.clickhouse.Query
    sql: CREATE DATABASE IF NOT EXISTS helloworld

  - id: table
    type: io.kestra.plugin.jdbc.clickhouse.Query
    sql: |
      CREATE TABLE IF NOT EXISTS helloworld.my_first_table
      (
          user_id String,
          message String,
          timestamp DateTime,
          metric Float32
      )
      ENGINE = MergeTree()
      PRIMARY KEY (user_id, timestamp)

  - id: insertData
    type: io.kestra.plugin.jdbc.clickhouse.Query
    sql: |
      INSERT INTO helloworld.my_first_table (user_id, message, timestamp, metric) VALUES
          (101, 'Hello, ClickHouse!',                                 now(),       -1.0    ),
          (102, 'Insert a lot of rows per batch',                     yesterday(), 1.41421 ),
          (102, 'Sort your data based on your commonly-used queries', today(),     2.718   ),
          (101, 'Granules are the smallest chunks of data read',      now() + 5,   3.14159 )

  - id: queryAndStoreAsJSON
    type: io.kestra.plugin.jdbc.clickhouse.Query
    sql: SELECT user_id, message FROM helloworld.my_first_table
    store: true

taskDefaults:
  - type: io.kestra.plugin.jdbc.clickhouse.Query
    values:
      url: jdbc:clickhouse://host.docker.internal:8123/
      username: default
",2023-07-26 00:35:37.284,"Ingest,SQL"
68,Extract JSON data from an API and load it as a document to MongoDB,"[""io.kestra.plugin.fs.http.Request"", ""io.kestra.plugin.mongodb.InsertOne""]","This flow will load a pokemon from the [PokeAPI](https://github.com/PokeAPI/pokeapi) into a MongoDB database.
The Pokémon name can be provided as an input at runtime.","id: loadPokemon
namespace: blueprint

inputs:
  - name: pokemon
    type: STRING
    defaults: psyduck


tasks:
  - id: pokemon
    type: io.kestra.plugin.fs.http.Request
    uri: https://pokeapi.co/api/v2/pokemon/{{inputs.pokemon}}
    method: GET

  - id: load
    type: io.kestra.plugin.mongodb.InsertOne
    connection:
      uri: ""mongodb://host.docker.internal:27017/""
    database: ""local""
    collection: ""pokemon""
    document: ""{{outputs.pokemon.body}}""
",2023-07-26 00:36:10.767,Ingest
51,Git workflow for dbt with Postgres,"[""io.kestra.core.tasks.flows.WorkingDirectory"", ""io.kestra.plugin.git.Clone"", ""io.kestra.plugin.dbt.cli.Build""]","This flow:
- clones a dbt Git repository from GitHub
- pulls a public container image with the required dependencies 
- runs dbt CLI commands in a Docker container.

To run a flow from this blueprint, you can simply copy-paste the contents of your `~/.dbt/profiles.yml` into the `profiles` section and you're good to go!","id: dbtGitDockerPostgres
namespace: blueprint

tasks:
  - id: dbt
    type: io.kestra.core.tasks.flows.WorkingDirectory
    tasks:
      - id: cloneRepository
        type: io.kestra.plugin.git.Clone
        url: https://github.com/kestra-io/dbt-demo
        branch: main

      - id: dbtCore
        type: io.kestra.plugin.dbt.cli.DbtCLI
        docker:
          image: ghcr.io/kestra-io/dbt-postgres:latest
        profiles: |
          jaffle_shop:
            outputs:
              dev:
                type: postgres
                host: myhostname.us-east-1.rds.amazonaws.com
                user: ""{{secret('POSTGRES_USER')}}""
                password: ""{{secret('POSTGRES_PASSWORD')}}""
                port: 5432
                dbname: postgres
                schema: public
                threads: 8
                connect_timeout: 10
            target: dev
        commands:
          - dbt deps
          - dbt build
",2023-08-11 15:11:12.436,"Postgres,dbt,Git"
146,Ingest Pipedrive CRM data to BigQuery using dlt and schedule it to run every hour,"[""io.kestra.plugin.scripts.python.Script"", ""io.kestra.core.models.triggers.types.Schedule""]","This flow demonstrates how to extract data from Pipedrive CRM and load it into BigQuery using dlt. The entire workflow logic is contained in a single Python script that uses the dlt Python library to ingest the data to BigQuery. 

The credentials to access the Pipedrive API and BigQuery are stored using [Kestra secrets](https://kestra.io/docs/developer-guide/secrets).","id: dlt_pipedrive_to_bigquery
namespace: blueprint

tasks:
  - id: dlt_pipeline
    type: io.kestra.plugin.scripts.python.Script
    docker:
      image: python:3.11
    beforeCommands:
      - pip install dlt[bigquery]
      - dlt --non-interactive init pipedrive bigquery
    warningOnStdErr: false
    env:
      DESTINATION__BIGQUERY__CREDENTIALS__PROJECT_ID: ""{{ secret('BIGQUERY_PROJECT_ID') }}""
      DESTINATION__BIGQUERY__CREDENTIALS__PRIVATE_KEY: ""{{ secret('BIGQUERY_PRIVATE_KEY') }}""
      DESTINATION__BIGQUERY__CREDENTIALS__CLIENT_EMAIL: ""{{ secret('BIGQUERY_CLIENT_EMAIL') }}""
      SOURCES__PIPEDRIVE__CREDENTIALS__PIPEDRIVE_API_KEY: ""{{ secret('PIPEDRIVE_API_KEY') }}""
    script: |
      import dlt
      from pipedrive import pipedrive_source

      pipeline = dlt.pipeline(
          pipeline_name=""pipedrive_pipeline"",
          destination=""biquery"",
          dataset_name=""pipedrive"",
      )

      load_info = pipeline.run(pipedrive_source())

triggers:
  - id: hourly
    type: io.kestra.core.models.triggers.types.Schedule
    cron: ""@hourly""",2023-10-10 12:19:32.042,"Python,Ingest"
80,Trigger a flow when other flows finish successfully,"[""io.kestra.core.tasks.debugs.Return"", ""io.kestra.core.models.triggers.types.Flow"", ""io.kestra.core.models.conditions.types.MultipleCondition"", ""io.kestra.core.models.conditions.types.ExecutionStatusCondition"", ""io.kestra.core.models.conditions.types.ExecutionFlowCondition""]",Trigger a flow as soon as two other flows finish successfully. This flow will be triggered when `flow_a` and `flow_b` both successfully finish their execution within the time window of 24 hours (`window: P1D` = 1 day) from now (`windowAdvance: P0D` = zero days).,"id: flow_downstream
namespace: blueprint

tasks:
  - id: task_c
    type: io.kestra.core.tasks.debugs.Return
    format: ""{{task.id}}""

triggers:
  - id: multiple-listen-flow
    type: io.kestra.core.models.triggers.types.Flow
    conditions:
      - type: io.kestra.core.models.conditions.types.ExecutionStatusCondition
        in:
          - SUCCESS
      - id: multiple
        type: io.kestra.core.models.conditions.types.MultipleCondition
        window: P1D
        windowAdvance: P0D
        conditions:
          flow_a:
            type: io.kestra.core.models.conditions.types.ExecutionFlowCondition
            namespace: blueprint
            flowId: flow_a
          flow_b:
            type: io.kestra.core.models.conditions.types.ExecutionFlowCondition
            namespace: blueprint
            flowId: flow_b
",2023-07-26 19:01:11.535,Trigger
70,Load data from Postgres to BigQuery using Singer,"[""io.kestra.plugin.singer.taps.PipelinewisePostgres"", ""io.kestra.plugin.singer.targets.AdswerveBigQuery""]","This flow will extract raw data from a Postgres database and load it into BigQuery using the Singer protocol. 
The credentials to both Postgres and BigQuery can be provided as secrets.","id: postgresToBigQuery
namespace: blueprint
  
tasks:
  - id: extract
    type: io.kestra.plugin.singer.taps.PipelinewisePostgres
    docker:
      image: python:3.8
    host: host.docker.internal
    port: 5432
    dbName: postgres
    username: postgres
    password: ""{{ secret('DB_PASSWORD') }}""
    streamsConfigurations:
      - replicationMethod: FULL_TABLE # FULL_TABLE, INCREMENTAL, LOG_BASED
        selected: true

  - id: load
    type: io.kestra.plugin.singer.targets.AdswerveBigQuery
    addMetadataColumns: true
    datasetId: singer
    docker:
      image: python:3.8
    from: ""{{ outputs.extract.raw }}""
    location: US
    projectId: yourProjectName
    serviceAccount: ""{{ secret('GCP_CREDS') }}""
",2023-10-10 12:37:29.947,"Ingest,BigQuery,Postgres"
52,Git workflow for dbt with Snowflake,"[""io.kestra.core.tasks.flows.WorkingDirectory"", ""io.kestra.plugin.git.Clone"", ""io.kestra.plugin.dbt.cli.Build""]","This flow:
- clones a dbt Git repository from GitHub, 
- pulls a public container image with the required dependencies 
- runs dbt CLI commands within a container.

This flow assumes that your Snowflake credentials are stored as secrets.

To run a flow from this blueprint, you can simply copy-paste the contents of your `~/.dbt/profiles.yml` into the `profiles` section and you're good to go!","id: dbtGitDockerSnowflake
namespace: blueprint

tasks:
  - id: dbt
    type: io.kestra.core.tasks.flows.WorkingDirectory
    tasks:
    - id: cloneRepository
      type: io.kestra.plugin.git.Clone
      url: https://github.com/kestra-io/dbt-demo
      branch: main

    - id: dbtCore
      type: io.kestra.plugin.dbt.cli.DbtCLI
      docker:
        image: ghcr.io/kestra-io/dbt-snowflake:latest
      profiles: |
        jaffle_shop:
          outputs:
            dev:
              type: snowflake
              account: ""{{ secret('SNOWFLAKE_ACCOUNT') }}""
              user: ""{{ secret('SNOWFLAKE_USER') }}""
              password: ""{{ secret('SNOWFLAKE_PASSWORD') }}""
              role: ""{{ secret('SNOWFLAKE_ROLE') }}""
              database: ""{{ secret('SNOWFLAKE_DATABASE') }}""
              warehouse: COMPUTE_WH
              schema: public
              threads: 4
              query_tag: dbt
              client_session_keep_alive: False
              connect_timeout: 10
          target: dev
      commands:
        - dbt deps
        - dbt build
",2023-08-11 15:07:37.428,"Snowflake,Git,dbt"
190,Send an alert to Zenduty when a flow fails,"[""io.kestra.plugin.scripts.shell.Commands"", ""io.kestra.plugin.notifications.zenduty.ZendutyAlert""]","This flow sends an alert to Zenduty when a flow fails. The only required input is an integration key string value. Check the [Zenduty documentation](https://docs.zenduty.com/docs/api) to learn how to create an API integration and generate the key. The API integration will send an API call that follows the format: `curl -X POST https://www.zenduty.com/api/events/[integration-key]/ -H 'Content-Type: application/json' -d '{""alert_type"":""critical"", ""message"":""Some message"", ""summary"":""some summary"", ""entity_id"":""some_entity_id""}'`. The `message` and `summary` parameters are required. The `alert_type` parameter is the severity of the issue, including `info`, `warning`, `error`, or `critical`.","id: unreliable_flow
namespace: prod

tasks:
  - id: fail
    type: io.kestra.plugin.scripts.shell.Commands
    commands:
      - exit 1

errors:
  - id: alert_on_failure
    type: io.kestra.plugin.notifications.zenduty.ZendutyAlert
    url: ""https://www.zenduty.com/api/events/{{ secret('ZENDUTY_INTEGRATION_KEY') }}/""
    payload: |
      {
        ""alert_type"": ""info"",
        ""message"": ""This is info alert"",
        ""summary"": ""This is the incident summary"",
        ""suppressed"": false,
        ""entity_id"": 12345,
        ""payload"": {
          ""status"": ""ACME Payments are failing"",
          ""severity"": ""1"",
          ""project"": ""kubeprod""
        },
        ""urls"": [
          {
            ""link_url"": ""https://www.example.com/alerts/12345/"",
            ""link_text"": ""Alert URL""
          }
        ]
      }
",2023-10-26 12:45:26.485,Notifications
132,Build a Docker image and push it to GitHub Container Registry (ghcr.io),"[""io.kestra.plugin.docker.Build""]","This flow will build a Docker image and push it to a remote container registry.

1. The `dockerfile` parameter is a multiline string that contains the Dockerfile content. However, it can also be a path to a file.
2. The `tags` parameter is a list of tags of the image to build. Make sure to replace the code below to match your GitHub username or organization.
3. The `push` parameter is a boolean that indicates whether to push the image to GitHub Container Registry.
4. Finally, make sure to securely store your registry credentials as secrets or environment variables.
","id: build_github_container_image
namespace: blueprint

tasks:
  - id: build
    type: io.kestra.plugin.docker.Build
    dockerfile: |
      FROM python:3.10
      RUN pip install --upgrade pip
      RUN pip install --no-cache-dir kestra requests ""polars[all]""
    tags: 
      - ghcr.io/kestra/polars:latest
    push: true
    credentials:
      username: kestra
      password: ""{{ secret('GITHUB_ACCESS_TOKEN') }}""
",2023-09-22 11:52:40.139,"Docker,Git"
84,Run R script in a Docker container and output downloadable artifacts,"[""io.kestra.core.tasks.flows.WorkingDirectory"", ""io.kestra.plugin.scripts.r.Script"", ""io.kestra.core.tasks.storages.LocalFiles""]","This flow runs R script in a working directory. It loads data, analyzes it using the `dplyr` package. Finally, it stores the result as  both CSV and Parquet files, which both can be downloaded from the Execution Outputs tab.

The R script is executed in a Docker container, providing isolated environment for the task and avoiding any dependency conflicts. All dependencies for the task are baked into a publicly available Docker image, maintained by Kestra: `ghcr.io/kestra-io/rdata:latest`. You can replace that image with your own, or install custom dependencies at runtime using the `beforeCommands` property, for example:

```
beforeCommands:
    - Rscript -e ""install.packages(c('httr', 'RSQLite'))"" > /dev/null 2>&1
```","id: rScript
namespace: blueprint

tasks:
  - id: directory
    type: io.kestra.core.tasks.flows.WorkingDirectory
    tasks:
      - id: RScript
        type: io.kestra.plugin.scripts.r.Script
        warningOnStdErr: false
        runner: DOCKER
        docker:
          image: ghcr.io/kestra-io/rdata:latest
        script: |
          library(dplyr)
          library(arrow)

          data(women)

          women <- women %>%
            mutate(height_cm = height * 2.54,
                  weight_kg = weight * 0.453592)

          print(head(women, 2))

          women_clean <- na.omit(women)
          df <- women_clean %>%
            summarise(mean_height_cm = mean(height_cm), 
                      median_height_cm = median(height_cm), 
                      mean_weight_kg = mean(weight_kg),
                      median_weight_kg = median(weight_kg))
          print(df)
          write_parquet(df, ""women.parquet"")
          write_csv_arrow(df, ""women.csv"")
      
      - id: outputs
        type: io.kestra.core.tasks.storages.LocalFiles
        outputs:
          - women.parquet
          - women.csv",2023-07-25 23:35:39.245,"R,Local files"
192,Query data from Dremio and process it in Polars with a Python script,"[""io.kestra.plugin.jdbc.dremio.Query"", ""io.kestra.plugin.scripts.python.Script""]","This flow queries data from a Dremio lakehouse and passes the query results to a downstream Python task that further transforms fetched data using [Polars](https://www.pola.rs). The only required inputs are the project ID and a Dremio token.

The project ID can be found in your Dremio URL. For example, if your Dremio URL is `https://app.dremio.cloud/sonar/ead79cc0-9e93-4d50-b364-77639a56d4a6`, then your project ID is the last string `ead79cc0-9e93-4d50-b364-77639a56d4a6`.

To create a Dremio token, go to your Dremio account settings and then to the section ""Personal Access Token"". From here, you can create the token, copy it and store it as a kestra secret. For more detailed description of how to create your token, visit the [following Dremio documentation](https://docs.dremio.com/cloud/security/authentication/personal-access-token#creating-a-pat). It's recommended to store the token as a [kestra secret](https://kestra.io/docs/developer-guide/secrets) to avoid exposing it directly in your flow.

If you are new to Dremio, the easiest way to familiarize yourself with the platform is to create a [Dremio Test Drive](https://docs.dremio.com/cloud/test-drive/) account, which provides you with a Dremio instance and sample datasets to query. You can reproduce this flow without any changes using the test drive account.","id: dremio_sql_python
namespace: blueprint

variables:
  project_id: ead79cc0-9e93-4d50-b364-77639a56d4a6

tasks:
  - id: query
    type: io.kestra.plugin.jdbc.dremio.Query
    disabled: false
    url: ""jdbc:dremio:direct=sql.dremio.cloud:443;ssl=true;PROJECT_ID={{vars.project_id}};schema=postgres.public""
    username: $token
    password: ""{{ secret('DREMIO_TOKEN') }}""
    sql: SELECT first_name, last_name, hire_date, salary FROM postgres.public.employees LIMIT 100;
    store: true

  - id: python
    type: io.kestra.plugin.scripts.python.Script
    warningOnStdErr: false
    docker:
      image: ghcr.io/kestra-io/polars:latest
    script: |
      import polars as pl
      import amazon.ion.simpleion as ion
      from amazon.ion.simple_types import IonPyDecimal, IonPyNull


      def convert_ion_types(value):
        if isinstance(value, IonPyNull):
            return None
        elif isinstance(value, IonPyDecimal):
            return float(value)
        else:
            return value

      uri = ""{{outputs.query.uri}}""
      with open(uri, 'rb') as f:
          ion_content = f.read()

      ion_data = ion.loads(ion_content, single_value=False)
      list_of_dicts = [dict(record) for record in ion_data]
      list_of_dicts = [
        {k: convert_ion_types(v) for k, v in record.items()} for record in list_of_dicts
      ]

      polars_df = pl.DataFrame(list_of_dicts)
      print(polars_df.glimpse())
",2023-10-27 13:56:29.691,"Python,Outputs,SQL"
92,"Run a SQL query on Databricks, output the result to a CSV file and read that CSV file in a Python script with Pandas","[""io.kestra.plugin.databricks.sql.Query"", ""io.kestra.plugin.serdes.csv.CsvWriter"", ""io.kestra.plugin.scripts.python.Script""]","This flow demonstrates how to run a SQL query on a Databricks lakehouse. The results are stored in a CSV file. Then, the flow uses Pandas to read the CSV file and further process the data.","id: databricksSql
namespace: blueprint
description: |
    This flow demonstrates how to run a SQL query on a Databricks lakehouse.

tasks:
  - id: sqlQuery
    type: io.kestra.plugin.databricks.sql.Query
    accessToken: ""{{ secret('DATABRICKS_TOKEN') }}""
    host: ""{{ secret('DATABRICKS_HOST') }}""
    httpPath: sql/protocolv1/o/3497567377305430/0713-130754-9oa8zceh
    sql: SELECT * FROM samples.nyctaxi.trips LIMIT 100;

  - id: csv
    type: io.kestra.plugin.serdes.csv.CsvWriter
    from: ""{{ outputs.sqlQuery.uri }}""

  - id: pandas
    type: io.kestra.plugin.scripts.python.Script
    beforeCommands:
      - pip install kestra pandas > /dev/null
    script: |
      import pandas as pd

      df = pd.read_csv(""{{outputs.csv.uri}}"")
      df.head()",2023-07-27 11:36:56.14,"pip,Python,Databricks"
101,Pull a container image from Amazon ECR registry and run a Python script,"[""io.kestra.plugin.aws.cli.AwsCLI"", ""io.kestra.plugin.scripts.python.Commands""]","This flow will retrieve an authorization token to authenticate with the Amazon ECR. Then, it will pull the specified image and will run a Python script (or whichever command you wish) in a Docker container. ","id: pythonAwsECR
namespace: blueprint

tasks:
  - id: ecr
    type: io.kestra.plugin.aws.ecr.GetAuthToken
    accessKeyId: ""{{ secret('AWS_ACCESS_KEY_ID') }}""
    secretKeyId: ""{{ secret('AWS_SECRET_ACCESS_KEY') }}""
    region: eu-central-1

  - id: py
    type: io.kestra.plugin.scripts.python.Commands
    docker:
      image: 338306982838.dkr.ecr.eu-central-1.amazonaws.com/data-infastructure:latest
      config: |
        {
          ""auths"": {
            ""338306982838.dkr.ecr.eu-central-1.amazonaws.com"": {
                ""username"": ""AWS"",
                ""password"": ""{ {outputs.ecr.token }}""
              }
          }
        }
    commands:
      - python --version # e.g. python yourscript.py
",2023-11-03 00:30:26.185,"AWS,CLI,Python,Docker"
69,Find documents in MongoDB based on a filter condition,"[""io.kestra.plugin.mongodb.Find""]",This flow will filter a MongoDB collection to find Pokemon with a base experience greater than 100 ,"id: filterMongoDB
namespace: blueprint

tasks:
  - id: filter
    type: io.kestra.plugin.mongodb.Find
    connection:
      uri: ""mongodb://host.docker.internal:27017/""
    database: ""local""
    collection: ""pokemon""
    store: true
    filter:
      base_experience:
        $gt: 100",2023-07-26 00:35:58.413,
114,"Scheduled data ingestion to AWS S3 data lake managed by Apache Iceberg, AWS Glue and Amazon Athena (inline Python script)","[""io.kestra.plugin.aws.s3.List"", ""io.kestra.core.tasks.flows.If"", ""io.kestra.plugin.scripts.python.Script"", ""io.kestra.plugin.aws.athena.Query"", ""io.kestra.plugin.aws.cli.AwsCLI"", ""io.kestra.core.models.triggers.types.Schedule""]","This scheduled flow checks for new files in a given S3 bucket every hour. If new files have been detected, the flow will:
- Download the raw CSV files from S3
- Read those CSV files as a dataframe, use Pandas to clean the data and ingest it into the S3 data lake managed by Iceberg and AWS Glue
- Move already processed files to the `archive/` folder in the same S3 bucket","id: ingestToDataLakeInlinePython
namespace: blueprint

variables:
  bucket: kestraio
  prefix: inbox
  database: default

tasks:
  - id: listObjects
    type: io.kestra.plugin.aws.s3.List
    prefix: ""{{vars.prefix}}""
    accessKeyId: ""{{ secret('AWS_ACCESS_KEY_ID') }}""
    secretKeyId: ""{{ secret('AWS_SECRET_ACCESS_KEY') }}""
    region: ""{{ secret('AWS_DEFAULT_REGION') }}""
    bucket: ""{{vars.bucket}}""
  
  - id: check
    type: io.kestra.core.tasks.flows.If
    condition: ""{{outputs.listObjects.objects}}""
    then:
      - id: ingestToDataLake
        type: io.kestra.plugin.scripts.python.Script
        warningOnStdErr: false
        env:
          AWS_ACCESS_KEY_ID: ""{{ secret('AWS_ACCESS_KEY_ID') }}""
          AWS_SECRET_ACCESS_KEY: ""{{ secret('AWS_SECRET_ACCESS_KEY') }}""
          AWS_DEFAULT_REGION: ""{{ secret('AWS_DEFAULT_REGION') }}""
        docker:
          image: ghcr.io/kestra-io/aws:latest
        script: |
          import awswrangler as wr
          from kestra import Kestra

          # Iceberg table
          BUCKET_NAME = ""{{vars.bucket}}""
          DATABASE = ""{{vars.database}}""
          TABLE = ""raw_fruits""

          # Iceberg table's location
          S3_PATH = f""s3://{BUCKET_NAME}/{TABLE}""
          S3_PATH_TMP = f""{S3_PATH}_tmp""

          # File to ingest
          PREFIX = ""{{vars.prefix}}""
          INGEST_S3_KEY_PATH = f""s3://{BUCKET_NAME}/{PREFIX}/""

          df = wr.s3.read_csv(INGEST_S3_KEY_PATH)
          nr_rows = df.id.nunique()
          print(f""Ingesting {nr_rows} rows"")
          Kestra.counter(""nr_rows"", nr_rows, {""table"": TABLE})

          df = df[~df[""fruit""].isin([""Blueberry"", ""Banana""])]
          df = df.drop_duplicates(subset=[""fruit""], ignore_index=True, keep=""first"")

          wr.catalog.delete_table_if_exists(database=DATABASE, table=TABLE)

          wr.athena.to_iceberg(
              df=df,
              database=DATABASE,
              table=TABLE,
              table_location=S3_PATH,
              temp_path=S3_PATH_TMP,
              partition_cols=[""berry""],
              keep_files=False,
          )
          print(f""New data successfully ingested into {S3_PATH}"")

      - id: mergeQuery
        type: io.kestra.plugin.aws.athena.Query
        accessKeyId: ""{{ secret('AWS_ACCESS_KEY_ID') }}""
        secretKeyId: ""{{ secret('AWS_SECRET_ACCESS_KEY') }}""
        region: ""{{ secret('AWS_DEFAULT_REGION') }}""
        database: ""{{vars.database}}""
        outputLocation: ""s3://{{vars.bucket}}/query_results/""
        query: |
          MERGE INTO fruits f USING raw_fruits r
              ON f.fruit = r.fruit
              WHEN MATCHED
                  THEN UPDATE
                      SET id = r.id, berry = r.berry, update_timestamp = current_timestamp
              WHEN NOT MATCHED
                  THEN INSERT (id, fruit, berry, update_timestamp)
                        VALUES(r.id, r.fruit, r.berry, current_timestamp);

      - id: optimize
        type: io.kestra.plugin.aws.athena.Query
        accessKeyId: ""{{ secret('AWS_ACCESS_KEY_ID') }}""
        secretKeyId: ""{{ secret('AWS_SECRET_ACCESS_KEY') }}""
        region: ""{{ secret('AWS_DEFAULT_REGION') }}""
        database: ""{{vars.database}}""
        outputLocation: ""s3://{{vars.bucket}}/query_results/""
        query: |
          OPTIMIZE fruits REWRITE DATA USING BIN_PACK;

      - id: moveToArchive
        type: io.kestra.plugin.aws.cli.AwsCLI
        accessKeyId: ""{{ secret('AWS_ACCESS_KEY_ID') }}""
        secretKeyId: ""{{ secret('AWS_SECRET_ACCESS_KEY') }}""
        region: ""{{ secret('AWS_DEFAULT_REGION') }}""
        commands:
          - aws s3 mv s3://{{vars.bucket}}/{{vars.prefix}}/ s3://{{vars.bucket}}/archive/{{vars.prefix}}/ --recursive      

triggers:
  - id: hourlySchedule
    type: io.kestra.core.models.triggers.types.Schedule
    cron: ""@hourly""
    disabled: true",2023-08-04 14:08:10.199,"AWS,Iceberg,CLI,S3,Python,If"
115,"Event-driven data ingestion to AWS S3 data lake managed by Apache Iceberg, AWS Glue and Amazon Athena","[""io.kestra.core.tasks.flows.WorkingDirectory"", ""io.kestra.plugin.git.Clone"", ""io.kestra.plugin.scripts.python.Commands"", ""io.kestra.plugin.aws.athena.Query"", ""io.kestra.plugin.aws.s3.Trigger""]","This workflow ingests data to an S3 data lake using a Python script. 

This script is stored in a public GitHub repository so you can directly use this workflow as long as you adjust your AWS credentials, S3 bucket name and the Amazon Athena table name. The script takes the detected S3 object key from the S3 event trigger as input argument and ingests it into the Iceberg table. The script runs in a Docker container -- the image provided here is public, feel free to use it. Alternatively, you can install the required `pip` dependencies using the `beforeCommands` property.","id: ingestToDataLakeEventDriven
namespace: blueprint

variables:
  sourcePrefix: inbox
  destinationPrefix: archive
  database: default
  bucket: kestraio

tasks:
  - id: wdir
    type: io.kestra.core.tasks.flows.WorkingDirectory
    tasks:
      - id: git
        type: io.kestra.plugin.git.Clone
        url: https://github.com/kestra-io/scripts

      - id: etl
        type: io.kestra.plugin.scripts.python.Commands
        warningOnStdErr: false
        docker:
          image: ghcr.io/kestra-io/aws:latest
        env:
          AWS_ACCESS_KEY_ID: ""{{ secret('AWS_ACCESS_KEY_ID') }}""
          AWS_SECRET_ACCESS_KEY: ""{{ secret('AWS_SECRET_ACCESS_KEY') }}""
          AWS_DEFAULT_REGION: ""{{ secret('AWS_DEFAULT_REGION') }}""
        commands:
          - python etl/aws_iceberg_fruit.py {{vars.destinationPrefix}}/{{ trigger.objects | jq('.[].key') | first }}

  - id: mergeQuery
    type: io.kestra.plugin.aws.athena.Query
    accessKeyId: ""{{ secret('AWS_ACCESS_KEY_ID') }}""
    secretKeyId: ""{{ secret('AWS_SECRET_ACCESS_KEY') }}""
    region: ""{{ secret('AWS_DEFAULT_REGION') }}""
    database: ""{{vars.database}}""
    outputLocation: ""s3://{{vars.bucket}}/query_results/""
    query: |
      MERGE INTO fruits f USING raw_fruits r
          ON f.fruit = r.fruit
          WHEN MATCHED
              THEN UPDATE
                  SET id = r.id, berry = r.berry, update_timestamp = current_timestamp
          WHEN NOT MATCHED
              THEN INSERT (id, fruit, berry, update_timestamp)
                    VALUES(r.id, r.fruit, r.berry, current_timestamp);

  - id: optimize
    type: io.kestra.plugin.aws.athena.Query
    accessKeyId: ""{{ secret('AWS_ACCESS_KEY_ID') }}""
    secretKeyId: ""{{ secret('AWS_SECRET_ACCESS_KEY') }}""
    region: ""{{ secret('AWS_DEFAULT_REGION') }}""
    database: ""{{vars.database}}""
    outputLocation: ""s3://{{vars.bucket}}/query_results/""
    query: |
      OPTIMIZE fruits REWRITE DATA USING BIN_PACK;       

triggers:
  - id: waitForNewS3objects
    type: io.kestra.plugin.aws.s3.Trigger
    bucket: kestraio
    interval: PT1S
    maxKeys: 1
    filter: FILES
    action: MOVE
    prefix: ""{{vars.sourcePrefix}}""  # e.g. s3://kestraio/inbox/fruit_1.csv
    moveTo:
      key: ""{{vars.destinationPrefix}}/{{vars.sourcePrefix}}"" # e.g. s3://kestraio/archive/inbox/fruit_1.csv
    region: ""{{ secret('AWS_DEFAULT_REGION') }}""
    accessKeyId: ""{{ secret('AWS_ACCESS_KEY_ID') }}""
    secretKeyId: ""{{ secret('AWS_SECRET_ACCESS_KEY') }}""",2023-08-04 14:08:46.97,"Iceberg,AWS"
60,Upload file to Google Drive,"[""io.kestra.plugin.fs.http.Download"", ""io.kestra.plugin.googleworkspace.drive.Upload""]","In this blueprint we upload a file to Google Drive.

> Note: the `parents` property here refers to an existing Google Drive directory. To be able to use Google Drive you will have to enable the API in your Google Cloud Platform project and share the folder with your service account email address.","id: upload_google_drive
namespace: blueprint

tasks:

  - id: download
    type: io.kestra.plugin.fs.http.Download
    uri: https://raw.githubusercontent.com/kestra-io/datasets/main/csv/orders.csv

  - id: upload
    type: io.kestra.plugin.googleworkspace.drive.Upload
    from: ""{{ outputs.download.uri }}""
    parents:
    - ""15OENbAxvonlASDkYyfGBftV2c0fDTmnB""
    name: ""Orders""
    contentType: ""text/csv""
    mimeType: ""application/vnd.google-apps.spreadsheet""",2023-07-26 00:19:23.16,GCP
97,Automate tasks on AWS using the AWS CLI,"[""io.kestra.plugin.aws.cli.AwsCLI""]","This flow demonstrates how you can use the AWS CLI plugin to automate various tasks on AWS. 

The task `aws` below runs two commands. They both list ECS clusters in the region `us-east-1` using the `aws ecs list-clusters` command. The output is a JSON array of cluster ARNs. 
1. The first task uses the `--query` parameter to filter the output to only show the cluster ARNs. 
2. The second task stores the output to a file called `output.json`, making it available for further processing or download.
","id: awsCLIlistECSclusters
namespace: dev

tasks:
  - id: aws
    type: io.kestra.plugin.aws.cli.AwsCLI
    accessKeyId: ""{{ secret('AWS_ACCESS_KEY_ID') }}""
    secretKeyId: ""{{ secret('AWS_SECRET_ACCESS_KEY') }}""
    region: ""us-east-1""
    commands:
      - aws ecs list-clusters --query 'clusterArns[*]'
      - aws ecs list-clusters > {{outputDir}}/output.json
",2023-07-20 12:36:35.518,AWS
112,Backfill a flow for the Yellow Taxi Trip dataset,"[""io.kestra.core.tasks.log.Log"", ""io.kestra.core.models.triggers.types.Schedule""]","To backfill a flow, add a `start` date from which Kestra should backfill the past runs before running the regular schedule.

The expression `{{ trigger.date ?? execution.startDate | date(""yyyy-MM"") }}` will always give you the execution date regardless of whether you triggered the flow ad-hoc or whether it ran based on a schedule. The date function formats the date according to the provided date template.","id: taxi_trip_data
namespace: blueprint

tasks:
  - id: log
    type: io.kestra.core.tasks.log.Log
    message: running backfill for file https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{{ trigger.date ?? execution.startDate | date(""yyyy-MM"") }}.parquet

triggers:
  - id: schedule
    type: io.kestra.core.models.triggers.types.Schedule
    cron: ""0 8 1 * *"" # at 8 AM on every first day of the month
    timezone: US/Eastern
    backfill:
      start: 2023-01-01T00:00:00Z
",2023-10-12 13:25:24.3,"Schedule,Backfills,Variables"
108,"Extract data, mask sensitive columns using DuckDB and load it to BigQuery","[""io.kestra.plugin.fs.http.Download"", ""io.kestra.plugin.jdbc.duckdb.Query"", ""io.kestra.plugin.gcp.bigquery.Load""]","This flow has three tasks: `extract`, `transform` and `load`.

1. The `extract` task here is a simple HTTP Download task, but you can replace it with any task or custom script that extracts data.
2. The `transform` task reads the output of the `extract` task and transforms it using DuckDB. Specifically, it hashes sensitive values so that they are masked before being loaded to the final destination.
3. The `load` task loads that extracted and transformed data to BigQuery.

If you use [MotherDuck](https://motherduck.com/), use Kestra [secret](https://kestra.io/docs/developer-guide/secrets) to store the [MotherDuck service token](https://motherduck.com/docs/authenticating-to-motherduck). Then, add the `url` property to point the task to your MotherDuck database.

```yaml
  - id: transform
    type: io.kestra.plugin.jdbc.duckdb.Query
    url: ""jdbc:duckdb:md:my_db?motherduck_token={{ secret('MOTHERDUCK_TOKEN') }}""
```","id: sensitiveData
namespace: blueprint

tasks:
  - id: extract
    type: io.kestra.plugin.fs.http.Download
    uri: https://raw.githubusercontent.com/kestra-io/examples/main/datasets/orders.csv

  - id: transform
    type: io.kestra.plugin.jdbc.duckdb.Query
    inputFiles:
      data.csv: ""{{outputs.extract.uri}}""
    sql: |
      CREATE TABLE orders_pii AS 
      SELECT order_id, 
          hash(customer_name) as customer_name_hash, 
          md5(customer_email) as customer_email_hash, 
          product_id, 
          price, 
          quantity, 
          total 
      FROM read_csv_auto('{{workingDir}}/data.csv');
      COPY (SELECT * FROM orders_pii) TO '{{ outputFiles.csv }}' (HEADER, DELIMITER ',');
    outputFiles:
      - csv

  - id: load
    type: io.kestra.plugin.gcp.bigquery.Load
    from: ""{{ outputs.transform.outputFiles.csv }}""
    serviceAccount: ""{{ secret('GCP_CREDS') }}""
    projectId: yourGcpProject
    destinationTable: yourGcpProject.stage.orders_pii
    format: CSV
    autodetect: true
    csvOptions:
      fieldDelimiter: "",""",2023-07-26 00:11:25.917,"DuckDB,SQL,BigQuery"
22,Retry a failing task up to 4 times (allowing up to 5 attempts with up to 4 retries),"[""io.kestra.plugin.scripts.shell.Commands"", ""io.kestra.core.tasks.debugs.Return""]",The task `fail4times` will fail 4 times and will finish successfully at the 5th attempt.,"id: retries
namespace: blueprint

tasks:
  - id: fail4times
    type: io.kestra.plugin.scripts.shell.Commands
    runner: PROCESS
    commands:
      - 'if [ ""{{taskrun.attemptsCount}}"" -eq 4 ]; then exit 0; else exit 1; fi'
    retry:
      type: constant
      interval: PT0.25S
      maxAttempt: 5
      maxDuration: PT1M
      warningOnRetry: false

errors:
  - id: will-never-happen
    type: io.kestra.core.tasks.debugs.Return
    format: This will never be executed as retries will fix the issue
",2023-07-26 19:00:48.182,
59,Export output of a Postgres SQL query to a CSV file,"[""io.kestra.plugin.jdbc.postgresql.CopyOut"", ""io.kestra.core.tasks.log.Log""]",This example shows how to extract a table from a Postgres table. The flow logs the number of extracted rows. The extracted file can be retrieved with the `{{ outputs.export.uri }}` command.,"id: export_from_postgres
namespace: blueprint

tasks:
  - id: export
    type: io.kestra.plugin.jdbc.postgresql.CopyOut
    url: jdbc:postgresql://sample_postgres:5433/world
    username: postgres
    password: postgres
    format: CSV
    header: true
    sql: SELECT * FROM country LIMIT 10
    delimiter: "",""

  - id: log
    type: io.kestra.core.tasks.log.Log
    message: '{{ outputs.export.rowCount }}'
",2023-07-27 00:34:33.545,"Outputs,SQL,Postgres"
197,Process a file from S3 only if it changed since the last execution,"[""io.kestra.plugin.scripts.python.Commands"", ""io.kestra.core.models.triggers.types.Schedule""]","Add the following Python script named `s3_modified.py` in the Editor:

```python
import boto3
from datetime import datetime
import argparse

def parse_date(date_str):
    if date_str.endswith('Z'):
        return datetime.fromisoformat(date_str.replace('Z', '+00:00'))
    return datetime.fromisoformat(date_str)

def check_s3_object_modification(bucket, object, trigger_date):
    s3 = boto3.client(""s3"")
    response = s3.head_object(Bucket=bucket, Key=object)
    last_modified = response[""LastModified""]
    comparison_datetime = parse_date(trigger_date)

    if last_modified > comparison_datetime:
        print(f""The file '{object}' was modified at {last_modified} and needs to be reprocessed."")
        # TODO implement your processing here, alternatively download the file and process it using other kestra tasks
    else:
        print(f""The file '{object}' is unchanged."")

def main():
    parser = argparse.ArgumentParser(description='Check if an S3 object was modified after a given date.')
    parser.add_argument('bucket_name', help='Name of the S3 bucket')
    parser.add_argument('object_key', help='Key of the S3 object')
    parser.add_argument('comparison_date', help='Date to compare against in ISO format')

    args = parser.parse_args()

    check_s3_object_modification(args.bucket_name, args.object_key, args.comparison_date)

if __name__ == ""__main__"":
    main()
```

Make sure to add [secrets](https://kestra.io/docs/developer-guide/secrets) for your AWS credentials and adjust the variables to point to your S3 bucket and object.","id: process_s3_file_if_changed
namespace: dev

variables:
  bucket: kestraio
  object: hello.txt

tasks:
  - id: process_file_if_changed
    type: io.kestra.plugin.scripts.python.Commands
    commands:
      - python s3_modified.py {{ vars.bucket }} {{ vars.object }} {{ trigger.date ?? execution.startDate }}
    docker:
      image: ghcr.io/kestra-io/aws:latest
    namespaceFiles:
      enabled: true
    env:
      AWS_ACCESS_KEY_ID: ""{{ secret('AWS_ACCESS_KEY_ID') }}""
      AWS_SECRET_ACCESS_KEY: ""{{ secret('AWS_SECRET_ACCESS_KEY') }}""
      AWS_DEFAULT_REGION: ""{{ secret('AWS_DEFAULT_REGION') }}""

triggers:
  - id: schedule
    type: io.kestra.core.models.triggers.types.Schedule
    cron: ""*/5 * * * *""",2023-11-17 11:09:04.378,"Trigger,Python,S3"
135,Send custom events from your application to AWS EventBridge,"[""io.kestra.plugin.aws.eventbridge.PutEvents"", ""io.kestra.core.tasks.flows.WorkingDirectory"", ""io.kestra.core.tasks.storages.LocalFiles""]","This flow will send one or more events to the specified event bus on AWS EventBridge. 

The events can be sent as a JSON file or as a list of maps (dictionaries). For simple use cases, the map syntax is the easiest to get started. However, if your application already emits events in the same format as required by the AWS CLI and API, you can extract JSON data in one task and pass it to the AWS EventBridge `PutEvents` task — just make sure to convert JSON file to the ION format, as this is a format expected by the task to allow row-by-row processing.","id: aws_event_bridge
namespace: blueprint

tasks:
  - id: send_events
    type: io.kestra.plugin.aws.eventbridge.PutEvents
    entries:
      - source: kestra
        eventBusName: default
        detailType: my-custom-app
        detail:
          message: this could be any event - a user sign-in event or a payment

      - source: kestra
        eventBusName: default
        detailType: my-custom-app
        detail:
          message: this could also be any event - an IoT device event or a sensor reading

      - source: kestra
        eventBusName: default
        detailType: my-custom-app
        detail:
          message: another event which could be a user sign-out event or a newsletter subscription

  - id: extract_json
    type: io.kestra.plugin.fs.http.Download
    uri: https://huggingface.co/datasets/kestra/datasets/raw/main/json/app_events.json
    
  - id: json_to_ion
    type: io.kestra.plugin.serdes.json.JsonReader
    from: ""{{ outputs.extract_json.uri }}""
    newLine: false

  - id: send_events_json
    type: io.kestra.plugin.aws.eventbridge.PutEvents
    entries: ""{{ outputs.json_to_ion.uri }}""
",2023-09-25 23:12:43.392,"API,AWS"
65,Run DDL queries and load data to Postgres,"[""io.kestra.plugin.fs.http.Download"", ""io.kestra.plugin.jdbc.postgresql.Query"", ""io.kestra.plugin.jdbc.postgresql.CopyIn""]","This flow will extract data from a remote CSV file and load it into a Postgres database. It's recommended to store database credentials using secrets.

For local testing, you can start a Postgres database using Docker:

```bash
docker run -d --name mypostgres -v mypostgresdb:/var/lib/postgresql/data -p 5432:5432 -e POSTGRES_USER=postgres -e POSTGRES_PASSWORD=topSecret42 -e POSTGRES_DB=postgres postgres:latest
```
","id: extractLoadPostgres
namespace: blueprint

variables:
  db: jdbc:postgresql://host.docker.internal:5432/postgres
  table: public.orders

tasks:
  - id: extract
    type: io.kestra.plugin.fs.http.Download
    uri: https://raw.githubusercontent.com/kestra-io/examples/main/datasets/orders.csv

  - id: query
    type: io.kestra.plugin.jdbc.postgresql.Query
    url: ""{{vars.db}}""
    username: postgres
    password: ""{{ secret('DB_PASSWORD') }}""
    sql: |
      create table if not exists {{vars.table}}
      (
          order_id       integer,
          customer_name  varchar(50),
          customer_email varchar(50),
          product_id     integer,
          price          real,
          quantity       integer,
          total          real
      );

  - id: loadToPostgres
    type: io.kestra.plugin.jdbc.postgresql.CopyIn
    url: ""{{vars.db}}""
    username: postgres
    password: ""{{ secret('DB_PASSWORD') }}""
    from: ""{{ outputs.extract.uri }}""
    format: CSV
    header: true
    table: ""{{vars.table}}""
",2023-07-26 00:33:13.261,"Ingest,SQL,Postgres"
127,Temporary workaround for missing backfill end,"[""io.kestra.core.tasks.log.Log"", ""io.kestra.core.tasks.flows.If"", ""io.kestra.core.tasks.executions.Fail"", ""io.kestra.core.models.triggers.types.Schedule""]","We are working on improving the backfill feature a.o. by adding an end date: https://github.com/kestra-io/kestra/issues/2042

Until then, you can use the following flow example to manually Fail the flow if the trigger date is after a specific cutoff date. Replace the date `'2023-09-13T13:37:00+02:00'` with the cutoff date that you need. ","id: backfill_until_specific_date
namespace: blueprint

tasks:
- id: log
  type: io.kestra.core.tasks.log.Log
  message: starting the flow

- id: after_cutoff_date
  type: io.kestra.core.tasks.flows.If
  condition: ""{{ trigger.date | timestamp > '2023-09-13T13:37:00+02:00' | date | timestamp }}""
  then:
    - id: fail_execution
      type: io.kestra.core.tasks.executions.Fail

  else:
    - id: continue
      type: io.kestra.core.tasks.log.Log
      message: Continue my job

triggers:
  - id: every_minute
    type: io.kestra.core.models.triggers.types.Schedule
    cron: ""*/1 * * * *""",2023-10-12 13:26:40.258,Backfills
124,Clone a Git repository with PySpark code and run a Spark job using the Spark Submit CLI,"[""io.kestra.core.tasks.flows.WorkingDirectory"", ""io.kestra.plugin.git.Clone"", ""io.kestra.plugin.spark.SparkCLI""]","This flow clones a git repository and runs a Spark job.

Make sure to expose the port 7077 on your Spark master in order for this flow to work.","id: gitSpark
namespace: blueprint

tasks:
  - id: working_directory
    type: io.kestra.core.tasks.flows.WorkingDirectory
    tasks:
      - id: git
        type: io.kestra.plugin.git.Clone
        url: https://github.com/kestra-io/scripts
        branch: main

      - id: spark_job
        type: io.kestra.plugin.spark.SparkCLI
        commands:
          - spark-submit --name Pi --master spark://localhost:7077 etl/spark_pi.py
",2023-08-27 14:57:41.829,"CLI,Git"
98,Automate tasks on Azure using Azure CLI,"[""io.kestra.plugin.azure.cli.AzCLI""]","This flow demonstrates how you can use the Azure CLI plugin to automate various tasks on Azure. The task below runs a simple command to list Azure regions using the `az account list-locations` command. The output is a JSON array of region names — you can either output that result to a JSON file or use the `--query` parameter to only show the region names.

While the example shown below is simple, the CLI allows you to automate virtually anything. Every action in Azure can be performed via an API call that you can trigger from the CLI. For instance, you can follow the [Azure documentation](https://learn.microsoft.com/en-us/azure/container-instances/container-instances-quickstart) to start containers on Azure. Azure Container Instances support both CPU and GPU workloads, and all that can be orchestrated with this simple CLI task.","id: azureCLI
namespace: blueprint

tasks:
  - id: listAzureRegions
    type: io.kestra.plugin.azure.cli.AzCLI
    tenant: ""{{ secret('AZURE_TENANT_ID') }}""
    username: ""{{ secret('AZURE_SERVICE_PRINCIPAL_CLIENT_ID') }}""
    password: ""{{ secret('AZURE_SERVICE_PRINCIPAL_PASSWORD') }}""
    servicePrincipal: true
    commands:
      - az account list-locations --query ""[].{Region:name}"" -o table
",2023-07-20 13:00:41.411,Azure
149,"Extract data from a REST API, load it to Weaviate and query it with GraphQL","[""io.kestra.plugin.fs.http.Download"", ""io.kestra.plugin.serdes.json.JsonReader"", ""io.kestra.plugin.weaviate.BatchCreate"", ""io.kestra.plugin.weaviate.Query""]","This flow shows how to extract data from an HTTP API, load it to a Weaviate cluster and query it with GraphQL.

This flow assumes that you have a [Weaviate cluster](https://console.weaviate.cloud/) running, and that you created an API key. Make sure to replace the `url` and `apiKey` values in the tasks with your Weaviate credentials. It's recommended to use [Secrets](https://kestra.io/docs/developer-guide/secrets/) to store your API key.

Once you've configured the Weaviate secret, you can reproduce this flow without any changes. It will load the data from the [Jeopardy dataset](https://www.kaggle.com/tunguz/200000-jeopardy-questions) to Weaviate, and then query it with GraphQL.

You can ingest data to Weaviate from a Kestra flow using one of the following options:
1. Key-value pairs, as shown in the `batch_load_map` task. This option is useful when you want to load simple key-value data.
2. An internal storage URI, as shown in the `batch_load` task. This option is recommended when you want to load data from a previous task in the same flow, e.g. after extracting it from a database or a file.

The last task performing a [Generative Search](https://weaviate.io/developers/weaviate/starter-guides/generative#what-is-generative-search) is currently disabled, as it requires an OpenAI API key and following the [Weaviate getting started documentation](https://weaviate.io/developers/weaviate/quickstart). You can enable it by removing the `disabled: true` line.
","id: weaviate_load_and_query
namespace: blueprint

tasks:
  - id: json
    type: io.kestra.plugin.fs.http.Download
    uri: https://raw.githubusercontent.com/weaviate-tutorials/quickstart/main/data/jeopardy_tiny.json

  - id: json_to_ion
    type: io.kestra.plugin.serdes.json.JsonReader
    from: ""{{ outputs.json.uri }}""
    newLine: false

  - id: batch_load
    type: io.kestra.plugin.weaviate.BatchCreate
    url: https://demo-oczq9ryw.weaviate.network
    apiKey: ""{{ secret('WEAVIATE_API_KEY') }}""
    className: Questions
    objects: ""{{ outputs.json_to_ion.uri }}""

  - id: batch_load_map
    type: io.kestra.plugin.weaviate.BatchCreate
    url: demo-oczq9ryw.weaviate.network
    apiKey: ""{{ secret('WEAVIATE_API_KEY') }}""
    className: Users
    objects:
      - company: kestra
        user: Anna
        city: Berlin
      - company: initech
        user: Peter Gibbons
        city: Austin
      - company: initech
        user: Bill Lumbergh
        city: Austin
      - company: initech
        user: Bob Slydell
        city: Austin

  - id: query_users
    type: io.kestra.plugin.weaviate.Query
    url: demo-oczq9ryw.weaviate.network # also works with https://cluster.url
    apiKey: ""{{ secret('WEAVIATE_API_KEY') }}""
    query: | 
      {
        Get {
          Questions(limit: 10) {
            answer
            category
            question
          }
        }
      }      

  - id: generative_search
    type: io.kestra.plugin.weaviate.Query
    disabled: true
    url: https://demo-oczq9ryw.weaviate.network
    apiKey: ""{{ secret('WEAVIATE_API_KEY') }}""
    headers: 
      X-OpenAI-Api-Key: ""{{ secret('OPENAI_API_KEY') }}""
    query: | 
      {
        Get {
          Question(limit: 5, nearText: {concepts: [""biology""]}) {
            answer
            category
            question
          }
        }
      }",2023-11-21 17:10:33.493,"API,Ingest,AI"
200,Run a Papermill notebook,"[""io.kestra.plugin.scripts.python.Commands""]","This blueprint shows how to execute a Jupyter Notebook within a Kestra flow using the Papermill library.

Here we use Namespace Files where we created the `src/example.ipynb.py` notebook. We expose the outputs of the notebook execution into the `output.ipynb` file.","id: papermill_notebook
namespace: blueprint

tasks:
  - id: python
    type: io.kestra.plugin.scripts.python.Commands
    namespaceFiles:
      enabled: true
    beforeCommands:
      - pip install ipykernel papermill
      - python -m ipykernel install --name python3.12.0
    commands:
      - papermill src/example.ipynb.py output.ipynb -k python3.12.0
    outputFiles:
      - output.ipynb",2023-11-30 12:54:44.47,"Namespace Files,Python"
73,Store and retrieve JSON data using Redis,"[""io.kestra.plugin.redis.Set"", ""io.kestra.plugin.redis.Get""]",This flow will set a key-value pair in Redis and then retrieve it. The key-value pair will be set using inputs which can be provided at runtime.,"id: redisKeyValueStore
namespace: blueprint

inputs:
  - name: key
    type: STRING
    defaults: johndoe

  - name: value
    type: JSON
    defaults: |
      {
        ""id"": 123456,
        ""name"": ""John Doe"",
        ""email"": ""johndoe@example.com"",
        ""age"": 30,
        ""address"": {
          ""street"": ""123 Main Street"",
          ""city"": ""Anytown"",
          ""state"": ""California"",
          ""country"": ""United States""
        },
        ""phone"": ""+1 555-123-4567"",
        ""isPremium"": true,
        ""interests"": [""programming"", ""reading"", ""traveling""]
      }

tasks:
  - id: set
    type: ""io.kestra.plugin.redis.Set""
    url: redis://host.docker.internal:6379/0
    serdeType: JSON
    key: ""{{inputs.key}}""
    value: ""{{inputs.value}}""

  - id: get
    type: io.kestra.plugin.redis.Get
    url: redis://host.docker.internal:6379/0
    serdeType: JSON
    key: ""{{inputs.key}}""
    ",2023-07-26 00:32:34.563,Ingest
11,Switch tasks depending on a specific value,"[""io.kestra.core.tasks.flows.Switch"", ""io.kestra.core.tasks.debugs.Return""]",The `Switch` task will drive the flow depending of the input value,"id: switch
namespace: blueprint

inputs:
  - name: string
    type: STRING

tasks:

  - id: switch
    type: io.kestra.core.tasks.flows.Switch
    value: ""{{ inputs.string }}""
    cases:
      A:
        - id: a
          type: io.kestra.core.tasks.debugs.Return
          format: ""The input is {{ inputs.string }}""
      B:
        - id: b
          type: io.kestra.core.tasks.debugs.Return
          format: ""The input is {{ inputs.string }}""
    defaults:
      - id: default
        type: io.kestra.core.tasks.debugs.Return
        format: ""This is the default case"" ",2023-07-26 18:59:02.052,
103,"Extract data, transform it, and load it in parallel to S3 and Postgres — in less than 7 seconds!","[""io.kestra.plugin.fs.http.Download"", ""io.kestra.plugin.serdes.json.JsonReader"", ""io.kestra.plugin.serdes.json.JsonWriter"", ""io.kestra.plugin.scripts.jython.FileTransform"", ""io.kestra.core.tasks.flows.Parallel"", ""io.kestra.core.tasks.flows.Sequential"", ""io.kestra.plugin.serdes.csv.CsvWriter"", ""io.kestra.plugin.jdbc.postgresql.Query"", ""io.kestra.plugin.jdbc.postgresql.CopyIn"", ""io.kestra.plugin.aws.s3.Upload""]","**EXTRACT**: This workflow downloads data from an external API via HTTP GET request and stores the result in Kestra's internal storage in ION format. 

**TRANSFORM**: The extracted file is then serialized to JSON and transformed row-by-row to add a new column.

**LOAD**:  The final result is ingested in parallel to S3 and Postgres.","id: apiJsonToPostgres
namespace: blueprint

tasks:
  - id: download
    type: io.kestra.plugin.fs.http.Download
    uri: https://gorest.co.in/public/v2/users

  - id: ionToJSON
    type: ""io.kestra.plugin.serdes.json.JsonReader""
    from: ""{{outputs.download.uri}}""
    newLine: false

  - id: json
    type: io.kestra.plugin.serdes.json.JsonWriter
    from: ""{{outputs.ionToJSON.uri}}""

  - id: addColumn
    type: io.kestra.plugin.scripts.jython.FileTransform
    from: ""{{outputs.json.uri}}""
    script: |
      from datetime import datetime
      logger.info('row: {}', row)
      row['inserted_at'] = datetime.utcnow()

  - id: parallel
    type: io.kestra.core.tasks.flows.Parallel
    tasks:
      - id: postgres
        type: io.kestra.core.tasks.flows.Sequential
        tasks:
          - id: finalCSV
            type: io.kestra.plugin.serdes.csv.CsvWriter
            from: ""{{outputs.addColumn.uri}}""
            header: true

          - id: createTable
            type: io.kestra.plugin.jdbc.postgresql.Query
            url: jdbc:postgresql://host.docker.internal:5432/
            username: postgres
            password: qwerasdfyxcv1234
            sql: |
              CREATE TABLE IF NOT EXISTS public.raw_users
                (
                    id            int,
                    name          VARCHAR,
                    email         VARCHAR,
                    gender        VARCHAR,
                    status        VARCHAR,
                    inserted_at   timestamp
                );

          - id: loadData
            type: io.kestra.plugin.jdbc.postgresql.CopyIn
            url: jdbc:postgresql://host.docker.internal:5432/
            username: postgres
            password: qwerasdfyxcv1234
            format: CSV
            from: ""{{outputs.finalCSV.uri}}""
            table: public.raw_users
            header: true

      - id: s3
        type: io.kestra.core.tasks.flows.Sequential
        tasks: 
          - id: finalJSON
            type: io.kestra.plugin.serdes.json.JsonWriter
            from: ""{{outputs.addColumn.uri}}""

          - id: jsonToS3
            type: io.kestra.plugin.aws.s3.Upload
            disabled: true
            from: ""{{outputs.finalJSON.uri}}""
            key: users.json
            bucket: kestraio
            region: ""{{ secret('AWS_DEFAULT_REGION') }}""
            accessKeyId: ""{{ secret('AWS_ACCESS_KEY_ID')}}""
            secretKeyId: ""{{ secret('AWS_SECRET_ACCESS_KEY')}}""",2023-07-26 00:50:38.283,"Parallel,Outputs,Postgres,Ingest,S3"
44,Pass data between Python script tasks and Shell tasks using Outputs,"[""io.kestra.core.tasks.debugs.Return"", ""io.kestra.plugin.scripts.python.Script"", ""io.kestra.core.tasks.log.Log"", ""io.kestra.plugin.scripts.shell.Commands""]","This flow shows how to pass data between tasks using outputs.
The first two tasks return some outputs and the next 2 tasks read those values for further processing.

Check the ""Outputs"" section in each task documentation to see what outputs it returns, and check the ""Outputs"" tab on the Execution page to validate what outputs are generated by this flow.
- In case of the `Return`task, it returns a value under the `value` key.
- All script tasks, including Python, return a map of outputs under the `vars` key. To access outputs in downstream tasks, use the format `{{outputs.task_name.vars.key_name}}`. Additionally, script tasks can return files as shown with the `myoutput.json` file.


","id: passDataBetweenTasks
namespace: blueprint

tasks:
    - id: passOutput
      type: io.kestra.core.tasks.debugs.Return
      format: ""hello""

    - id: pyOutputs
      type: io.kestra.plugin.scripts.python.Script
      docker:
        image: ghcr.io/kestra-io/pydata:latest
      script: |
        import json
        from kestra import Kestra

        my_kv_pair = {'mykey': 'from Kestra'}
        Kestra.outputs(my_kv_pair)

        with open('{{outputDir}}/myoutput.json', 'w') as f:
            json.dump(my_kv_pair, f)

    - id: takeInputs
      type: io.kestra.core.tasks.log.Log
      message: |
        data from previous tasks: {{outputs.passOutput.value}} and {{outputs.pyOutputs.vars.mykey}}
    
    - id: checkOutputFile
      type: io.kestra.plugin.scripts.shell.Commands
      runner: PROCESS
      commands:
        - cat {{outputs.pyOutputs.outputFiles['myoutput.json']}}",2023-07-25 22:54:21.916,"Python,Outputs"
42,Transform data from CSV files with Pandas in Python containers (in parallel),"[""io.kestra.core.tasks.flows.EachParallel"", ""io.kestra.plugin.scripts.python.Script""]",This flow reads a list of CSV files and processes each file in parallel in isolated Python scripts using Pandas.,"id: pythonCsvEachParallel
namespace: blueprint

tasks:
  - id: csv
    type: io.kestra.core.tasks.flows.EachParallel
    value:
      - https://raw.githubusercontent.com/kestra-io/dbt_duckdb/duckdb/seeds/raw_customers.csv
      - https://raw.githubusercontent.com/kestra-io/dbt_duckdb/duckdb/seeds/raw_orders.csv
      - https://raw.githubusercontent.com/kestra-io/dbt_duckdb/duckdb/seeds/raw_payments.csv
    tasks:
      - id: pandas
        type: io.kestra.plugin.scripts.python.Script
        warningOnStdErr: false
        docker:
          image: ghcr.io/kestra-io/pydata:latest
        script: |
          import pandas as pd
          df = pd.read_csv(""{{taskrun.value}}"")
          df.info()
",2023-07-25 18:02:29.879,"Python,Parallel"
189,Send an alert to Zenduty when any flow fails in the production namespace,"[""io.kestra.plugin.notifications.zenduty.ZendutyExecution"", ""io.kestra.core.models.triggers.types.Flow"", ""io.kestra.core.models.conditions.types.ExecutionStatusCondition"", ""io.kestra.core.models.conditions.types.ExecutionNamespaceCondition""]","This flow sends an alert to Zenduty when a production flow fails. The only required input is an integration key string value. Check the [Zenduty documentation](https://docs.zenduty.com/docs/api) to learn how to create an API integration and generate the key. The API integration will send an API call that follows the format: 


```bash
curl -X POST https://www.zenduty.com/api/events/[integration-key]/ -H 'Content-Type: application/json' -d '{""alert_type"":""critical"", ""message"":""Some message"", ""summary"":""some summary"", ""entity_id"":""some_entity_id""}'
```


The `message` and `summary` parameters are required. The `alert_type` parameter is the severity of the issue, including `info`, `warning`, `error`, or `critical`. 

This kestra task abstracts away raw API calls and only requires the integration key, which you can store as a [kestra secret](https://kestra.io/docs/developer-guide/secrets). The default value is `error`. Visit the Zenduty [Events API documentation](https://apidocs.zenduty.com/#tag/Events) for more details.","id: zenduty_failure_alert
namespace: prod.monitoring

tasks:
  - id: send_alert
    type: io.kestra.plugin.notifications.zenduty.ZendutyExecution
    url: ""https://www.zenduty.com/api/events/{{ secret('ZENDUTY_INTEGRATION_KEY') }}/""
    executionId: ""{{ trigger.executionId }}""
    message: Kestra workflow execution {{ trigger.executionId }} of a flow {{ trigger.flowId }} in the namespace {{ trigger.namespace }} changed status to {{ trigger.state }}

triggers:
  - id: failed_prod_workflows
    type: io.kestra.core.models.triggers.types.Flow
    conditions:
      - type: io.kestra.core.models.conditions.types.ExecutionStatusCondition
        in:
          - FAILED
          - WARNING
      - type: io.kestra.core.models.conditions.types.ExecutionNamespaceCondition
        namespace: prod
        prefix: true
",2023-10-26 12:45:35.835,Notifications
121,"Extract data from a REST API, process it in Python with Polars in a Docker container, then run DuckDB query and preview results as a table in the Outputs tab","[""io.kestra.plugin.fs.http.Request"", ""io.kestra.plugin.scripts.python.Script"", ""io.kestra.plugin.jdbc.duckdb.Query""]","This flow will download a file from a REST API, process it with Python and SQL, and store the result in the internal storage. 

- the `Request` task uses a public API — to interact with a private API endpoint, check the task documentation for examples on how to authenticate your request
- the `Python` task runs in a separate Docker container and installs `Polars` before starting the script
- the DuckDB query task uses data from a previous task and outputs the query result to internal storage. 

The Outputs tab provides a well-formatted table with the query results.","id: api-python-sql
namespace: blueprint

tasks:
  - id: api
    type: io.kestra.plugin.fs.http.Request
    uri: https://dummyjson.com/products

  - id: python
    type: io.kestra.plugin.scripts.python.Script
    docker:
      image: python:slim
    beforeCommands:
      - pip install polars
    warningOnStdErr: false
    script: |
      import polars as pl
      data = {{outputs.api.body | jq('.products') | first}}
      df = pl.from_dicts(data)
      df.glimpse()
      df.select([""brand"", ""price""]).write_csv(""{{outputDir}}/products.csv"")

  - id: sqlQuery
    type: io.kestra.plugin.jdbc.duckdb.Query
    inputFiles:
      in.csv: ""{{ outputs.python.outputFiles['products.csv'] }}""
    sql: |
      SELECT brand, round(avg(price), 2) as avg_price
      FROM read_csv_auto('{{workingDir}}/in.csv', header=True)
      GROUP BY brand
      ORDER BY avg_price DESC;
    store: true",2023-08-23 09:18:02.91,"Outputs,SQL,DuckDB,Python,API"
117,Orchestrate CloudQuery data ingestion syncs to integrate data about your AWS resources into Postgres and and analyze it in SQL,"[""io.kestra.core.tasks.flows.WorkingDirectory"", ""io.kestra.core.tasks.storages.LocalFiles"", ""io.kestra.plugin.scripts.shell.Commands"", ""io.kestra.plugin.jdbc.postgresql.Query""]","[CloudQuery](https://www.cloudquery.io/) is an open-source data integration platform built for developers. CloudQuery [integrates seamlessly with Kestra](https://www.cloudquery.io/docs/deployment/kestra).

The flow below syncs metadata about various AWS services and loads it to a Postgres database. 

You can use the following command to start a Postgres instance using a Docker container:

```bash
docker run -d --name mypostgres -v mypostgresdb:/var/lib/postgresql/data -p 5432:5432 -e POSTGRES_USER=postgres -e POSTGRES_PASSWORD=yourPassword1234 -e POSTGRES_DB=postgres postgres:latest
```

You can securely manage the Postgres password and AWS credentials in Kestra by using [Secrets](https://kestra.io/docs/developer-guide/secrets). 

After you execute the sync, we can start querying information about AWS resources in Postgres, e.g. to find all S3 buckets that are permitted to be public.

Note that instead of only querying tables about S3 (on line 17), you can add many more tables such as: `tables: [""aws_s3*"", ""aws_ec2*"", ""aws_ecs*"", ""aws_iam*"", ""aws_glue*"", ""aws_dynamodb*""]`. Check the Cloud Query documentation for [a full list of supported tables](https://www.cloudquery.io/docs/plugins/sources/aws/tables).","id: cloudQueryAWS
namespace: blueprint

tasks:
  - id: wdir
    type: io.kestra.core.tasks.flows.WorkingDirectory
    tasks:
      - id: config
        type: io.kestra.core.tasks.storages.LocalFiles
        inputs:
          config.yml: |
            kind: source
            spec:
              name: aws
              path: cloudquery/aws
              version: ""v22.4.0""
              tables: [""aws_s3*""]
              destinations: [""postgresql""]
              spec:
            ---
            kind: destination
            spec:
              name: ""postgresql""
              version: ""v5.0.3""
              path: ""cloudquery/postgresql""
              write_mode: ""overwrite-delete-stale""
              spec:
                connection_string: ${PG_CONNECTION_STRING}

      - id: cloudQuery
        type: io.kestra.plugin.scripts.shell.Commands
        runner: DOCKER
        env:
          AWS_ACCESS_KEY_ID: ""{{ secret('AWS_ACCESS_KEY_ID') }}""
          AWS_SECRET_ACCESS_KEY: ""{{ secret('AWS_SECRET_ACCESS_KEY') }}""
          AWS_DEFAULT_REGION: ""{{ secret('AWS_DEFAULT_REGION') }}""    
          PG_CONNECTION_STRING: ""postgresql://postgres:{{secret('DB_PASSWORD')}}@host.docker.internal:5432/postgres?sslmode=disable""
        docker:
          image: ghcr.io/cloudquery/cloudquery:latest
          entryPoint: [""""]
        warningOnStdErr: false
        commands:
        - /app/cloudquery sync config.yml --log-console
      
  - id: query-s3-metadata-in-postgres
    type: io.kestra.plugin.jdbc.postgresql.Query
    url: jdbc:postgresql://host.docker.internal:5432/postgres
    username: postgres
    password: ""{{ secret('DB_PASSWORD') }}""
    fetch: true
    sql: |
      SELECT arn, region
      FROM public.aws_s3_buckets
      WHERE block_public_acls IS NOT TRUE
          OR block_public_policy IS NOT TRUE
          OR ignore_public_acls IS NOT TRUE
          OR restrict_public_buckets IS NOT TRUE;
",2023-08-07 14:54:14.443,"SQL,S3,Ingest,AWS"
88,Use BigQuery and Python script running in Docker to analyze Wikipedia page views,"[""io.kestra.plugin.gcp.bigquery.Query"", ""io.kestra.plugin.serdes.csv.CsvWriter"", ""io.kestra.core.tasks.flows.WorkingDirectory"", ""io.kestra.core.tasks.storages.LocalFiles"", ""io.kestra.plugin.scripts.python.Script""]","This flow will do the following:

1. Use `bigquery.Query` task to query the top 10 wikipedia pages for the current day 
2. Use `CsvWriter` to store the results in a CSV file. 
3. Use `python.Script` task to read the CSV file and use pandas to find the maximum number of views.
4. Use Kestra `outputs` to track the maximum number of views over time.

The Python script will run in a Docker container based on the public image `ghcr.io/kestra-io/pydata:latest`. 

The BigQuery task exposes (by default) a variety of **metrics** such as:
- total.bytes.billed
- total.partitions.processed
- number of rows processed
- query duration.

You can view those metrics on the Execution page in the Metrics tab. ","id: wikipediaTop10pythonPandas
namespace: blueprint
description: analyze top 10 Wikipedia pages

tasks:
  - id: query
    type: io.kestra.plugin.gcp.bigquery.Query
    sql: |
      SELECT DATETIME(datehour) as date, title, views FROM `bigquery-public-data.wikipedia.pageviews_2023`
      WHERE DATE(datehour) = current_date() and wiki = 'en'
      ORDER BY datehour desc, views desc
      LIMIT 10
    store: true
    projectId: ""{{envs.gcp_project_id}}""
    serviceAccount: ""{{ secret('GCP_CREDS') }}""

  - id: write-csv
    type: io.kestra.plugin.serdes.csv.CsvWriter
    from: ""{{outputs.query.uri}}""

  - id: wdir
    type: io.kestra.core.tasks.flows.WorkingDirectory
    tasks:
    - id: file
      type: io.kestra.core.tasks.storages.LocalFiles
      inputs:
        data.csv: ""{{outputs['write-csv'].uri}}""
    
    - id: pandas
      type: io.kestra.plugin.scripts.python.Script
      warningOnStdErr: false
      docker: 
        image: ghcr.io/kestra-io/pydata:latest
      script: |
        import pandas as pd
        from kestra import Kestra
        
        df = pd.read_csv(""data.csv"")
        df.head(10)
        views = df['views'].max()
        Kestra.outputs({'views': int(views)})",2023-07-25 17:49:07.692,"Metrics,Outputs,Python,Local files,BigQuery"
82,Docker container installing pip packages before starting a Python Script task,"[""io.kestra.plugin.scripts.python.Script""]","This flow uses the default `DOCKER` runner and the default `python:latest` Docker image. This means that `pip` is available out of the box and you can add several `beforeCommands` to install custom Pip packages, and prepare the environment for the task.

Adding `warningOnStdErr: false` ensures that warnings raised during pip package installation don't set the task to a `WARNING` state. However, by default, any warning raised during the setup process (i.e. when executing `beforeCommands`) will end the task in the `WARNING` state.
","id: pipPackagesDocker
namespace: blueprint

tasks:
  - id: run_python
    type: io.kestra.plugin.scripts.python.Script
    beforeCommands:
      - pip install requests kestra > /dev/null
    warningOnStdErr: false
    script: | 
      import requests
      import json

      response = requests.get(""https://api.github.com"")
      data = response.json()
      print(data)",2023-07-25 23:58:46.545,"Python,pip"
188,Send an alert to Sentry when a flow fails,"[""io.kestra.core.tasks.executions.Fail"", ""io.kestra.plugin.notifications.sentry.SentryAlert""]","This flow shows how to send an alert to Sentry when a flow fails.

The only required input is a DSN string value, which you can find when you go to your Sentry project settings and go to the section ""Client Keys (DSN)"". You can find more detailed description of how to find your DSN in the [following Sentry documentation](https://docs.sentry.io/product/sentry-basics/concepts/dsn-explainer/#where-to-find-your-dsn).

You can customize the alert `payload`, which is a JSON object, or you can skip it and use the default payload created by kestra. For more information about the payload, check the [Sentry Event Payloads documentation](https://develop.sentry.dev/sdk/event-payloads/).

The `event_id` is an optional payload attribute that you can use to override the default event ID. If you don't specify it (recommended), kestra will generate a random UUID. You can use this attribute to group events together, but note that this must be a UUID type. For more information, check the [Sentry documentation](https://docs.sentry.io/product/issues/grouping-and-fingerprints/).","id: sentry_alert
namespace: blueprint

tasks:
  - id: fail
    type: io.kestra.core.tasks.executions.Fail

errors:
  - id: alert_on_failure
    type: io.kestra.plugin.notifications.sentry.SentryAlert
    dsn: ""{{ secret('SENTRY_DSN') }}""
    payload: |
      {
          ""timestamp"": ""{{ execution.startDate }}"",
          ""platform"": ""java"",
          ""level"": ""error"",
          ""transaction"": ""/execution/id/{{ execution.id }}"",
          ""server_name"": ""localhost:8080"",
          ""extra"": {
            ""Namespace"": ""{{ flow.namespace }}"",
            ""Flow ID"": ""{{ flow.id }}"",
            ""Execution ID"": ""{{ execution.id }}"",
            ""Link"": ""http://localhost:8080/ui/executions/{{flow.namespace}}/{{flow.id}}/{{execution.id}}""
          }      
      }
",2023-10-28 10:06:07.35,Notifications
119,Run a Hightouch sync to orchestrate a reverse ETL process,"[""io.kestra.plugin.hightouch.Sync""]","This example runs a Hightouch sync.

Make sure to add an API token (available in `Settings > API key` in the Hightouch UI) and the sync ID.","id: hightouch-sync
namespace: blueprint

tasks:
  - id: reverseETL
    type: io.kestra.plugin.hightouch.Sync
    token: ""{{ secret('HIGHTOUCH_API_TOKEN') }}""
    syncId: 1716609",2023-08-23 09:20:59.759,
125,Limit Docker container memory to 500MB for a Python script,"[""io.kestra.plugin.scripts.python.Script""]",The example below will use no more than 500MB of memory for the Docker container,"id: limit_memory
namespace: blueprint

tasks:
  - id: docker_memory
    type: io.kestra.plugin.scripts.python.Script
    docker:
      image: python:3.11-slim
      memory: 
        memory: 500MB
    script: |
      import time
      time.sleep(2)",2023-08-30 09:19:39.052,"Python,Docker"
113,Extract multiple tables from Postgres using SQL queries and process those as Pandas dataframes on schedule,"[""io.kestra.core.tasks.flows.Parallel"", ""io.kestra.plugin.jdbc.postgresql.CopyOut"", ""io.kestra.core.tasks.flows.WorkingDirectory"", ""io.kestra.core.tasks.storages.LocalFiles"", ""io.kestra.plugin.scripts.python.Script""]","This flow uses the `Parallel` task to run multiple tasks concurrently - the limit of how many tasks will run at the same time is defined using the `concurrent` property. 

The flow extracts data from a Postgres database. That data is then passed to a Python task using `LocalFiles`. ","id: postgresToPandasDataframes
namespace: blueprint

variables:
  db_host: host.docker.internal

tasks:
  - id: getTables
    type: io.kestra.core.tasks.flows.Parallel
    concurrent: 2
    tasks:
      - id: products
        type: io.kestra.plugin.jdbc.postgresql.CopyOut
        sql: SELECT * FROM products
      
      - id: orders
        type: io.kestra.plugin.jdbc.postgresql.CopyOut
        sql: SELECT * FROM orders
  
  - id: wdir
    type: io.kestra.core.tasks.flows.WorkingDirectory
    tasks:
      - id: inputs
        type: io.kestra.core.tasks.storages.LocalFiles
        inputs:
          products.csv: ""{{outputs.products.uri}}""
          orders.csv: ""{{outputs.orders.uri}}""
      
      - id: pandas
        type: io.kestra.plugin.scripts.python.Script
        warningOnStdErr: false
        docker:
          image: ghcr.io/kestra-io/pydata:latest
        script: |
          import pandas as pd
          products = pd.read_csv(""products.csv"")
          orders = pd.read_csv(""orders.csv"")
          df = orders.merge(products, on=""product_id"", how=""left"")

          top = (
              df.groupby(""product_name"", as_index=False)[""total""]
              .sum()
              .sort_values(""total"", ascending=False)
              .head(10)
          )
          
          top.to_json(""{{outputDir}}/bestsellers_pandas.json"", orient=""records"")

taskDefaults:
  - type: io.kestra.plugin.jdbc.postgresql.CopyOut
    values:
      url: jdbc:postgresql://{{vars.db_host}}:5432/
      username: postgres
      password: ""{{ secret('DB_PASSWORD') }}""
      format: CSV
      header: true
      delimiter: "",""

triggers:
  - id: everyMorning
    type: io.kestra.core.models.triggers.types.Schedule
    cron: 0 9 * * * ",2023-08-09 22:33:34.257,"Postgres,Local files,Python,Parallel,SQL"
40,Clone a GitHub repository and run a Python ETL script,"[""io.kestra.core.tasks.flows.WorkingDirectory"", ""io.kestra.plugin.git.Clone"", ""io.kestra.plugin.scripts.python.Commands""]","This flow clones a git repository and runs a Python ETL script.
- The Python task can install additional pip packages before running the script using `beforeCommands`.
- The image used in this flow is public and you can use it in your workflows.
- You can add `username` and `password` on the `git.Clone` task but note that they are required only for private repositories. When using public GitHub repositories such as [this one](https://github.com/kestra-io/scripts), you can only reference the `url` and `branch`  name.","id: gitPython
namespace: blueprint

tasks:
  - id: pythonScripts
    type: io.kestra.core.tasks.flows.WorkingDirectory
    tasks:
    - id: cloneRepository
      type: io.kestra.plugin.git.Clone
      url: https://github.com/kestra-io/scripts
      branch: main
    
    - id: python
      type: io.kestra.plugin.scripts.python.Commands
      warningOnStdErr: false
      docker:
        image: ghcr.io/kestra-io/pydata:latest
      commands:
        - python etl/global_power_plant.py
",2023-07-25 18:08:24.335,"Python,Git,CLI"
153,Send a Discord message when a production workflow fails,"[""io.kestra.plugin.notifications.discord.DiscordExecution"", ""io.kestra.core.models.triggers.types.Flow"", ""io.kestra.core.models.conditions.types.ExecutionStatusCondition"", ""io.kestra.core.models.conditions.types.ExecutionNamespaceCondition""]","This flow implements a namespace-level monitoring for all workflows in the namespace `prod`. Whenever an execution in that namespace fails, this flow will automatically send a message to a Discord channel. This means that you don't need any boilerplate code to implement alerting logic. You only need to implement that once and thanks to the Flow trigger, Kestra will make sure that all executions matching the conditions will receive notifications.

To use this flow, you need to create a Discord webhook and store it as a [Secret](https://kestra.io/docs/developer-guide/secrets/). Follow this [guide on Discord](https://support.discord.com/hc/en-us/articles/228383668-Intro-to-Webhooks) to create a webhook.","id: discord_alert
namespace: blueprint

tasks:
  - id: send_discord_message
    type: io.kestra.plugin.notifications.discord.DiscordExecution
    url: ""{{ secret('DISCORD_WEBHOOK') }}""
    username: Rick Astley
    embedList:
      - title: Kestra Flow Notification
        color:
          - 255
          - 255
          - 255
    executionId: ""{{ trigger.executionId }}""

triggers:
  - id: failed_prod_workflows
    type: io.kestra.core.models.triggers.types.Flow
    conditions:
      - type: io.kestra.core.models.conditions.types.ExecutionStatusCondition
        in:
          - FAILED
          - WARNING
      - type: io.kestra.core.models.conditions.types.ExecutionNamespaceCondition
        namespace: prod
        prefix: true
",2023-10-12 23:46:45.02,Notifications
43,"Run a Python script and generate outputs, metrics and downloadable file artifact specified with a variable","[""io.kestra.core.tasks.flows.WorkingDirectory"", ""io.kestra.plugin.scripts.python.Script"", ""io.kestra.core.tasks.storages.LocalFiles""]","This flow generates a CSV file with 100 random orders and then calculates the sum and average of the ""total"" column. It then reports the results as outputs and metrics. 

The CSV file generated by a Python task is set as `outputFiles`, allowing you to download the file from the UI's Execution page. It is helpful to share the results of your workflow with business stakeholders who can download the file from the UI and use it in their processes.

To avoid hardcoding values, the filename `orders.csv` is specified as a variable.","id: pythonGenerateOutputs
namespace: blueprint

variables:
  file: orders.csv

tasks:
  - id: wdir
    type: io.kestra.core.tasks.flows.WorkingDirectory
    tasks:
      - id: analyzeOrders
        type: io.kestra.plugin.scripts.python.Script
        warningOnStdErr: false
        beforeCommands:
          - pip install faker kestra > /dev/null
        script: |
          import csv
          import random
          import time
          from faker import Faker
          from kestra import Kestra

          start_time = time.time()
          fake = Faker()

          # list of columns for the CSV file
          columns = [
              ""order_id"",
              ""customer_name"",
              ""customer_email"",
              ""product_id"",
              ""price"",
              ""quantity"",
              ""total"",
          ]
          filename = ""{{vars.file}}""
          tags = {'file': filename}

          # Generate 100 random orders
          orders = []
          for i in range(100):
              order_id = i + 1
              customer_name = fake.name()
              customer_email = fake.email()
              product_id = random.randint(1, 20)
              price = round(random.uniform(10.0, 200.0), 2)
              quantity = random.randint(1, 10)
              total = round(price * quantity, 2)
              orders.append(
                  [order_id, customer_name, customer_email, product_id, price, quantity, total]
              )

          # Write the orders to a CSV file
          with open(filename, ""w"", newline="""") as file:
              writer = csv.writer(file)
              writer.writerow(columns)
              writer.writerows(orders)

          # Calculate and print the sum and average of the ""total"" column
          total_sum = sum(order[6] for order in orders)
          average_order = round(total_sum / len(orders), 2)
          print(f""Total sum: {total_sum}"")
          print(f""Average Order value: {average_order}"")

          Kestra.outputs({""total_sum"": total_sum, ""average_order"": average_order})
          Kestra.counter('total_sum', total_sum, tags)
          Kestra.counter('average_order', average_order, tags)

          end_time = time.time()  
          processing_time = end_time - start_time
          Kestra.timer('processing_time', processing_time, tags)
          print(f""The script execution took: {processing_time} seconds"")

      
      - id: outputCsv
        type: io.kestra.core.tasks.storages.LocalFiles
        outputs:
          - ""{{vars.file}}""",2023-07-26 03:03:32.284,"pip,Python,Local files,Metrics,Outputs"
147,Ingest Zendesk data into DuckDB using dlt,"[""io.kestra.plugin.scripts.python.Script""]","This flow demonstrates how to extract data from Zendesk into a DuckDB database using dlt. The entire workflow logic is contained in a single Python script that uses the dlt Python library to ingest the data into a DuckDB database. The credentials to access the Zendesk API are stored using [Kestra secrets](https://kestra.io/docs/developer-guide/secrets).
","id: dlt_zendesk_to_duckdb
namespace: blueprint

tasks:
  - id: dlt_pipeline
    type: io.kestra.plugin.scripts.python.Script
    docker:
      image: python:3.11
    beforeCommands:
      - pip install dlt[duckdb]
      - dlt --non-interactive init zendesk duckdb
    warningOnStdErr: false
    env:
      SOURCES__ZENDESK__ZENDESK_SUPPORT__CREDENTIALS__PASSWORD: ""{{ secret('ZENDESK_PASSWORD') }}""
      SOURCES__ZENDESK__ZENDESK_SUPPORT__CREDENTIALS__SUBDOMAIN: ""{{ secret('ZENDESK_SUBDOMAIN') }}""
      SOURCES__ZENDESK__ZENDESK_SUPPORT__CREDENTIALS__EMAIL: ""{{ secret('ZENDESK_EMAIL') }}""
    script: |
      import dlt
      from zendesk import zendesk_support

      pipeline = dlt.pipeline(
          pipeline_name=""zendesk_pipeline"",
          destination=""duckdb"",
          dataset_name=""zendesk"",
      )

      zendesk_source = zendesk_support(load_all=False)
      tickets = zendesk_source.tickets

      load_info = pipeline.run(tickets)
",2023-10-10 12:21:19.825,Ingest
202,"Purge execution data including logs, metrics and outputs on a schedule","[""io.kestra.core.tasks.storages.Purge"", ""io.kestra.core.models.triggers.types.Schedule""]","This flow will [purge all execution data](https://kestra.io/plugins/core/tasks/storages/io.kestra.core.tasks.storages.purge) older than 1 month, including logs, metrics and outputs. In this specific flow, we chose to purge only the successful executions in order to keep the audit trail of failed executions. However, you can leave the `states` field empty to purge all execution data, regardless of the execution status, or adjust it to your needs.

It is recommended to run this flow on a [monthly schedule](https://kestra.io/docs/developer-guide/triggers/schedule) to keep your Kestra instance clean and save storage space.

Note that this flow will not purge the flow definitions or the [namespace files](https://kestra.io/docs/developer-guide/namespace-files) — your code will be safe. Only the execution data will be purged.
","id: purge_storage
namespace: system

tasks:
  - id: clean_up_storage
    type: io.kestra.core.tasks.storages.Purge
    endDate: ""{{ trigger.date | dateAdd(-1, 'MONTHS') }}""
    purgeExecution: true
    purgeLog: true
    purgeMetric: true
    purgeStorage: true
    states:
      - SUCCESS

triggers:
  - id: monthly
    type: io.kestra.core.models.triggers.types.Schedule
    cron: ""0 9 1 * *"" # every month at 9am on the 1st day of the month",2023-12-10 02:24:34.811,Trigger
9,Pass data between subflows — use outputs from the child flow in a parent flow,"[""io.kestra.core.tasks.flows.Flow"", ""io.kestra.core.tasks.log.Log""]","First, create a child flow:

```yaml
id: flow1
namespace: blueprint
tasks:
  - id: returnData
    type: io.kestra.core.tasks.debugs.Return
    format: this is a secret message returned from {{flow.id}}
```

Then, you can run this parent flow that will retrieve data from the subflow and store it under a specified key. In this example, the subflow uses the key `dataFromChildFlow`.","
id: passDataBetweenSubflows
namespace: blueprint

tasks:
  - id: flow1
    type: io.kestra.core.tasks.flows.Flow
    namespace: blueprint
    flowId: flow1
    wait: true
    outputs:
      dataFromChildFlow: ""{{outputs.returnData.value}}""
  
  - id: log
    type: io.kestra.core.tasks.log.Log
    message: ""{{outputs.flow1.outputs.dataFromChildFlow}}""",2023-07-26 14:51:44.863,Outputs
196,Add a parametrized Python script as a Namespace File and run it in parallel in Docker containers,"[""io.kestra.core.tasks.flows.EachParallel"", ""io.kestra.core.tasks.flows.WorkingDirectory"", ""io.kestra.plugin.scripts.python.Commands""]","Add a Python script called `parametrized.py` as a Namespace File and run it in parallel with different parameter `values` using a Python script added as a Namespace File. 

Here is the content of the `parametrized.py` script:

```python
import argparse

parser = argparse.ArgumentParser()

parser.add_argument(""--num"", type=int, default=42, help=""Enter an integer"")

args = parser.parse_args()
result = args.num * 2
print(result)
```

You can add that file directly from the [embedded Visual Studio Code Editor](https://kestra.io/docs/developer-guide/namespace-files) in the Kestra UI. ","id: parallel_python
namespace: blueprint

tasks:
  - id: parallel
    type: io.kestra.core.tasks.flows.EachParallel
    value: [1, 2, 3, 4, 5, 6, 7, 8, 9]
    tasks:
      - id: python
        type: io.kestra.plugin.scripts.python.Commands
        namespaceFiles: 
          enabled: true
        runner: DOCKER
        docker:
          image: ghcr.io/kestra-io/pydata:latest
        commands:
          - python parametrized.py --num {{ workerTaskrun.value }}",2023-11-17 11:05:35.327,"Docker,Python,Namespace Files"
120,"Run a dbt job and Hightouch sync, send a Telegram message on workflow completion","[""io.kestra.core.tasks.flows.WorkingDirectory"", ""io.kestra.plugin.git.Clone"", ""io.kestra.plugin.dbt.cli.DbtCLI"", ""io.kestra.plugin.hightouch.Sync"", ""io.kestra.plugin.notifications.telegram.TelegramSend""]","This flow shows how to run a dbt job and Hightouch Reverse ETL.

Send a notification into a dedicated Telegram channel to let users know data has been updated.","id: elt_notification_telegram
namespace: blueprint
tasks:
  - id: workflow
    type: io.kestra.core.tasks.flows.WorkingDirectory
    tasks:
    - id: cloneRepository
      type: io.kestra.plugin.git.Clone
      url: https://github.com/kestra-io/dbt-demo
      branch: main

    - id: dbt
      type: io.kestra.plugin.dbt.cli.DbtCLI
      docker:
        image: ghcr.io/kestra-io/dbt-snowflake:latest
      profiles: |
        jaffle_shop:
          outputs:
            dev:
              type: snowflake
              account: ""{{ secret('SNOWFLAKE_ACCOUNT') }}""
              user: ""{{ secret('SNOWFLAKE_USER') }}""
              password: ""{{ secret('SNOWFLAKE_PASSWORD') }}""
              role: ""{{ secret('SNOWFLAKE_ROLE') }}""
              database: ""{{ secret('SNOWFLAKE_DATABASE') }}""
              warehouse: COMPUTE_WH
              schema: public
              threads: 4
              query_tag: dbt
              client_session_keep_alive: False
              connect_timeout: 10
          target: dev
      commands:
        - dbt run

  - id: hightouch
    type: io.kestra.plugin.hightouch.Sync
    token: '{{ secret(""HIGHTOUCH_API_TOKEN"") }}'
    syncId: 1716609

  - id: telegram_notification
    type: io.kestra.plugin.notifications.telegram.TelegramSend
    channel: ""@customer_third_party_channel""
    token: ""{{ secret('TELEGRAM_BOT_TOKEN') }}""
    payload: ""Data has been synced to your platform""",2023-08-23 09:23:03.769,"dbt,API,Snowflake,Notifications"
15,Log content in the console,"[""io.kestra.core.tasks.log.Log""]",A simple example to show how to display message in the console.,"id: log
namespace: blueprint

tasks:
  - id: log
    type: io.kestra.core.tasks.log.Log
    message: Hello world!",2023-07-26 18:59:42.131,
79,Query data in Snowflake,"[""io.kestra.plugin.jdbc.snowflake.Query""]","This flow runs a query within a Snowflake data warehouse. The `fetchOne` property will retrieve only the first row, while using the `fetch` property will retrieve all rows. Setting `store: true` will provide the results as a downloadable file.

The flow assumes the password is stored as a secret.","id: snowflakeQuery
namespace: blueprint

tasks:
  - id: query
    type: io.kestra.plugin.jdbc.snowflake.Query
    url: jdbc:snowflake://accountID.snowflakecomputing.com?warehouse=COMPUTE_WH
    username: yourSnowflakeUser
    password: ""{{ secret('SNOWFLAKE_PASSWORD') }}""
    fetchOne: true
    sql: |
      SELECT * FROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.CUSTOMER
",2023-07-26 00:33:53.423,"SQL,Snowflake"
48,CI/CD to deploy Kestra flows using a GitHub webhook trigger,"[""io.kestra.core.tasks.flows.WorkingDirectory"", ""io.kestra.plugin.git.Clone"", ""io.kestra.plugin.scripts.shell.Commands"", ""io.kestra.core.models.triggers.types.Webhook""]","This blueprint demonstrates how to use Kestra to build a CI/CD pipeline.

This flow:
- deploys other flows, effectively running a CI/CD pipeline for Kestra flows directly from Kestra itself
- clones a GitHub repository, validates the flows, and then deploys them to a given Kestra namespace
- uses a [webhook trigger](https://kestra.io/docs/developer-guide/triggers/webhook) so that the flow can be triggered upon GitHub push event sent by a GitHub webhook. 

To learn more about this CI/CD pattern, check the following [documentation page](https://kestra.io/docs/developer-guide/cicd#deploy-flows-from-a-flow) and [blog post](https://levelup.gitconnected.com/when-github-actions-get-painful-to-troubleshoot-try-this-instead-9a134c9e9baf).","id: ci-cd
namespace: blueprint

tasks:
  - id: deploy
    type: io.kestra.core.tasks.flows.WorkingDirectory
    tasks:
      - id: cloneRepository
        type: io.kestra.plugin.git.Clone
        url: https://github.com/anna-geller/kestra-ci-cd
        branch: main

      - id: validateFlows
        type: io.kestra.plugin.scripts.shell.Commands
        runner: PROCESS
        commands:
          - /app/kestra flow validate flows/

      - id: deployFlows
        type: io.kestra.plugin.scripts.shell.Commands
        runner: PROCESS
        commands:
          - /app/kestra flow namespace update prod flows/prod/ --no-delete
          - /app/kestra flow namespace update prod.marketing flows/prod.marketing/ --no-delete

triggers:
  - id: github
    type: io.kestra.core.models.triggers.types.Webhook",2023-07-27 00:18:39.493,"Git,Trigger"
26,Download a file and upload it to S3,"[""io.kestra.plugin.fs.http.Download"", ""io.kestra.plugin.aws.s3.Upload""]","This flow downloads a single file and uploads it to an S3 bucket. 

This flow assumes AWS credentials stored as secrets `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`.
","id: uploadFileToS3
namespace: blueprint

inputs:
  - name: bucket
    type: STRING
    defaults: declarative-data-orchestration
  - name: fileURL
    type: STRING
    defaults: https://wri-dataportal-prod.s3.amazonaws.com/manual/global_power_plant_database_v_1_3.zip

tasks:
  - id: downloadFile
    type: io.kestra.plugin.fs.http.Download
    uri: ""{{inputs.fileURL}}""

  - id: uploadToS3
    type: io.kestra.plugin.aws.s3.Upload
    from: ""{{outputs.downloadFile.uri}}""
    key: powerplant/global_power_plant_database.zip
    bucket: ""{{inputs.bucket}}""
    region: ""{{ secret('AWS_DEFAULT_REGION') }}""
    accessKeyId: ""{{ secret('AWS_ACCESS_KEY_ID') }}""
    secretKeyId: ""{{ secret('AWS_SECRET_ACCESS_KEY') }}""
",2023-07-26 20:14:13.739,"Inputs,S3"
85,Create custom outputs from a Shell script,"[""io.kestra.plugin.scripts.shell.Commands"", ""io.kestra.core.tasks.debugs.Return""]","This blueprint shows how to expose custom outputs within a shell script.

The `::{""outputs"":{""test"":""value""}}::` allow to expose your data in task output. Those outputs are accessible through the `{{ outputs.<taskid>.vars.<your_output> }}` command in other tasks.","id: outputsFromShellCommands
namespace: demo

tasks:
  - id: process
    type: io.kestra.plugin.scripts.shell.Commands
    commands:
      - echo '::{""outputs"":{""test"":""value"",""int"":2,""bool"":true,""float"":3.65}}::'

  - id: return
    type: io.kestra.core.tasks.debugs.Return
    format: '{{ outputs.process.vars.test }}'",2023-07-26 19:01:20.532,Outputs
72,Load and search data using Elasticsearch,"[""io.kestra.plugin.fs.http.Request"", ""io.kestra.plugin.elasticsearch.Put"", ""io.kestra.core.tasks.flows.Parallel"", ""io.kestra.plugin.elasticsearch.Search"", ""io.kestra.plugin.elasticsearch.Scroll""]","This flow will extract data from the Pokemon API and will load it to a given Elasticsearch index.

To validate that data has been successfully stored and indexed in Elasticsearch, you can perform search looking up that Pokemon by name.","id: elasticsearch
namespace: blueprint

inputs:
  - name: pokemon
    type: STRING
    defaults: jigglypuff

variables:
  host: http://host.docker.internal:9200

tasks:
  - id: extract
    type: io.kestra.plugin.fs.http.Request
    uri: https://pokeapi.co/api/v2/pokemon/{{inputs.pokemon}}
    method: GET

  - id: load
    type: io.kestra.plugin.elasticsearch.Put
    connection:
      hosts:
        - ""{{vars.host}}""
    index: ""local""
    key: ""{{inputs.pokemon}}""
    value: ""{{outputs.extract.body}}""
  
  - id: parallel
    type: io.kestra.core.tasks.flows.Parallel
    tasks:
      - id: search
        type: io.kestra.plugin.elasticsearch.Search
        connection:
          hosts:
            - ""{{vars.host}}""
        indexes:
          - local
        request:
          query: 
            term:
              name:
                value: ""{{inputs.pokemon}}""
      
      - id: scroll
        type: io.kestra.plugin.elasticsearch.Scroll
        connection:
          hosts:
            - ""{{vars.host}}""
        indexes:
          - local
        request:
          query:
            term:
              name:
                value: ""{{inputs.pokemon}}""",2023-07-26 00:34:57.349,"Ingest,Parallel,Inputs"
95,Run a task on an on-demand Databricks cluster,"[""io.kestra.plugin.databricks.cluster.CreateCluster"", ""io.kestra.plugin.databricks.job.CreateJob"", ""io.kestra.plugin.databricks.cluster.DeleteCluster""]","This flow will create a Databricks cluster, run a task on the cluster, and then delete the cluster.
- The Python script referenced on `pythonFile` property is stored in the Databricks workspace.
- The flow will run the script on a Databricks cluster and wait up to five minutes (as declared on the `waitForCompletion` property) for the task to complete.

Even if the job fails, the `AllowFailure` tasks ensures that Databricks cluster will be deleted in the end.","id: onDemandClusterJob
namespace: blueprint

tasks:
  - id: create_cluster
    type: io.kestra.plugin.databricks.cluster.CreateCluster
    authentication:
      token: ""{{ secret('DATABRICKS_TOKEN') }}""
    host: ""{{ secret('DATABRICKS_HOST') }}""
    clusterName: kestra-demo
    nodeTypeId: n2-highmem-4
    numWorkers: 1
    sparkVersion: 13.0.x-scala2.12

  - id: allow_failure
    type: io.kestra.core.tasks.flows.AllowFailure
    tasks:
      - id: run_job
        type: io.kestra.plugin.databricks.job.CreateJob
        authentication:
          token: ""{{ secret('DATABRICKS_TOKEN') }}""
        host: ""{{ secret('DATABRICKS_HOST') }}""
        jobTasks:
          - existingClusterId: ""{{outputs.createCluster.clusterId}}""
            taskKey: yourArbitraryTaskKey
            sparkPythonTask:
              pythonFile: /Shared/hello.py
              sparkPythonTaskSource: WORKSPACE
        waitForCompletion: PT5M
    
  - id: delete_cluster
    type: io.kestra.plugin.databricks.cluster.DeleteCluster
    authentication:
      token: ""{{ secret('DATABRICKS_TOKEN') }}""
    host: ""{{ secret('DATABRICKS_HOST') }}""
    clusterId: ""{{outputs.createCluster.clusterId}}""",2023-08-27 14:35:15.079,Databricks
57,Conditional branching based on a parameter value,"[""io.kestra.core.tasks.flows.If"", ""io.kestra.core.tasks.log.Log""]","This flow takes an optional input parameter. It then executes subsequent tasks based on whether the input was provided or not.
","id: conditionalBranching
namespace: blueprint

inputs:
  - name: parameter
    type: STRING
    required: false

tasks:
  - id: if
    type: io.kestra.core.tasks.flows.If
    condition: ""{{inputs.customInput ?? false }}""
    then:
      - id: if-not-null
        type: io.kestra.core.tasks.log.Log
        message: Received input {{inputs.parameter}}
    else:
      - id: if-null
        type: io.kestra.core.tasks.log.Log
        message: No input provided",2023-07-27 00:30:15.016,"Inputs,Variables"
61,Trigger a single Airbyte sync on schedule,"[""io.kestra.plugin.airbyte.connections.Sync"", ""io.kestra.core.models.triggers.types.Schedule""]","This flow will sync data using Airbyte.

It's best to provide Airbyte server credentials (the `username` and `password`) as secrets, but you can also paste them in plain text during testing.
","id: airbyteSync
namespace: blueprint
tasks:
  - id: data-ingestion
    type: io.kestra.plugin.airbyte.connections.Sync
    connectionId: e3b1ce92-547c-436f-b1e8-23b6936c12ab
    url: http://host.docker.internal:8000/
    username: ""{{ secret('AIRBYTE_USERNAME') }}""
    password: ""{{ secret('AIRBYTE_PASSWORD') }}""

triggers:
  - id: everyMinute
    type: io.kestra.core.models.triggers.types.Schedule
    cron: ""*/1 * * * *""",2023-07-26 00:43:37.769,"Schedule,Ingest"
81,"Snowflake ETL: load files to internal stage, copy from stage to a table and run analytical SQL queries ","[""io.kestra.plugin.jdbc.snowflake.Query"", ""io.kestra.plugin.fs.http.Download"", ""io.kestra.plugin.jdbc.snowflake.Upload""]","This flow is an end-to-end tutorial for Snowflake. It creates a new database and a table. It then extracts data from an external source, and loads that data as a CSV file into Snowflake's internal stage.

The CSV file uploaded to stage is then loaded into the table.

Finally, we do some analytics by aggregating (imaginary) new hires at Kestra over time. The final result is fetched into the Kestra's internal storage and converted to a CSV file that you can download from the Outputs tab on the Execution's page. ","id: snowflake
namespace: blueprint

tasks:
  - id: createDatabase
    type: io.kestra.plugin.jdbc.snowflake.Query
    sql: CREATE OR REPLACE DATABASE kestra;

  - id: createTable
    type: io.kestra.plugin.jdbc.snowflake.Query
    sql: |
      CREATE OR REPLACE TABLE KESTRA.PUBLIC.EMPLOYEES (
        first_name STRING ,
        last_name STRING ,
        email STRING ,
        streetaddress STRING ,
        city STRING ,
        start_date DATE
        );
  
  - id: extract
    type: io.kestra.plugin.fs.http.Download
    uri: https://raw.githubusercontent.com/kestra-io/examples/main/datasets/employees00.csv

  - id: loadToInternalStage
    type: io.kestra.plugin.jdbc.snowflake.Upload
    from: ""{{outputs.extract.uri}}""
    fileName: employees00.csv
    prefix: raw
    stageName: ""@kestra.public.%employees""
    compress: true

  - id: loadFromStageToTable
    type: io.kestra.plugin.jdbc.snowflake.Query
    sql: |
      COPY INTO KESTRA.PUBLIC.EMPLOYEES
      FROM @kestra.public.%employees
      FILE_FORMAT = (type = csv field_optionally_enclosed_by='""' skip_header = 1)
      PATTERN = '.*employees0[0-9].csv.gz'
      ON_ERROR = 'skip_file';
  
  - id: analyze
    type: io.kestra.plugin.jdbc.snowflake.Query
    description: Growth of new hires per month
    sql: |
      SELECT year(START_DATE) as year, monthname(START_DATE) as month, count(*) as nr_employees
      FROM kestra.public.EMPLOYEES
      GROUP BY year(START_DATE), monthname(START_DATE)
      ORDER BY nr_employees desc;
    store: true
  
  - id: csvReport
    type: io.kestra.plugin.serdes.csv.CsvWriter
    from: ""{{outputs.analyze.uri}}""

taskDefaults:
  - type: io.kestra.plugin.jdbc.snowflake.Query
    values:
      url: jdbc:snowflake://accountID.snowflakecomputing.com?warehouse=COMPUTE_WH
      username: yourSnowflakeUser
      password: ""{{ secret('SNOWFLAKE_PASSWORD') }}""

  - type: io.kestra.plugin.jdbc.snowflake.Upload
    values:
      url: jdbc:snowflake://accountID.snowflakecomputing.com?warehouse=COMPUTE_WH
      username: yourSnowflakeUser
      password: ""{{ secret('SNOWFLAKE_PASSWORD') }}""",2023-07-26 00:32:13.569,"SQL,Snowflake,Ingest"
77,Generate a CSV file report from a SQL query using Trino,"[""io.kestra.plugin.jdbc.trino.Query"", ""io.kestra.plugin.serdes.csv.CsvWriter""]","This flow queries data using Trino SQL and generates a downloadable CSV report.

To test this integration, you can start Trino in a Docker container:

```bash
docker run -d -p 8090:8080 --name trino trinodb/trino
```","id: trinoQuery
namespace: blueprint

tasks:  
  - id: analyzeOrders
    type: io.kestra.plugin.jdbc.trino.Query
    url: jdbc:trino://host.docker.internal:8090/tpch
    username: trino
    sql: |
      select orderpriority as priority, sum(totalprice) as total
      from tpch.tiny.orders
      group by orderpriority
      order by orderpriority
    store: true
  
  - id: csvReport
    type: io.kestra.plugin.serdes.csv.CsvWriter
    from: ""{{outputs.analyzeOrders.uri}}""


",2023-07-26 00:16:29.169,"Outputs,SQL"
30,"Trigger multiple Airbyte syncs, then run a dbt job","[""io.kestra.core.tasks.flows.Parallel"", ""io.kestra.plugin.airbyte.connections.Sync"", ""io.kestra.core.tasks.flows.WorkingDirectory"", ""io.kestra.plugin.git.Clone"", ""io.kestra.plugin.dbt.cli.Build""]","This flow executes several Airbyte syncs in parallel and then runs dbt Core's CLI commands. 

It's recommended to set GCP and Airbyte server credentials as secrets.","id: airbyteSyncParallelWithDbt
namespace: blueprint

tasks:
  - id: data-ingestion
    type: io.kestra.core.tasks.flows.Parallel
    tasks:
      - id: salesforce
        type: io.kestra.plugin.airbyte.connections.Sync
        connectionId: e3b1ce92-547c-436f-b1e8-23b6936c12ab

      - id: google-analytics
        type: io.kestra.plugin.airbyte.connections.Sync
        connectionId: e3b1ce92-547c-436f-b1e8-23b6936c12cd

      - id: facebook-ads
        type: io.kestra.plugin.airbyte.connections.Sync
        connectionId: e3b1ce92-547c-436f-b1e8-23b6936c12ef

  - id: dbt
    type: io.kestra.core.tasks.flows.WorkingDirectory
    tasks:
    - id: cloneRepository
      type: io.kestra.plugin.git.Clone
      url: https://github.com/kestra-io/dbt-demo
      branch: main
    
    - id: dbt-build
      type: io.kestra.plugin.dbt.cli.Build
      runner: DOCKER
      dbtPath: /usr/local/bin/dbt
      dockerOptions:
        image: ghcr.io/kestra-io/dbt-bigquery:latest
      inputFiles:
        .profile/profiles.yml: |
          jaffle_shop:
            outputs:
              dev:
                type: bigquery
                dataset: your_big_query_dataset_name
                project: your_big_query_project
                fixed_retries: 1
                keyfile: sa.json
                location: EU
                method: service-account
                priority: interactive
                threads: 8
                timeout_seconds: 300
            target: dev
        sa.json: ""{{ secret('GCP_CREDS') }}""

taskDefaults:
  - type: io.kestra.plugin.airbyte.connections.Sync
    values:
      url: http://host.docker.internal:8000/
      username: ""{{ secret('AIRBYTE_USERNAME') }}""
      password: ""{{ secret('AIRBYTE_PASSWORD') }}""
",2023-07-26 23:33:13.67,"Ingest,Git,BigQuery,dbt,Parallel"
21,Error handling: send Slack alert on failure,"[""io.kestra.plugin.scripts.shell.Commands"", ""io.kestra.plugin.notifications.slack.SlackIncomingWebhook""]",This flow will fail and the `errors` section declares tasks that should run on failure.,"id: onFailureAlert
namespace: blueprint

tasks:
  - id: fail
    type: io.kestra.plugin.scripts.shell.Commands
    runner: PROCESS
    commands:
      - exit 1

errors:
  - id: slack
    type: io.kestra.plugin.notifications.slack.SlackIncomingWebhook
    url: ""{{ secret('SLACK_WEBHOOK') }}""
    payload: |
      {
        ""channel"": ""#alerts"",
        ""text"": ""Failure alert for flow {{ flow.namespace }}.{{ flow.id }} with ID {{ execution.id }}""
      }",2023-07-26 18:59:58.904,Notifications
131,Build and push a docker image to DockerHub,"[""io.kestra.plugin.docker.Build""]","This flow will build a Docker image and push it to a remote container registry.

1. The `dockerfile` parameter is a multiline string that contains the Dockerfile content. However, it can also be a path to a file.
2. The `tags` parameter is a list of tags of the image to build. Make sure to replace the `kestra` prefix with your DockerHub ID.
3. The `push` parameter is a boolean that indicates whether to push the image to DockerHub.
4. Finally, make sure to securely store your DockerHub credentials as secrets or environment variables.","id: build_dockerhub_image
namespace: blueprint

tasks:
  - id: build
    type: io.kestra.plugin.docker.Build
    dockerfile: |
      FROM python:3.10
      RUN pip install --upgrade pip
      RUN pip install --no-cache-dir kestra requests ""polars[all]""
    tags: 
      - kestra/polars:latest
    push: true
    credentials:
      username: ""{{ secret('DOCKERHUB_USERNAME') }}""
      password: ""{{ secret('DOCKERHUB_PASSWORD') }}""",2023-09-22 11:50:13.634,Docker
3,Run multiple subflows in parallel and wait for their completion - use taskDefaults to avoid boilerplate code,"[""io.kestra.core.tasks.flows.Parallel"", ""io.kestra.core.tasks.flows.Flow""]","Add the child flows first: 

First flow:    
```yaml
id: flow1
namespace: blueprint
tasks:
- id: get
  type: io.kestra.core.tasks.debugs.Return
  format: hi from {{flow.id}}
```

Second flow:
```yaml
id: flow2
namespace: blueprint
tasks:
- id: get
  type: io.kestra.core.tasks.debugs.Return
  format: hi from {{flow.id}}
```

Third flow:
```yaml
id: flow3
namespace: blueprint
tasks:
- id: get
  type: io.kestra.core.tasks.debugs.Return
  format: hi from {{flow.id}}
```
  
Then run the parent flow `parallelSubflows` to trigger multiple subflows in parallel.","id: parallelSubflows
namespace: blueprint

tasks:
  - id: parallel
    type: io.kestra.core.tasks.flows.Parallel
    tasks:
      - id: flow1
        type: io.kestra.core.tasks.flows.Flow
        flowId: flow1
      - id: flow2
        type: io.kestra.core.tasks.flows.Flow
        flowId: flow2
      - id: flow3
        type: io.kestra.core.tasks.flows.Flow
        flowId: flow3
        
taskDefaults:
  - type: io.kestra.core.tasks.flows.Flow
    values:
      namespace: blueprint
      wait: true
      transmitFailed: true
",2023-09-13 16:42:43.535,Parallel
198,"Create a Python subflow, acting like an abstracted component","[""io.kestra.plugin.scripts.python.Commands""]","This Flow shows how you can create a templated flow (subflow) to run a custom script.

This flow can used in another flow, actin like a separated component. We can imagine having a complex flow, with many tasks but abstracted with inputs and outputs, so users can only deal with a simple interface.
Here is an example of calling this flow, giving inputs and retrieve the desired outputs.


```
id: call_python_component
namespace: dev
tasks:
  - id: python
    type: io.kestra.core.tasks.flows.Flow
    namespace: dev
    flowId: python_subflow_component
    wait: true
    inputs:
      arg1: 5
      arg2: 3
    outputs:
      result: '{{ outputs.python.vars.result }}'
```","id: python_subflow_component
namespace: dev

inputs:
  - name: arg1
    type: INT

  - name: arg2
    type: INT

tasks:

  - id: python
    type: io.kestra.plugin.scripts.python.Commands
    docker:
        image: ghcr.io/kestra-io/pydata:latest
    inputFiles:
      main.py: |
        import argparse
        from kestra import Kestra

        def multiply_arguments(arg1, arg2):
            return arg1 * arg2

        if __name__ == ""__main__"":
            parser = argparse.ArgumentParser(description='Multiply two integers.')
            parser.add_argument('--arg1', type=int, help='First integer argument')
            parser.add_argument('--arg2', type=int, help='Second integer argument')
            args = parser.parse_args()

            result = multiply_arguments(args.arg1, args.arg2)
            print(f""The product of {args.arg1} and {args.arg2} is: {result}"")
            Kestra.outputs({'result': result})
    commands:
      - python main.py --arg1 {{ inputs.arg1 }} --arg2 {{ inputs.arg2 }}",2023-11-16 14:52:09.644,"Inputs,Outputs,Python"
17,Send a Slack message via incoming webhook,"[""io.kestra.plugin.notifications.slack.SlackIncomingWebhook""]",Send messages through [Slack Incoming Webhook](https://api.slack.com/messaging/webhooks).,"id: slack_incoming_webhook
namespace: blueprint

tasks:

  - id: slack
    type: io.kestra.plugin.notifications.slack.SlackIncomingWebhook
    url: ""{{ secret('SLACK_WEBHOOK') }}""
    payload: |
      {
        ""channel"": ""#alerts"",
        ""text"": ""Flow {{ flow.namespace }}.{{ flow.id }} started with execution {{ execution.id }}""
      }",2023-07-17 14:18:10.974,Notifications
28,"Trigger multiple Fivetran syncs in parallel, then run a dbt Cloud job","[""io.kestra.core.tasks.flows.Parallel"", ""io.kestra.plugin.fivetran.connectors.Sync"", ""io.kestra.plugin.dbt.cloud.TriggerRun""]","This flow will sync data from multiple sources using Fivetran and then trigger a dbt Cloud job.

Given that you may have tens or hundreds of Fivetran connectors that need to be synced before running dbt, `taskDefaults` property is used to avoid boilerplate configuration.

The `wait` flag on the dbt Cloud job task is set to `true` allowing to capture logs and job status and ensuring that dynamically generated tasks (based on dbt models and tests) will be displayed in the Gantt view. This will also ensure that Kestra will wait for the dbt Cloud job to finish before continuing any downstream tasks, and it will fail the execution if dbt Cloud job fails.","id: fivetranDbtCloud
namespace: blueprint

tasks:
  - id: fivetran-syncs
    type: io.kestra.core.tasks.flows.Parallel
    tasks:
      - id: salesforce
        type: io.kestra.plugin.fivetran.connectors.Sync
        connectorId: ""enterYourFivetranConnectorId""

      - id: google-analytics
        type: io.kestra.plugin.fivetran.connectors.Sync
        connectorId: ""enterYourFivetranConnectorId""

      - id: facebook
        type: io.kestra.plugin.fivetran.connectors.Sync
        connectorId: ""enterYourFivetranConnectorId""

  - id: dbt-cloud-job
    type: io.kestra.plugin.dbt.cloud.TriggerRun
    jobId: ""396284""
    accountId: ""{{ secret('DBT_CLOUD_ACCOUNT_ID') }}""
    token: ""{{ secret('DBT_CLOUD_API_TOKEN') }}""
    wait: true

taskDefaults:
  - type: io.kestra.plugin.fivetran.connectors.Sync
    values:
      apiKey: ""{{ secret('FIVETRAN_API_KEY') }}""
      apiSecret: ""{{ secret('FIVETRAN_API_SECRET') }}""
",2023-07-26 20:27:51.363,"Ingest,dbt,Parallel"
37,AWS S3 Event Trigger ,"[""io.kestra.core.tasks.flows.EachParallel"", ""io.kestra.core.tasks.debugs.Return"", ""io.kestra.plugin.aws.s3.Trigger""]","This flow will be triggered whenever new files with a given prefix are detected in the specified S3 bucket. It will download the files into the internal storage 
and move the S3 objects to an `archive` folder (i.e. S3 object prefix with the name `archive`).

The `EachParallel` task will iterate over the objects and print their URIs.

It's recommended to set the `accessKeyId` and `secretKeyId` properties as secrets.","id: s3Trigger
namespace: blueprint

tasks:
  - id: each
    type: io.kestra.core.tasks.flows.EachParallel
    tasks:
      - id: s3object
        type: io.kestra.core.tasks.debugs.Return
        format: ""{{taskrun.value}}""
    value: ""{{ trigger.objects | jq('.[].uri') }}""

triggers:
  - id: waitForS3object
    type: io.kestra.plugin.aws.s3.Trigger
    bucket: declarative-orchestration
    prefix: demo
    interval: PT1S
    filter: FILES
    action: MOVE
    moveTo:
      key: archive/demo/
    region: ""{{ secret('AWS_DEFAULT_REGION') }}""
    accessKeyId: ""{{ secret('AWS_ACCESS_KEY_ID') }}""
    secretKeyId: ""{{ secret('AWS_SECRET_ACCESS_KEY') }}""
",2023-07-27 00:07:42.748,"Trigger,S3"
5,Read a file from inputs - a parametrized workflow with files input parameters,"[""io.kestra.plugin.scripts.shell.Commands""]",This example shows how to read a file from flow inputs.,"id: input_file
namespace: blueprint

inputs:
  - name: text_file
    type: FILE

tasks:
  - id: read_file
    type: io.kestra.plugin.scripts.shell.Commands
    runner: PROCESS
    commands:
      - cat ""{{ inputs.text_file }}""
",2023-07-26 18:58:35.035,
104,Send a prompt to OpenAI's ChatCompletion API,"[""io.kestra.plugin.openai.ChatCompletion"", ""io.kestra.core.tasks.log.Log""]","This flow will send a prompt to OpenAI. You can select the desired model and additional configuration such as temperature. 

The next task shows how you can retrieve the message content from the API response. ","id: OpenAI
namespace: blueprint

tasks:
  - id: prompt
    type: io.kestra.plugin.openai.ChatCompletion
    apiKey: ""{{ secret('OPENAI_API_KEY') }}""
    model: gpt-4
    prompt: Explain in one sentence why data engineers build data pipelines

  - id: useOutput
    type: io.kestra.core.tasks.log.Log
    message: ""{{outputs.prompt.choices | jq('.[].message.content') | first }}""",2023-07-20 16:18:23.294,AI
56,GPU-accelerated Python script with Modal,"[""io.kestra.core.tasks.flows.WorkingDirectory"", ""io.kestra.plugin.git.Clone"", ""io.kestra.plugin.scripts.shell.Commands""]","This flow clones a repository, pulls a public container image with the required dependencies and runs a Python script in a Docker container.
  
The Python script uses Modal Python Client to run the script on a GPU-enabled server. 
- To learn more about GPU support on modal.com, check the [Modal documentation](https://modal.com/docs/guide/gpu)
- The Python script is available in the repository: [kestra-io/scripts](https://github.com/kestra-io/scripts/blob/main/modal/gpu.py)

This flow assumes that you have a Modal account and that you stored the Modal credentials as secrets.","id: gpuModal
namespace: blueprint

tasks:
  - id: pythonRepository
    type: io.kestra.core.tasks.flows.WorkingDirectory
    tasks:
      - id: cloneRepository
        type: io.kestra.plugin.git.Clone
        branch: main
        url: https://github.com/kestra-io/scripts

      - id: runModal
        type: io.kestra.plugin.scripts.shell.Commands
        commands:
          - modal run modal/gpu.py
        runner: DOCKER
        docker:
          image: ghcr.io/kestra-io/modal:latest
        env:
          MODAL_TOKEN_ID: ""{{ secret('MODAL_TOKEN_ID') }}""
          MODAL_TOKEN_SECRET: ""{{ secret('MODAL_TOKEN_SECRET') }}""
",2023-07-25 18:08:50.469,"Python,CLI"
151,Extract data from a REST API and send it to a Kafka topic using the Kafka producer task,"[""io.kestra.plugin.fs.http.Request"", ""io.kestra.plugin.kafka.Produce""]","This flow shows how to extract data from an HTTP API and send it to a Kafka topic using the Kafka plugin. It assumes that you have a Kafka cluster running, and that you created a topic named `mytopic`. Make sure to replace the `bootstrap.servers` value with your Kafka cluster URL.

The `from` argument expects a map or a list of maps with key-value pairs. The allowed keys are: `key`, `value`, `partition`, `timestamp`, and `headers`.

In this example, we're using the `outputs.api.body` value, which is a JSON-formatted response body from the `api` task. This is why the `valueSerializer` argument is set to `JSON`.
","id: produce_kafka_message
namespace: blueprint

tasks:
  - id: api
    type: io.kestra.plugin.fs.http.Request
    uri: https://dummyjson.com/products

  - id: produce
    type: io.kestra.plugin.kafka.Produce
    from:
      key: mykey
      value: ""{{ outputs.api.body }}""
      timestamp: ""{{ execution.startDate }}""
      headers:
        x-header: some value
    keySerializer: STRING
    valueSerializer: JSON
    topic: mytopic
    properties:
      bootstrap.servers: my.kafka.k8s.com:9094
",2023-10-12 15:43:17.497,"Queue,Variables,API"
91,Download a Parquet file from Databricks and use it in a Python script,"[""io.kestra.plugin.databricks.dbfs.Download"", ""io.kestra.plugin.scripts.python.Script""]","This flow will:
1. Download a Parquet file from Databricks File System (DBFS) to Kestra's internal storage 
2. Use the file in a Python script running in a container","id: downloadParquetFromDatabricks
namespace: blueprint
description: |
  This flow will download a Parquet file from Databricks File System (DBFS) to Kestra's internal storage.

tasks:
  - id: download
    type: io.kestra.plugin.databricks.dbfs.Download
    authentication:
      token: ""{{ secret('DATABRICKS_TOKEN') }}""
    host: ""{{ secret('DATABRICKS_HOST') }}""
    from: /Share/myFile.parquet

  - id: processDownloadedFile
    type: io.kestra.plugin.scripts.python.Script
    docker:
      image: ghcr.io/kestra-io/pydata:latest
    script: |
      import pandas as pd

      df = pd.read_parquet(""{{outputs.download.uri}}"")
      df.head()
",2023-07-25 23:55:58.958,"Outputs,Python,Databricks"
74,Add multiple Redis keys in parallel from JSON input,"[""io.kestra.core.tasks.flows.EachParallel"", ""io.kestra.plugin.redis.Set""]",This flow adds multiple keys in parallel to a Redis data store based on JSON input provided by the user at runtime.,"id: redis_set_parallel
namespace: blueprint

inputs:
  - name: values
    type: JSON
    description: Enter your favorite plugins and tasks
    defaults: |
      [
        {""dbt"": [""build"", ""test"", ""snapshot""]},
        {""aws"": [""s3"", ""sqs"", ""sns"", ""athena""]},
        {""gcp"": [""big-query"", ""gcs"", ""cloudrun""]}
      ]

tasks:
  - id: parallel
    type: io.kestra.core.tasks.flows.EachParallel
    value: ""{{inputs.values}}""
    tasks:
      - id: set
        type: ""io.kestra.plugin.redis.Set""
        url: redis://host.docker.internal:6379/0
        serdeType: STRING
        key: ""{{json(taskrun.value) | keys | first}}""
        value: |
          {{ taskrun.value | jq('.[]') | first }}",2023-10-12 15:44:21.845,"Ingest,Parallel"
54,Pandas ETL - passing data between Python script tasks running in separate containers,"[""io.kestra.core.tasks.flows.WorkingDirectory"", ""io.kestra.plugin.scripts.python.Script""]","This flow demonstrates how to use the `WorkingDirectory` task to persist data between multiple Python script tasks running in separate containers.

The first task stores the data as a CSV file called ""raw_data.csv"". The second task loads the CSV file, transforms it and outputs the final CSV file to Kestra's internal storage using the `{{outputDir}}` property. You can download that file from the Outputs tab on the Execution's page. 

Kestra's internal storage allows you to use the output in other tasks in the flow, even if those tasks are processed in different containers. The final result file will also be available as a downloadable artifact, making it easier to share the results of your data workflow with various business stakeholders.","id: wdirPandasPythonOutputs
namespace: blueprint

tasks:
  - id: ETL
    type: io.kestra.core.tasks.flows.WorkingDirectory
    tasks:
      - id: extractCsv
        type: io.kestra.plugin.scripts.python.Script
        warningOnStdErr: false
        docker:
          image: ghcr.io/kestra-io/pydata:latest
        script: |
          import pandas as pd
          data = {
              'Column1': ['A', 'B', 'C', 'D'],
              'Column2': [1, 2, 3, 4],
              'Column3': [5, 6, 7, 8]
          }
          df = pd.DataFrame(data)
          print(df.head())
          df.to_csv(""raw_data.csv"", index=False)

      - id: transformAndLoadCsv
        type: io.kestra.plugin.scripts.python.Script
        warningOnStdErr: false
        docker:
          image: ghcr.io/kestra-io/pydata:latest
        script: |
          import pandas as pd
          df = pd.read_csv(""raw_data.csv"")
          df['Column4'] = df['Column2'] + df['Column3']
          print(df.head())
          df.to_csv(""{{outputDir}}/final.csv"", index=False)
",2023-07-25 23:04:13.874,"Python,Outputs"
29,"Trigger multiple Fivetran syncs in parallel, then run a dbt job","[""io.kestra.core.tasks.flows.Parallel"", ""io.kestra.plugin.fivetran.connectors.Sync"", ""io.kestra.core.tasks.flows.WorkingDirectory"", ""io.kestra.plugin.git.Clone"", ""io.kestra.plugin.dbt.cli.Build""]","This flow runs Fivetran syncs in parallel and then runs dbt core's CLI commands.
The flow assumes that you store the Fivetran API credentials, referenced in the `taskDefaults`, as secrets.","id: fivetranSyncParallelDbtCore
namespace: blueprint

tasks:
  - id: dataIngestion
    type: io.kestra.core.tasks.flows.Parallel
    tasks:
      - id: salesforce
        type: io.kestra.plugin.fivetran.connectors.Sync
        connectorId: vesicle_movement

      - id: stripe
        type: io.kestra.plugin.fivetran.connectors.Sync
        connectorId: cell_delivery

      - id: google-analytics
        type: io.kestra.plugin.fivetran.connectors.Sync
        connectorId: equivocal_sandy

      - id: facebook-ads
        type: io.kestra.plugin.fivetran.connectors.Sync
        connectorId: molecule_transport

  - id: dbtCore
    type: io.kestra.core.tasks.flows.WorkingDirectory
    tasks:
    - id: cloneRepository
      type: io.kestra.plugin.git.Clone
      url: https://github.com/kestra-io/dbt-demo
      branch: main

    - id: dbt-build
      type: io.kestra.plugin.dbt.cli.Build
      runner: DOCKER
      dbtPath: /usr/local/bin/dbt
      dockerOptions:
        image: ghcr.io/kestra-io/dbt-bigquery:latest
      inputFiles:
        .profile/profiles.yml: |
          jaffle_shop:
            outputs:
              dev:
                type: bigquery
                dataset: your_big_query_dataset_name
                project: your_big_query_project
                fixed_retries: 1
                keyfile: sa.json
                location: EU
                method: service-account
                priority: interactive
                threads: 8
                timeout_seconds: 300
            target: dev
        sa.json: ""{{ secret('GCP_CREDS') }}""

taskDefaults:
  - type: io.kestra.plugin.fivetran.connectors.Sync
    values:
      apiKey: ""{{ secret('FIVETRAN_API_KEY') }}""
      apiSecret: ""{{ secret('FIVETRAN_API_SECRET') }}""
",2023-07-26 21:28:27.599,"Parallel,Ingest,Git,BigQuery,dbt"
4,Read a Google Spreadsheet & Load it to BigQuery,"[""io.kestra.plugin.googleworkspace.sheets.Read"", ""io.kestra.plugin.serdes.csv.CsvWriter"", ""io.kestra.plugin.gcp.bigquery.Load""]"," Read a Google Spreadsheet and upload corresponding data to BigQuery.
 
To connect to Google Sheets, follow these steps:

- **Enable Google Sheets API:** If not done already, enable the Google Sheets API from the Google Cloud Console.
- **Create Service Account** & Download the Key:
    - Navigate to IAM & Admin → Service Accounts in Google Cloud Console.
    - Create a new service account and download the JSON key.
- **Share Sheet:** Open the Google Sheet and share it with the email address of the service account.

Then, make sure to add the `GOOGLE_APPLICATION_CREDENTIALS` environment variable with a value of the path to the JSON keyfile (note this must be a path to a file, not the contents of the Service Account).

Here is how to find the Spreadsheet ID: https://docs.google.com/spreadsheets/d/**your_id**/edit#gid=0","id: gsheet_to_bigquery
namespace: blueprint

tasks:
  - id: read_gsheet
    type: io.kestra.plugin.googleworkspace.sheets.Read
    description: Read data from Google Spreadsheet
    spreadsheetId: 1ybRy9G-sGznXI9GM6FEb0duQyByJbgq4LoYd7oYFr5c
    store: true
    valueRender: FORMATTED_VALUE
  
  - id: write_csv
    type: io.kestra.plugin.serdes.csv.CsvWriter
    description: Write CSV into Kestra internal storage
    from: ""{{ outputs.read_gsheet.uris.Sheet }}""
  
  - id: load_biqquery
    type: io.kestra.plugin.gcp.bigquery.Load
    description: Load data into BigQuery
    autodetect: true
    csvOptions:
      fieldDelimiter: "",""
    destinationTable: kestra-dev.demo.spotify_song_feature
    format: CSV
    from: ""{{ outputs.write_csv.uri }}""
",2023-10-04 20:41:49.855,"Ingest,GCP"
138,Logging configuration in a Python script using Loguru,"[""io.kestra.plugin.scripts.python.Script""]","This flow demonstrates how to configure logging in a Python script using [Loguru](https://github.com/Delgan/loguru). The `Script` task will generate, by default, 100 random log messages, but this number of logs can be changed at runtime using the input parameter `nr_logs`. 

- The `warningOnStdErr` property is set to `false` to prevent the `Script` task from failing when the `logger.warning` method is used. 
- The `docker.image` property is set to `ghcr.io/kestra-io/pydata:latest` to use the `pydata` Docker image that contains the `loguru` and `faker` libraries. 
- The `script` property contains the Python code that will be executed by the `Script` task. 

The log level is set to `INFO` in the `Script` task. Therefore, the `logger.debug` message will NOT show up in the logs. The `logger.warning` messages will be translated to WARN-level logs in Kestra. The `logger.info` messages will be translated to INFO-level logs in Kestra.","id: loguru
namespace: blueprints

inputs:
  - name: nr_logs
    type: INT
    defaults: 100

tasks:
  - id: reproducer
    type: io.kestra.plugin.scripts.python.Script
    warningOnStdErr: false
    docker:
      image: ghcr.io/kestra-io/pydata:latest
    script: |
        from loguru import logger
        from faker import Faker
        import time
        import sys

        logger.remove()
        logger.add(sys.stdout, level=""INFO"")
        logger.add(sys.stderr, level=""WARNING"")


        def generate_logs(fake, num_logs):
            logger.debug(""This message will not show up as the log level is set to INFO"")
            logger.warning(""Starting to generate log messages"")
            for _ in range(num_logs):
                log_message = fake.sentence()
                logger.info(log_message)
                time.sleep(0.01)
            logger.warning(""Finished generating log messages"")


        if __name__ == ""__main__"":
            faker_ = Faker()
            generate_logs(faker_, int(""{{ inputs.nr_logs }}""))
",2023-09-26 08:54:17.991,Python
34,Add a new item to a DynamoDB table,"[""io.kestra.plugin.aws.dynamodb.PutItem""]","This flow adds an item to a DynamoDB table.

The `item` property can be either a map or a JSON string.
The `tableName` property must point to an already existing DynamoDB table.
The `region` property must be a valid AWS region.
It's recommended to set the `accessKeyId` and `secretKeyId` properties as secrets.","id: add_items_to_dynamodb
namespace: blueprint

tasks:
  - id: firstItemAsMap
    type: io.kestra.plugin.aws.dynamodb.PutItem
    item:
      id: 1
      flow: ""{{ flow.id }}""
      task: ""{{ task.id }}""

  - id: secondItemAsJSON
    type: io.kestra.plugin.aws.dynamodb.PutItem
    item: |
        {
            ""id"": 2,
            ""flow"": ""{{ flow.id }}"",
            ""task"": ""{{ task.id }}""
        }

taskDefaults:
  - type: io.kestra.plugin.aws.dynamodb.PutItem
    values: 
      tableName: demo
      region: ""{{ secret('AWS_DEFAULT_REGION') }}""
      accessKeyId: ""{{ secret('AWS_ACCESS_KEY_ID') }}""
      secretKeyId: ""{{ secret('AWS_SECRET_ACCESS_KEY') }}""",2023-12-10 02:57:53.254,"Ingest,AWS"
96,Automate tasks on GCP using Google Cloud CLI,"[""io.kestra.plugin.gcp.cli.GCloudCLI""]","This flow demonstrates how you can use the Google Cloud CLI to automate various tasks on GCP. 

The task below runs commands to list storage buckets in a given GCP project using the `gcloud storage ls` command.
","id: gcloudCLI
namespace: blueprint

tasks:
  - id: hello
    type: io.kestra.plugin.gcp.cli.GCloudCLI
    serviceAccount: ""{{ secret('GCP_CREDS') }}""
    projectId: yourProject
    commands:
      - gcloud storage ls # plain text e.g. gs://bucket/
      - gcloud storage ls --json > {{outputDir}}/storage.json # output to file makes it available for further processing or download.
      - gcloud storage ls --json | tr -d '\n ' | xargs -0 -I {} echo '::{""outputs"":{""gcloud"":{}}}::' # output to Kestra output variables allows passing those variables to downstream tasks
",2023-07-20 12:29:47.69,GCP
83,Run Shell Scripts and Shell commands in a working directory using a PROCESS runner,"[""io.kestra.core.tasks.flows.WorkingDirectory"", ""io.kestra.plugin.scripts.shell.Script"", ""io.kestra.plugin.scripts.shell.Commands""]",This flow sequentially executes Shell Scripts and Shell Commands in the same working directory using a local `PROCESS`.,"id: shellScripts
namespace: blueprint

tasks:
  - id: workingDirectory
    type: io.kestra.core.tasks.flows.WorkingDirectory
    tasks:
      - id: createCsvFile
        type: io.kestra.plugin.scripts.shell.Script
        runner: PROCESS
        script: |
          #!/bin/bash
          echo ""Column1,Column2,Column3"" > file.csv
          for i in {1..10}
          do
            echo ""$i,$RANDOM,$RANDOM"" >> file.csv
          done
      
      - id: inspectFile
        type: io.kestra.plugin.scripts.shell.Commands
        runner: PROCESS
        commands:
          - cat file.csv  
      
      - id: filterFile
        type: io.kestra.plugin.scripts.shell.Commands
        runner: PROCESS
        description: select only the first five rows of the second column
        commands:
          - cut -d ',' -f 2 file.csv | head -n 6
",2023-07-26 00:15:22.286,CLI
67,Extract image metadata using Apache Tika,"[""io.kestra.plugin.fs.http.Download"", ""io.kestra.plugin.tika.Parse""]",This flow extracts metadata from an image using Apache Tika.,"id: tika
namespace: blueprint

tasks:
  - id: getImage
    type: io.kestra.plugin.fs.http.Download
    uri: https://kestra.io/blogs/2023-05-31-beginner-guide-kestra.jpg

  - id: tika
    type: io.kestra.plugin.tika.Parse
    from: '{{ outputs.getImage.uri }}'
    store: false
    contentType: TEXT
    ocrOptions:
      strategy: OCR_AND_TEXT_EXTRACTION
",2023-07-26 00:17:25.19,Outputs
201,Extract and transform a Parquet file using DuckDB and export it in Excel format,"[""io.kestra.plugin.jdbc.duckdb.Query"", ""io.kestra.plugin.serdes.excel.IonToExcel""]","This flow will extract a Parquet file from a remote location, transform it using DuckDB and export it in Excel format.
The Parquet file is a dataset of 4.5 million items from a fictional e-commerce store.
Because Excel has a limit of roughly 1 million rows, we will limit the number of query results to 1 million rows in order to export it in Excel format.","id: parquet_duckdb_to_excel
namespace: blueprints

tasks:
  - id: parquet_duckdb
    type: io.kestra.plugin.jdbc.duckdb.Query
    sql: |
      INSTALL parquet;
      LOAD parquet;
      INSTALL httpfs;
      LOAD httpfs;
      SELECT * 
      FROM read_parquet('https://huggingface.co/datasets/kestra/datasets/resolve/main/jaffle-large/raw_items.parquet?download=true')
      LIMIT 1000000;
    store: true

  - id: duckdb_to_excel
    type: io.kestra.plugin.serdes.excel.IonToExcel
    from: ""{{ outputs.parquet.uri }}""
",2023-12-04 18:02:05.27,"Local files,DuckDB"
63,Trigger multiple Airbyte Cloud syncs in parallel,"[""io.kestra.core.tasks.flows.Parallel"", ""io.kestra.plugin.airbyte.cloud.jobs.Sync""]","This flow will sync data from multiple sources in parallel using Airbyte Cloud.

It's recommended to configure the [Airbyte API token](https://portal.airbyte.com/apiKeys) as a secret.
","id: airbyteCloudDbtCloud
namespace: blueprint

tasks:
  - id: data-ingestion
    type: io.kestra.core.tasks.flows.Parallel
    tasks:
      - id: salesforce
        type: io.kestra.plugin.airbyte.cloud.jobs.Sync
        connectionId: e3b1ce92-547c-436f-b1e8-23b6936c12ab
      - id: google-analytics
        type: io.kestra.plugin.airbyte.cloud.jobs.Sync
        connectionId: e3b1ce92-547c-436f-b1e8-23b6936c12cd
      - id: facebook-ads
        type: io.kestra.plugin.airbyte.cloud.jobs.Sync
        connectionId: e3b1ce92-547c-436f-b1e8-23b6936c12ef

taskDefaults:
  - type: io.kestra.plugin.airbyte.cloud.jobs.Sync
    values:
      token: ""{{ secret('AIRBYTE_CLOUD_API_TOKEN') }}""
",2023-07-26 00:49:41.391,"Ingest,Parallel"
90,Upload a Parquet file to Databricks,"[""io.kestra.plugin.databricks.dbfs.Upload""]",This flow will upload a local Parquet file to Databricks File System (DBFS).,"id: uploadParquetToDatabricks
namespace: blueprint
tasks:
  - id: uploadFile
    type: io.kestra.plugin.databricks.dbfs.Upload
    authentication:
      token: ""{{ secret('DATABRICKS_TOKEN') }}""
    host: ""{{ secret('DATABRICKS_HOST') }}""
    from: ""{{inputs.myFile}}""
    to: /Share/myFile.parquet",2023-07-26 00:13:27.885,Databricks
10,Schedule a flow every day at 6:30 AM,"[""io.kestra.core.tasks.log.Log"", ""io.kestra.core.models.triggers.types.Schedule""]","This example shows how to schedule a flow at 6:30 AM every day.

- If the flow is triggered manually, it will log the start date of the task. 
- If the flow is triggered by a regular schedule, it will log the start date of the schedule trigger.

The `date()` function formats the output to display hours and minutes. You can use that function to format the date based on your needs. 
","id: dailyFlow
namespace: blueprint

tasks:
  - id: log
    type: io.kestra.core.tasks.log.Log
    message: It's {{ trigger.date ?? taskrun.startDate | date(""HH:mm"") }} 

triggers:
  - id: schedule
    type: io.kestra.core.models.triggers.types.Schedule
    cron: ""30 6 * * *""
",2023-07-26 18:32:50.002,"Schedule,Variables"
105,Create an image using OpenAI's DALL-E,"[""io.kestra.plugin.openai.CreateImage""]","This flow will use OpenAI's DALL-E to create an image based on a prompt. If you set the download attribute to `true`, the image will be available for download from the Outputs tab on the Executions page. 

Example result:

![dog](https://storage.googleapis.com/strapi--kestra-prd/dog_bf751be6a4/dog_bf751be6a4.png)","id: DALL-E
namespace: blueprint

tasks:
  - id: puppy
    type: io.kestra.plugin.openai.CreateImage
    apiKey: ""{{ secret('OPENAI_API_KEY') }}""
    n: 1
    download: true
    prompt: the cutest little happy smiling puppy
",2023-07-20 16:34:30.183,AI
35,Extract and process data from DynamoDB,"[""io.kestra.plugin.aws.dynamodb.Scan"", ""io.kestra.core.tasks.scripts.Bash""]","This flow scans a DynamoDB table and outputs the extracted data as a JSON string. The subsequent task processes that data.

The `tableName` property must point to an already existing DynamoDB table.

It's recommended to set the `accessKeyId` and `secretKeyId` properties as secrets.","id: scanDynamoDBTable
namespace: blueprint

tasks:
  - id: extractData
    type: io.kestra.plugin.aws.dynamodb.Scan
    tableName: demo
    fetchType: FETCH
    region: ""{{ secret('AWS_DEFAULT_REGION') }}""
    accessKeyId: ""{{ secret('AWS_ACCESS_KEY_ID') }}""
    secretKeyId: ""{{ secret('AWS_SECRET_ACCESS_KEY') }}""

  - id: processData
    type: io.kestra.plugin.scripts.shell.Commands
    runner: PROCESS
    commands:
      - echo {{outputs.extractData.rows}}
",2023-07-27 00:04:57.256,AWS
45,Parametrized workflow with multiple schedules,"[""io.kestra.core.tasks.log.Log"", ""io.kestra.core.models.triggers.types.Schedule""]","This flow takes a runtime-specific input and uses it to log a message to the console.
The flow has two scheduled attached to it: 
- one that runs every 15 minutes with the default input parameter value
- another one that runs every 1 minute with a custom input parameter value

Note that both schedules are currently disabled. 
To start scheduling the flow, set the `disabled` property to `false` or delete that property entirely.
","id: parametrizedFlowWithMultipleSchedules
namespace: blueprint

inputs:
  - name: user
    type: STRING
    defaults: Data Engineer
    required: false

tasks:
  - id: hello
    type: io.kestra.core.tasks.log.Log
    message: Hello {{ inputs.user }} from Kestra!

triggers:
  - id: quarterHourly
    type: io.kestra.core.models.triggers.types.Schedule
    disabled: true
    cron: ""*/15 * * * *""
    inputs:
     name: user
  
  - id: everyMinute
    type: io.kestra.core.models.triggers.types.Schedule
    disabled: true
    cron: ""*/1 * * * *""
    inputs:
     name: user
     value: custom value",2023-07-26 18:48:09.458,"Schedule,Inputs,Variables"
107,Process files in parallel,"[""io.kestra.core.tasks.flows.WorkingDirectory"", ""io.kestra.core.tasks.scripts.Bash"", ""io.kestra.core.tasks.storages.LocalFiles"", ""io.kestra.core.tasks.flows.EachParallel"", ""io.kestra.core.tasks.debugs.Return"", ""io.kestra.plugin.scripts.shell.Commands""]","This example demonstrates how to process files in parallel.

Instead of the `bash` script, you may have a Python/R/Node.js script that generates such files.","id: parallelFiles
namespace: blueprint

tasks:
  - id: workingDir
    type: io.kestra.core.tasks.flows.WorkingDirectory
    tasks:
    - id: bash
      type: io.kestra.plugin.scripts.shell.Commands
      runner: PROCESS
      commands:
        - mkdir -p out
        - echo ""Hello from 1"" >> out/output1.txt
        - echo ""Hello from 2"" >> out/output2.txt
        - echo ""Hello from 3"" >> out/output3.txt
        - echo ""Hello from 4"" >> out/output4.txt

    - id: out
      type: io.kestra.core.tasks.storages.LocalFiles
      outputs:
        - out/**
    
  - id: each
    type: io.kestra.core.tasks.flows.EachParallel
    value: ""{{outputs.out.uris | jq('.[]')}}""
    tasks:
      - id: path
        type: io.kestra.core.tasks.debugs.Return
        format: ""{{taskrun.value}}""
      - id: contents
        type: io.kestra.plugin.scripts.shell.Commands
        runner: PROCESS
        commands:
          - cat ""{{taskrun.value}}""
",2023-08-13 23:16:33.105,"Local files,Outputs,Parallel"
6,Read a file from inputs and upload it to GCS,"[""io.kestra.plugin.gcp.gcs.Upload""]","This blueprint shows how to read a file from inputs and upload it to GCS

> Note: Authentication to GCP can be done by setting the `GOOGLE_APPLICATION_CREDENTIALS` variable in environment (via a service account for example).","id: input_file_upload_gcs
namespace: blueprint

inputs:
  - name: file
    type: FILE

  - name: rename
    type: STRING

tasks:
  - id: upload
    type: io.kestra.plugin.gcp.gcs.Upload
    from: ""{{ inputs.file }}""
    to: ""gs://kestra-demo/{{ inputs.rename }}""

",2023-07-26 00:29:54.258,GCP
134,Build a Docker image and push it to AWS Elastic Container Registry (ECR),"[""io.kestra.plugin.aws.cli.AwsCLI"", ""io.kestra.plugin.docker.Build""]","This flow will build a Docker image and push it to a remote container registry.

1. The `dockerfile` parameter is a multiline string that contains the Dockerfile content. However, it can also be a path to a file.
2. The `tags` parameter is a list of tags of the image to build. Make sure to set a correct AWS region, and adjust the rest of the image URL to match your ECR repository.
3. The `push` parameter is a boolean that indicates whether to push the image to the Elastic Container Registry.
4. Make sure to securely store your AWS credentials as secrets or environment variables.
5. Finally, note how we used the `GetAuthToken` task to fetch an access token for the `docker` command. This is necessary to authenticate the task with your ECR registry.","id: build_aws_ecr_image
namespace: blueprint

tasks:
  - id: fetch_auth_token
    type: io.kestra.plugin.aws.ecr.GetAuthToken
    accessKeyId: ""{{ secret('AWS_ACCESS_KEY_ID') }}""
    secretKeyId: ""{{ secret('AWS_SECRET_ACCESS_KEY') }}""
    region: ""{{ secret('AWS_DEFAULT_REGION') }}""

  - id: build
    type: io.kestra.plugin.docker.Build
    dockerfile: |
      FROM python:3.10
      RUN pip install --upgrade pip
      RUN pip install --no-cache-dir kestra requests ""polars[all]""
    tags: 
      - 123456789.dkr.ecr.eu-central-1.amazonaws.com/data-infastructure:latest
    push: true
    credentials:
      username: AWS
      password: ""{{outputs.fetch_auth_token.token}}""
",2023-11-03 00:27:35.978,"AWS,Docker"
86,Expose custom metrics from a Shell script,"[""io.kestra.plugin.scripts.shell.Commands""]","This blueprint shows how to expose metrics within a Shell script.

Metrics are intended to track custom numeric (metric `type: counter`) or duration (metric `type: timer`) attributes that you may want to visualize across task runs and flow executions.
","id: metricsFromShellCommands
namespace: blueprint

tasks:
  - id: process
    type: io.kestra.plugin.scripts.shell.Commands
    commands:
      - echo '::{""metrics"":[{""name"":""count"",""type"":""counter"",""value"":1}]}::'
",2023-07-26 19:01:34.82,Metrics
144,Ingest a JSON API payload into a DuckDB database using dlt,"[""io.kestra.plugin.scripts.python.Script""]",This flow demonstrates how to extract data from an API and load it into a DuckDB database using dlt. The entire workflow logic is contained in a single Python script that uses the dlt Python library to ingest the JSON file into a DuckDB database.,"id: dlt_json_to_duckdb
namespace: blueprint

tasks:
  - id: dlt_pipeline
    type: io.kestra.plugin.scripts.python.Script
    docker:
      image: python:3.11-slim
    beforeCommands:
      - pip install dlt[duckdb]
    warningOnStdErr: false
    script: |
      import dlt
      import requests

      response = requests.get('https://dummyjson.com/products')
      response.raise_for_status()
      data = response.json()['products']

      pipeline = dlt.pipeline(
          pipeline_name='dummyjson_products_pipeline',
          destination='duckdb',
          dataset_name='products',
          credentials=""{{outputDir}}/dummy_products.duckdb""
      )

      pipeline.run(data, table_name='product')",2023-10-10 12:15:21.052,"API,Ingest"
39,Allow failure of certain tasks,"[""io.kestra.core.tasks.flows.AllowFailure"", ""io.kestra.plugin.scripts.shell.Commands""]","This flow demonstrates how to use the `AllowFailure` task to allow a task to fail without blocking downstream tasks.
The `AllowFailure` task is useful for when you want to run a task that may fail, but you don't want to block downstream tasks.


In this example, the task `failSilently` will fail, but the flow will continue to run and the task `printToConsole` will run.
The task `willNeverRun` will never run because the task `fail` will fail and block downstream tasks.","id: allowFailureDemo
namespace: blueprint

tasks:
  - id: allowFailure
    type: io.kestra.core.tasks.flows.AllowFailure
    tasks:
      - id: failSilently
        type: io.kestra.plugin.scripts.shell.Commands
        runner: PROCESS
        commands:
          - exit 1
  
  - id: printToConsole
    type: io.kestra.plugin.scripts.shell.Commands
    runner: PROCESS
    commands:
      - echo ""this will run since previous failure was allowed ✅""
  
  - id: fail
    type: io.kestra.plugin.scripts.shell.Commands
    runner: PROCESS
    commands:
      - echo ""failing and blocking downstream tasks"" && exit 1
  
  - id: willNeverRun
    type: io.kestra.plugin.scripts.shell.Commands
    runner: PROCESS
    commands:
      - echo ""this will never run ❌""",2023-07-26 19:00:15.69,
93,Execute a Spark or Python script on an existing Databricks cluster and wait for its completion,"[""io.kestra.plugin.databricks.job.SubmitRun"", ""io.kestra.core.tasks.log.Log""]","This flow will run a task on an existing Databricks cluster specified by ID. The Python script referenced on the `pythonFile` property is stored in the Databricks workspace.
The flow will run the script on a Databricks cluster and wait up to five minutes for the task to complete before running the next task (here, simply printing a log message to the console).","id: runTasksOnDatabricks
namespace: blueprint

tasks:
  - id: submitRun
    type: io.kestra.plugin.databricks.job.SubmitRun
    host: ""{{ secret('DATABRICKS_HOST') }}""
    authentication:
      token: ""{{ secret('DATABRICKS_TOKEN') }}""
    runTasks:
      - existingClusterId: abcdefgh12345678
        taskKey: pysparkTask
        sparkPythonTask:
          pythonFile: /Shared/hello.py
          sparkPythonTaskSource: WORKSPACE
    waitForCompletion: PT5M
  
  - id: logStatus
    type: io.kestra.core.tasks.log.Log
    message: The job finished, all done!",2023-08-27 14:37:03.673,Databricks
89,Event-driven workflow based on a Snowflake query condition,"[""io.kestra.core.tasks.flows.EachSequential"", ""io.kestra.core.tasks.debugs.Return"", ""io.kestra.plugin.jdbc.snowflake.Trigger""]","This workflow will be executed only when the query returns any results. The flow checks if there are any new rows that match a specific criteria (e.g. you may query a table storing new events that haven't been processed yet). If so, the flow performs some actions as defined in the tasks e.g.:

- loading that data to another table
- sending a message about the new event
- triggering some automated process
- updating or deleting the event from that original table to avoid that it will trigger another (duplicated) run. 

Make sure that you delete, update or move the rows that triggered the flow. This avoids duplicated runs. Here, we simply use the same WHERE clause in the SQL Event Trigger and in the UPDATE/DELETE statement:
- Trigger: `SELECT * FROM KESTRA.PUBLIC.EMPLOYEES WHERE START_DATE = CURRENT_DATE()`
- Update: `UPDATE KESTRA.PUBLIC.EMPLOYEES SET UPDATE_TIMESTAMP = SYSDATE() WHERE START_DATE = CURRENT_DATE();`
- Delete: `DELETE FROM KESTRA.PUBLIC.EMPLOYEES WHERE START_DATE = CURRENT_DATE()`","id: snowflakeQueryTrigger
namespace: blueprint

tasks:
  - id: each
    type: io.kestra.core.tasks.flows.EachSequential
    value: ""{{ trigger.rows }}""
    tasks:      
      - id: automatedProcess
        type: io.kestra.plugin.scripts.shell.Commands
        runner: PROCESS
        commands:
          - echo ""{{json(taskrun.value)}}""
          - echo ""Welcome to Kestra {{json(taskrun.value).FIRST_NAME }} {{json(taskrun.value).LAST_NAME }}""

  - id: update
    type: io.kestra.plugin.jdbc.snowflake.Query
    description: Update rows to avoid double trigger
    sql: |
      UPDATE KESTRA.PUBLIC.EMPLOYEES
      SET UPDATE_TIMESTAMP = SYSDATE()
      WHERE START_DATE = CURRENT_DATE();

taskDefaults:
  - type: io.kestra.plugin.jdbc.snowflake.Trigger
    values:
      url: jdbc:snowflake://your_account_id.snowflakecomputing.com?warehouse=DEMO
      username: your_username
      password: ""{{ secret('SNOWFLAKE_PASSWORD') }}""
  
  - type: io.kestra.plugin.jdbc.snowflake.Query
    values:
      url: jdbc:snowflake://your_account_id.snowflakecomputing.com?warehouse=DEMO
      username: your_username
      password: ""{{ secret('SNOWFLAKE_PASSWORD') }}""

triggers:
  - id: wait
    type: io.kestra.plugin.jdbc.snowflake.Trigger
    sql: ""SELECT * FROM KESTRA.PUBLIC.EMPLOYEES WHERE START_DATE = CURRENT_DATE() and UPDATE_TIMESTAMP IS NULL;""
    interval: ""PT1M"" # check every 1 minute for the query condition
    fetch: true
",2023-07-18 09:51:22.381,"Trigger,Snowflake"
36,Run Python script in a Docker container based on Google Artifact Registry container image,"[""io.kestra.core.tasks.flows.WorkingDirectory"", ""io.kestra.plugin.fs.http.Download"", ""io.kestra.plugin.gcp.auth.OauthAccessToken"", ""io.kestra.core.tasks.storages.LocalFiles"", ""io.kestra.plugin.scripts.python.Script""]","This flow downloads a CSV file from a public URL, analyzes it with a Python script, and outputs the results.
The Python script is executed in a Docker container.
The Docker image is stored in Google Artifact Registry.

To push an image to Google Artifact Registry, you need to:
- Create a Google Cloud Platform service account with the `Artifact Registry Writer` role.
- Create a JSON key for the service account.
- Create a secret with the contents of the JSON key. 
- Build a Docker image: `docker build -t yourGcpRegion-docker.pkg.dev/YOUR_GCP_PROJECT_NAME/flows/python:latest .`
- Push the image to Google Artifact Registry: `docker push yourGcpRegion-docker.pkg.dev/YOUR_GCP_PROJECT_NAME/flows/python:latest`

Note that the `OauthAccessToken` task is necessary to securely fetch a short-lived [access token](https://cloud.google.com/artifact-registry/docs/docker/authentication#token).
","id: pythonDockerArtifactRegistryGCP
namespace: blueprint

tasks:
  - id: wdir
    type: io.kestra.core.tasks.flows.WorkingDirectory
    tasks:
      - id: downloadCSV
        type: io.kestra.plugin.fs.http.Download
        uri: https://raw.githubusercontent.com/kestra-io/examples/main/datasets/orders.csv

      - id: fetchAuthToken
        type: io.kestra.plugin.gcp.auth.OauthAccessToken
        projectId: YOUR_GCP_PROJECT_NAME
        serviceAccount: ""{{ secret('GCP_CREDS') }}""

      - id: local
        type: io.kestra.core.tasks.storages.LocalFiles
        inputs:
          data.csv: ""{{outputs.downloadCSV.uri}}""

      - id: analyzeSales
        type: io.kestra.plugin.scripts.python.Script
        script: |
          import pandas as pd
          from kestra import Kestra

          df = pd.read_csv(""data.csv"")
          sales = df.total.sum()
          med = df.quantity.median()

          Kestra.outputs({""total_sales"": sales, ""median_quantity"": med})

          top_sellers = df.sort_values(by=""total"", ascending=False).head(3)
          print(f""Top 3 orders: {top_sellers}"")
        docker:
          image: yourGcpRegion-docker.pkg.dev/YOUR_GCP_PROJECT_NAME/flows/python:latest
          config: |
            {
              ""auths"": {
                ""europe-west3-docker.pkg.dev"": {
                    ""username"": ""oauth2accesstoken"",
                    ""password"": ""{{outputs.fetchAuthToken.accessToken.tokenValue}}""
                  }
              }
            }
",2023-07-25 18:21:56.71,"Python,Docker,GCP"
31,"Trigger multiple Airbyte Cloud syncs, then run a dbt Cloud job","[""io.kestra.core.tasks.flows.Parallel"", ""io.kestra.plugin.airbyte.cloud.jobs.Sync"", ""io.kestra.plugin.dbt.cloud.TriggerRun""]","This flow will sync data from multiple sources in parallel using Airbyte Cloud. Then, it will run a dbt Cloud job.

It's recommended to configure the Dbt Cloud and [Airbyte API](https://portal.airbyte.com/apiKeys) tokens as secrets.","id: airbyteCloudDbtCloud
namespace: blueprint

tasks:
  - id: data-ingestion
    type: io.kestra.core.tasks.flows.Parallel
    tasks:
      - id: salesforce
        type: io.kestra.plugin.airbyte.cloud.jobs.Sync
        connectionId: e3b1ce92-547c-436f-b1e8-23b6936c12ab
      - id: google-analytics
        type: io.kestra.plugin.airbyte.cloud.jobs.Sync
        connectionId: e3b1ce92-547c-436f-b1e8-23b6936c12cd
      - id: facebook-ads
        type: io.kestra.plugin.airbyte.cloud.jobs.Sync
        connectionId: e3b1ce92-547c-436f-b1e8-23b6936c12ef

  - id: dbt-cloud-job
    type: io.kestra.plugin.dbt.cloud.TriggerRun
    jobId: ""396284""
    accountId: ""{{ secret('DBT_CLOUD_ACCOUNT_ID') }}""
    token: ""{{ secret('DBT_CLOUD_API_TOKEN') }}""
    wait: true

taskDefaults:
  - type: io.kestra.plugin.airbyte.cloud.jobs.Sync
    values:
      token: ""{{ secret('AIRBYTE_CLOUD_API_TOKEN') }}""",2023-07-26 00:51:50.768,"Ingest,dbt,Parallel"
50,Git workflow for dbt with DuckDB,"[""io.kestra.core.tasks.flows.WorkingDirectory"", ""io.kestra.plugin.git.Clone"", ""io.kestra.plugin.dbt.cli.Build""]","This flow:
- clones a dbt Git repository from GitHub, 
- pulls a public [container image with the required dependencies](https://github.com/kestra-io/examples/pkgs/container/dbt-duckdb)
- runs dbt CLI commands in a Docker container.
","id: dbtGitDockerDuckDB
namespace: blueprint

tasks:
  - id: dbt
    type: io.kestra.core.tasks.flows.WorkingDirectory
    tasks:
    - id: cloneRepository
      type: io.kestra.plugin.git.Clone
      url: https://github.com/kestra-io/dbt-demo
      branch: main

    - id: dbt-build
      type: io.kestra.plugin.dbt.cli.DbtCLI
      runner: DOCKER
      docker:
        image: ghcr.io/kestra-io/dbt-duckdb:latest
      commands:
        - dbt deps
        - dbt build
",2023-09-27 21:39:10.258,"Git,DuckDB,dbt"
102,"Extract data from an API using Python, then load it to Postgres and S3","[""io.kestra.plugin.scripts.python.Script"", ""io.kestra.plugin.aws.s3.Upload""]","This flow runs a Python script that:
1. Extracts data from an API  
2. Loads that extracted data to Postgres and a local JSON file. The local file is then uploaded to S3 in the following task. 

The Python task runs in a Docker container. Before starting the script, Kestra will install custom package dependencies, as defined by the `beforeCommands` property.","id: postgresS3PythonScript
namespace: dev

tasks:
  - id: apiToPostgres
    type: io.kestra.plugin.scripts.python.Script
    beforeCommands:
      - pip install requests pandas psycopg2 sqlalchemy > /dev/null
    warningOnStdErr: false
    script: |
      import pandas as pd
      import requests
      from sqlalchemy import create_engine
      
      URL = ""https://gorest.co.in/public/v2/users""
      req = requests.get(url=URL)
      res = req.json()
      
      df_users = pd.DataFrame(res)
      df_users[""inserted_from""] = ""kestra""
      df_users.head()
      password = ""{{secret('DB_PASSWORD')}}""
      host = ""host.docker.internal""
      
      engine = create_engine(f""postgresql://postgres:{password}@{host}:5432"")
      
      df_users.to_sql(""users"", engine, if_exists=""append"", index=False)
      df_users.to_json(""{{outputDir}}/users.json"")

  - id: s3upload
    type: io.kestra.plugin.aws.s3.Upload
    from: ""{{ outputs.apiToPostgres.outputFiles['users.json'] }}""
    key: users.json
    bucket: kestraio
    region: eu-central-1
    accessKeyId: ""{{ secret('AWS_ACCESS_KEY_ID') }}""
    secretKeyId: ""{{ secret('AWS_SECRET_ACCESS_KEY') }}""
",2023-07-25 23:52:12.193,"Postgres,pip,S3,Ingest,Python"
14,Write data in MongoDB,"[""io.kestra.plugin.mongodb.InsertOne""]",This blueprint shows how to insert a document in MongoDB database.,"id: write_mongo
namespace: blueprint

tasks:

  - id: write
    type: io.kestra.plugin.mongodb.InsertOne
    connection:
      uri: ""mongodb://root:example@mongo:27017/""
    database: ""my_database""
    collection: ""my_collection""
    document:
      _id:
        $oid: 60930c39a982931c20ef6cd6
      name: ""John Doe""
      city: ""Paris""",2023-07-26 00:32:46.849,Ingest
66,Create table and load data to MySQL using SQL - using taskDefaults to eliminate boilerplate code,"[""io.kestra.plugin.fs.http.Download"", ""io.kestra.plugin.jdbc.mysql.Query""]","This flow will extract data from a remote CSV file and load it into a MySQL database. It's recommended to store database credentials as secrets.

Note that to avoid repetition, we use `taskDefaults` specifying attributes needed to interact with MySQL.

To start a MySQL database in a Docker container, you can use the following command:
```bash
docker run -d -p 3306:3306 --name mymysql -v mymysqldb:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=topSecret42 -e MYSQL_DATABASE=stage mysql:latest
```","id: extractLoadMySQL
namespace: blueprint

variables:
  table: new

tasks:
  - id: extract
    type: io.kestra.plugin.fs.http.Download
    uri: https://raw.githubusercontent.com/kestra-io/examples/main/datasets/orders.csv

  - id: enableLocalFiles
    type: io.kestra.plugin.jdbc.mysql.Query
    sql: SET GLOBAL local_infile=1;

  - id: createTable
    type: io.kestra.plugin.jdbc.mysql.Query
    sql: |
      create table if not exists {{vars.table}}
      (
          order_id       integer,
          customer_name  varchar(50),
          customer_email varchar(50),
          product_id     integer,
          price          real,
          quantity       integer,
          total          real
      );

  - id: loadData
    type: io.kestra.plugin.jdbc.mysql.Query
    inputFile: ""{{ outputs.extract.uri }}""
    sql: |
      LOAD DATA LOCAL INFILE '{{ inputFile }}' 
      INTO TABLE {{vars.table}} 
      FIELDS TERMINATED BY ','
      LINES TERMINATED BY '\n'
      IGNORE 1 ROWS;

taskDefaults:
  - type: io.kestra.plugin.jdbc.mysql.Query
    values:
      url: jdbc:mysql://host.docker.internal:3306/stage
      username: root
      password: ""{{ secret('DB_PASSWORD') }}""
",2023-07-25 23:34:11.572,"Ingest,SQL,Local files"
109,"Upload data to S3 in Python using boto3, transform it in a SQL query with DuckDB and send a CSV report via email every first day of the month","[""io.kestra.plugin.scripts.python.Script"", ""io.kestra.plugin.jdbc.duckdb.Query"", ""io.kestra.plugin.serdes.csv.CsvWriter"", ""io.kestra.plugin.notifications.mail.MailSend"", ""io.kestra.core.models.triggers.types.Schedule""]","Replace the S3 bucket `kestraio` with your bucket name to reproduce the example.

This flow assumes:
- an in-process DuckDB
- AWS credentials with S3 access permissions stored using Kestra [secrets](https://kestra.io/docs/developer-guide/secrets).

If you use [MotherDuck](https://motherduck.com/) and [MotherDuck's managed S3 secrets](https://motherduck.com/docs/authenticating-to-s3), you can replace the `query` task with the following simpler configuration:

```yaml
  - id: query
    type: io.kestra.plugin.jdbc.duckdb.Query
    sql: |
      SELECT month(order_date) as month, sum(total) as total 
      FROM read_csv_auto('s3://{{vars.bucket}}/monthly_orders/*.csv', FILENAME = 1) 
      GROUP BY 1
      ORDER BY 2 desc;
    store: true
    timeout: PT30S
    url: ""jdbc:duckdb:md:my_db?motherduck_token={{ secret('MOTHERDUCK_TOKEN') }}""
```","id: monthlySalesReport
namespace: blueprint

variables:
  bucket: kestraio

tasks:
  - id: rawDataToS3
    type: io.kestra.plugin.scripts.python.Script
    docker:
      image: ghcr.io/kestra-io/aws:latest
    env:
      AWS_ACCESS_KEY_ID: ""{{ secret('AWS_ACCESS_KEY_ID') }}""
      AWS_SECRET_ACCESS_KEY: ""{{ secret('AWS_SECRET_ACCESS_KEY') }}""
      AWS_DEFAULT_REGION: ""{{ secret('AWS_DEFAULT_REGION') }}""
    script: |
      import requests
      import boto3
      from kestra import Kestra

      BUCKET = ""{{vars.bucket}}""


      def extract_and_upload(file):
          url = f""https://raw.githubusercontent.com/kestra-io/datasets/main/{file}""

          response = requests.get(url)
          data = response.content.decode(""utf-8"")
          s3 = boto3.resource(""s3"")
          s3.Bucket(BUCKET).put_object(Key=file, Body=data)
          print(f""{url} downloaded and saved to {BUCKET}/{file}"")


      for month in range(1, 13):
          filename = f""monthly_orders/2023_{str(month).zfill(2)}.csv""
          extract_and_upload(filename)
          Kestra.outputs({f""{filename}"": f""s3://{BUCKET}/{filename}""})

  - id: query
    type: io.kestra.plugin.jdbc.duckdb.Query
    sql: |
      INSTALL httpfs;
      LOAD httpfs;
      SET s3_region='{{ secret('AWS_DEFAULT_REGION') }}';
      SET s3_access_key_id='{{ secret('AWS_ACCESS_KEY_ID') }}';
      SET s3_secret_access_key='{{ secret('AWS_SECRET_ACCESS_KEY') }}';
      SELECT month(order_date) as month, sum(total) as total 
      FROM read_csv_auto('s3://kestraio/monthly_orders/*.csv', FILENAME = 1) 
      GROUP BY 1
      ORDER BY 2 desc;
    store: true
    timeout: PT30S
  
  - id: csv
    type: io.kestra.plugin.serdes.csv.CsvWriter
    from: ""{{ outputs.query.uri }}""

  - id: email
    type: io.kestra.plugin.notifications.mail.MailSend
    subject: The monthly sales report is ready
    from: hello@kestra.io
    to: you@example.com;yourboss@example.com
    username: hello@kestra.io
    host: mail.privateemail.com
    port: 465 # 587
    password: ""{{ secret('EMAIL_PASSWORD') }}""
    sessionTimeout: 6000
    attachments:
      - name: monthly_sales_report.csv
        uri: ""{{ outputs.csv.uri }}""
    htmlTextContent: |
      Please find attached the current sales report. <br /><br />
      Best regards,<br />
      Data Team

triggers:
  - id: monthly
    type: io.kestra.core.models.triggers.types.Schedule
    cron: 0 9 1 * * # every first day of the month",2023-07-26 18:54:23.223,"Schedule,Notifications,S3,DuckDB,Outputs,Python,Variables"
155,Send an alert to Sentry when any flow fails in the production namespace,"[""io.kestra.plugin.notifications.sentry.SentryExecution"", ""io.kestra.core.models.triggers.types.Flow"", ""io.kestra.core.models.conditions.types.ExecutionStatusCondition"", ""io.kestra.core.models.conditions.types.ExecutionNamespaceCondition""]","This flow shows how to send an alert to Sentry when a flow fails. The only required input is a DSN string value, which you can find when you go to your Sentry project settings and go to the section ""Client Keys (DSN)"". For more detailed description of how to find your DSN, visit the [following Sentry documentation](https://docs.sentry.io/product/sentry-basics/concepts/dsn-explainer/#where-to-find-your-dsn).

You can customize the alert `payload`, which is a JSON object. For more information about the payload, check the [Sentry Event Payloads documentation](https://develop.sentry.dev/sdk/event-payloads/).

The `level` parameter is the severity of the issue. The task documentation lists all available options including `DEBUG`, `INFO`, `WARNING`, `ERROR`, `FATAL`. The default value is `ERROR`.

This monitoring flow is triggered anytime a flow fails in the `prod` namespace. You can fully customize the [trigger conditions](https://kestra.io/plugins/core#conditions).","id: failure_alert_sentry
namespace: prod.monitoring

tasks:
  - id: send_alert
    type: io.kestra.plugin.notifications.sentry.SentryExecution
    executionId: ""{{ trigger.executionId} }""
    transaction: ""/execution/id/{{ trigger.executionId} }""
    dsn: ""{{ secret('SENTRY_DSN') }}""
    level: ERROR 

triggers:
  - id: failed_prod_workflows
    type: io.kestra.core.models.triggers.types.Flow
    conditions:
      - type: io.kestra.core.models.conditions.types.ExecutionStatusCondition
        in:
          - FAILED
          - WARNING
      - type: io.kestra.core.models.conditions.types.ExecutionNamespaceCondition
        namespace: prod
        prefix: true
",2023-10-28 10:06:13.192,Notifications
13,Trigger a sub Flow,"[""io.kestra.core.tasks.debugs.Return"", ""io.kestra.core.tasks.flows.Flow""]",This blueprint shows how to trigger a (sub)flow from another flow. It's a great pattern to reuse common flows. Thanks to inputs you can parameter your flows and reuse them easily.,"id: trigger_subflow
namespace: blueprint

tasks:

  - id: taskA
    type: io.kestra.core.tasks.debugs.Return
    format: ""{{task.id}} - Flow A""

  - id: flowB
    type: io.kestra.core.tasks.flows.Flow
    description: This Task trigger the Flow `subflow` with corresponding inputs.
    namespace: demo
    flowId: subflow
    inputs:
      data: ""{{ outputs.taskA.value }}""

",2023-06-22 14:32:32.151,Trigger
118,Extract data from an API and load it to S3 on schedule (every Friday afternoon),"[""io.kestra.plugin.fs.http.Download"", ""io.kestra.plugin.aws.s3.Upload"", ""io.kestra.core.models.triggers.types.Schedule""]",This flow extracts data from an HTTP API and loads it to S3 as a JSON file,"id: api-to-s3
namespace: blueprint

tasks:
  - id: getPokemon
    type: io.kestra.plugin.fs.http.Download
    method: GET
    uri: https://pokeapi.co/api/v2/pokemon/psyduck

  - id: upload
    type: io.kestra.plugin.aws.s3.Upload
    bucket: kestraio
    from: ""{{outputs.getPokemon.uri}}""
    key: psyduck.json
    accessKeyId: ""{{ secret('AWS_ACCESS_KEY_ID') }}""
    secretKeyId: ""{{ secret('AWS_SECRET_ACCESS_KEY') }}""
    region: ""{{ secret('AWS_DEFAULT_REGION') }}""

triggers:
  - id: firstMondayOfTheMonth
    type: io.kestra.core.models.triggers.types.Schedule
    timezone: Europe/Berlin # adjust to your timezone
    cron: ""0 17 * * FRI"" # at 5 PM on every Friday",2023-08-13 10:38:55.513,"S3,API,Ingest"
133,Build a Docker image and push it to Google Cloud Artifact Registry,"[""io.kestra.plugin.gcp.auth.OauthAccessToken"", ""io.kestra.plugin.docker.Build""]","This flow will build a Docker image and push it to a remote container registry.

1. The `dockerfile` parameter is a multiline string that contains the Dockerfile content. However, it can also be a path to a file.
2. The `tags` parameter is a list of tags of the image to build. Make sure to replace the `europe-west3` prefix with your GCP region, and adjust the rest of the URL to match your Artifact Registry repository.
3. The `push` parameter is a boolean that indicates whether to push the image to GitHub Container Registry.
4. Make sure to securely store your GCP credentials as secrets or environment variables.
5. Finally, note how we used the `OauthAccessToken` task to fetch an access token for the `docker` command. This is necessary because Docker doesn't directly support GCP service accounts.","id: build_gcp_artifact_registry_image
namespace: blueprint

tasks:
  - id: fetch_auth_token
    type: io.kestra.plugin.gcp.auth.OauthAccessToken
    projectId: your_gcp_project_id
    serviceAccount: ""{{ secret('GCP_CREDS') }}""

  - id: build
    type: io.kestra.plugin.docker.Build
    dockerfile: |
      FROM python:3.10
      RUN pip install --upgrade pip
      RUN pip install --no-cache-dir kestra requests ""polars[all]""
    tags: 
      - europe-west3-docker.pkg.dev/your_gcp_project_id/kestra/polars:latest
    push: true
    credentials:
      username: oauth2accesstoken
      password: ""{{outputs.fetchAuthToken.accessToken.tokenValue}}""
",2023-09-22 11:53:13.933,"GCP,Docker"
195,Manage Aiven resources from the CLI — start and stop services or databases on schedule,"[""io.kestra.plugin.scripts.python.Commands"", ""io.kestra.core.models.triggers.types.Schedule""]","This flow shows how to manage Aiven resources from the CLI. It assumes that you have an Aiven account and that you [created an API token](https://docs.aiven.io/docs/platform/howto/create_authentication_token). Make sure to replace the `AVN_AUTH_TOKEN` value in the task with your Aiven API token. It's recommended to use [Secrets](https://kestra.io/docs/developer-guide/secrets/) to store sensitive data such as API tokens.

Once you've configured the Aiven secret, you can reproduce this flow without any changes. 

The first command is great to test the setup — the command will just list your Aiven projects. However, there is a lot more you can do with the Aiven CLI. Check out the [Aiven CLI guide](https://aiven.io/developer/aiven-cmdline) for more information.

For example, you can use it to start and stop specific services in your Aiven projects using scheduled flows in Kestra. This is useful if you want to save money by stopping your services when you don't need them, e.g. at night or during the weekend. 

You can also use the CLI to create and delete services or databases on demand.
","id: aiven
namespace: blueprint

tasks:
  - id: cli
    type: io.kestra.plugin.scripts.python.Commands
    warningOnStdErr: false
    docker:
      image: python:slim
    beforeCommands:
      - pip install aiven-client
    commands:
      - avn --auth-token $AVN_AUTH_TOKEN project list
      - avn --auth-token $AVN_AUTH_TOKEN service update YOUR-SERVICE-NAME --power-off
      - avn --auth-token $AVN_AUTH_TOKEN service update YOUR-SERVICE-NAME --power-on
    env:
      AVN_AUTH_TOKEN: ""{{ secret('AVN_AUTH_TOKEN') }}""

triggers:
  - id: every_morning
    type: io.kestra.core.models.triggers.types.Schedule
    cron: ""0 9 * * *""",2023-11-07 18:36:41.717,"Trigger,Python,CLI"
111,Git workflow for dbt with MotherDuck,"[""io.kestra.core.tasks.flows.WorkingDirectory"", ""io.kestra.plugin.git.Clone"", ""io.kestra.plugin.dbt.cli.Build""]","This flow:
- clones a dbt Git repository from GitHub, 
- pulls a public [container image with the required dependencies](https://github.com/kestra-io/examples/pkgs/container/dbt-duckdb)
- runs dbt CLI commands in a Docker container.

To use [MotherDuck](https://motherduck.com/), use Kestra [secret](https://kestra.io/docs/developer-guide/secrets) to store the [MotherDuck service token](https://motherduck.com/docs/authenticating-to-motherduck).","id: dbtGitDockerDuckDB
namespace: blueprint

tasks:
  - id: dbt
    type: io.kestra.core.tasks.flows.WorkingDirectory
    tasks:
    - id: cloneRepository
      type: io.kestra.plugin.git.Clone
      url: https://github.com/kestra-io/dbt-demo
      branch: main

    - id: dbt-build
      type: io.kestra.plugin.dbt.cli.DbtCLI
      runner: DOCKER
      docker:
        image: ghcr.io/kestra-io/dbt-duckdb:latest
      profiles: |
        jaffle_shop:
          outputs:
            dev:
              type: duckdb
              disable_transactions: true
              path: md:jaffle_shop?motherduck_token={{secret('MOTHERDUCK_TOKEN')}}
              fixed_retries: 1
              threads: 16
              timeout_seconds: 300
          target: dev
      commands:
        - dbt deps
        - dbt build
",2023-08-11 14:58:48.423,"DuckDB,dbt,Git"
2,"Analyze salaries of data professionals using a SQL query in DuckDB, extract the results to a CSV file","[""io.kestra.plugin.fs.http.Download"", ""io.kestra.plugin.jdbc.duckdb.Query"", ""io.kestra.plugin.serdes.csv.CsvWriter""]","Download a CSV file and query data with DuckDB. Then, extract the results to a CSV file.","id: duckdb_query
namespace: blueprint

tasks:
  - id: download_csv
    type: io.kestra.plugin.fs.http.Download
    description: salaries of data professionals from 2020 to 2023 (source ai-jobs.net)
    uri: https://raw.githubusercontent.com/kestra-io/datasets/main/csv/salaries.csv

  - id: avg_salary_by_job_title
    type: io.kestra.plugin.jdbc.duckdb.Query
    inputFiles:
      data.csv: ""{{ outputs.download_csv.uri }}""
    sql: |
      SELECT 
        job_title,
        ROUND(AVG(salary),2) AS avg_salary
      FROM read_csv_auto('{{workingDir}}/data.csv', header=True)
      GROUP BY job_title
      HAVING COUNT(job_title) > 10
      ORDER BY avg_salary DESC;
    store: true

  - id: result
    type: io.kestra.plugin.serdes.csv.CsvWriter
    from: ""{{ outputs.avg_salary_by_job_title.uri }}""",2023-07-26 12:17:00.748,"Outputs,SQL,DuckDB"
16,Download data and upload to Google Cloud Storage,"[""io.kestra.plugin.fs.http.Download"", ""io.kestra.plugin.gcp.gcs.Upload""]","","id: load_to_cloud_storage
namespace: blueprint

tasks:
  - id: data
    type: io.kestra.plugin.fs.http.Download
    uri: https://gist.githubusercontent.com/Ben8t/f182c57f4f71f350a54c65501d30687e/raw/940654a8ef6010560a44ad4ff1d7b24c708ebad4/salary-data.csv

  - id: cloud_storage
    type: io.kestra.plugin.gcp.gcs.Upload
    from: ""{{ outputs.data.uri }}""
    to: ""gs://kestra-demo/data.csv""
",2023-07-26 00:29:35.498,GCP
199,"Extract data from a CSV file, load it in batch to Weaviate and query it with GraphQL","[""io.kestra.plugin.fs.http.Download"", ""io.kestra.plugin.serdes.csv.CsvReader"", ""io.kestra.plugin.weaviate.BatchCreate"", ""io.kestra.plugin.weaviate.Query""]","This flow shows how to extract data from a CSV file using the HTTP API, load it to a Weaviate cluster and query it with GraphQL.

This flow assumes that you have a [Weaviate cluster](https://console.weaviate.cloud/) running, and that you created an API key. Make sure to replace the `url` and `apiKey` values in the tasks with your Weaviate credentials. It's recommended to use [Secrets](https://kestra.io/docs/developer-guide/secrets/) to store your API key.

Once you've configured the Weaviate secret, you can reproduce this flow without any changes. It will load the data from the [Jeopardy dataset](https://www.kaggle.com/tunguz/200000-jeopardy-questions) to Weaviate, and then query it with GraphQL.","id: weaviate_csv
namespace: blueprint

variables:
  host: https://demo-ito81rf6.weaviate.network
  secret: YOUR_WEAVIATE_API_KEY # in production, use a Secret instead e.g. ""{{ secret('WEAVIATE_API_KEY') }}""

tasks:
  - id: csv
    type: io.kestra.plugin.fs.http.Download
    uri: https://huggingface.co/datasets/kestra/datasets/raw/main/csv/trivia_questions.csv

  - id: csv_to_ion
    type: io.kestra.plugin.serdes.csv.CsvReader
    from: ""{{ outputs.csv.uri }}""

  - id: batch_load
    type: io.kestra.plugin.weaviate.BatchCreate
    url: ""{{ vars.host }}""
    apiKey: ""{{ vars.secret }}""
    className: QuestionsCsv
    objects: ""{{ outputs.csv_to_ion.uri }}""

  - id: query
    type: io.kestra.plugin.weaviate.Query
    url: ""{{ vars.host }}""
    apiKey: ""{{ vars.secret }}""
    query: | 
      {
        Get {
          QuestionsCsv(limit: 10) {
            answer
            category
            question
          }
        }
      }",2023-11-21 17:11:29.487,"API,Ingest,AI"
62,Trigger a single Airbyte Cloud sync on schedule,"[""io.kestra.plugin.airbyte.cloud.jobs.Sync"", ""io.kestra.core.models.triggers.types.Schedule""]","This flow will sync data using Airbyte Cloud. You can generate an API key in the [Airbyte’s Cloud developer portal](https://portal.airbyte.com/apiKeys).

It's best to set the Airbyte Cloud API key as a secret, but you can also paste it into the flow in plain text during testing.","id: airbyteCloudSync
namespace: blueprint
tasks:
  - id: data-ingestion
    type: io.kestra.plugin.airbyte.cloud.jobs.Sync
    connectionId: ac127cf2-9ae3-4f9b-9dd0-e3a0944d1447
    token: ""{{ secret('AIRBYTE_CLOUD_API_TOKEN') }}""

triggers:
  - id: everyMinute
    type: io.kestra.core.models.triggers.types.Schedule
    cron: ""*/1 * * * *""",2023-07-26 00:43:53.246,"Ingest,Schedule"
123,Parametrized flow with custom validators to ensure correct integer value range and Regex-based string pattern validation,"[""io.kestra.core.tasks.log.Log""]","This flow uses several input validators.

The `age` property must be within a valid range between `min` and `max` integer value.

The Regex expression `^student(\d+)?$` is used to validate that the input argument `user` is of type STRING and that it follows a given pattern:
- `^`: Asserts the start of the string.
- `student`: Matches the word ""student"".
- `\d`: Matches any digit (0-9).
- `+`: Asserts that there is one or more of the preceding token (i.e., one or more digits).
- `()?`: The parentheses group the digits together, and the question mark makes the entire group optional.
- `$`: Asserts the end of the string. This ensures that the string doesn't contain any characters after the optional digits.

With this pattern:
- ""student"" would be a match.
- ""student123"" would be a match.
- ""studentabc"" would not be a match because ""abc"" isn't a sequence of digits.

Try running this flow with various inputs or adjust the Regex pattern to see how the input validation works.
","id: regex-input
namespace: blueprint

inputs:
  - name: age
    type: INT
    defaults: 42
    required: false
    min: 18
    max: 64

  - name: user
    type: STRING
    defaults: student
    required: false
    validator: ^student(\d+)?$

tasks:
  - id: validator
    type: io.kestra.core.tasks.log.Log
    message: User {{ inputs.user }}, age {{ inputs.age }}",2023-08-24 23:11:24.228,Inputs
75,Add a list of strings to Redis,"[""io.kestra.plugin.redis.ListPop"", ""io.kestra.plugin.redis.ListPush""]","The `ListPush` task will append objects to an existing list. Iif the list already contains elements, using `ListPush` could result in duplicate entries. To prevent this, the `ListPop` task is used to empty the list before `ListPush` adds the new values.","id: redis_list
namespace: blueprint

variables:
  key: favorite_plugins

tasks:  
  - id: clear_list
    type: io.kestra.plugin.redis.ListPop
    url: redis://host.docker.internal:6379/0
    key: ""{{vars.key}}""
    maxRecords: 1

  - id: publish_list
    type: io.kestra.plugin.redis.ListPush
    url: redis://host.docker.internal:6379/0
    key: ""{{vars.key}}""
    from:
      - redis
      - duckdb
      - gcp
      - aws",2023-12-10 02:56:42.71,Ingest
128,Run a subflow for each value in parallel and wait for their completion — recommended pattern to iterate over hundreds or thousands of list items,"[""io.kestra.core.tasks.flows.EachParallel"", ""io.kestra.core.tasks.flows.Flow""]","First, create the following flow that we'll use as a parametrized subflow:

```yaml
id: mysubflow
namespace: blueprint

inputs:
  - name: myinput
    type: STRING
    defaults: kestra

tasks:
  - id: super_task
    type: io.kestra.core.tasks.debugs.Return
    format: hi from {{flow.id}} using input {{ inputs.myinput }} 
```

Then, create the flow `subflowForEachValue`.

This flow will trigger multiple executions of the flow `mysubflow`. Each execution will be triggered in parallel using input from the list of values. In this example, you should see three executions of the subflow, one with the input user1, another with the input user2 and yet another execution with the input user3. 

This pattern is particularly useful if the list of values you iterate over is large. As explained in the [Flow best practices documentation](https://kestra.io/docs/developer-guide/best-practice), it's better to execute thousands of flows, each running one task, than to trigger one flow with thousands of parallel tasks. We recommend this pattern with subflows as it's easier to scale and it makes your workflows more modular and easier to maintain. ","id: subflowForEachValue
namespace: blueprint

tasks:
  - id: parallel
    type: io.kestra.core.tasks.flows.EachParallel
    value: [""user1"", ""user2"", ""user3""]
    tasks:
      - id: subflow
        type: io.kestra.core.tasks.flows.Flow
        flowId: mysubflow
        namespace: blueprint
        wait: true
        transmitFailed: true
        inputs:
            myinput: ""{{ taskrun.value }}""

",2023-09-13 16:54:34.5,Parallel
126,Microservice orchestration: invoke multiple AWS Lambda functions in parallel,"[""io.kestra.core.tasks.flows.Parallel"", ""io.kestra.plugin.aws.lambda.Invoke"", ""io.kestra.plugin.scripts.shell.Commands""]","This flow invokes multiple AWS lambda functions in parallel. It demonstrates how you can:
1. Invoke a Lambda function based on its ARN, including invoking a specific `version` or `alias` of your function
2. Easily pass custom `functionPayload` in a simple dictionary format
3. Access the output of the Lambda result JSON payload and extract relevant data using `jq` 

To test this blueprint, you can create a simple function, e.g. in Python, named `demo` as follows:

```python
import json

def lambda_handler(event, context):
    print(event)
    return {
        'statusCode': 200,
        'body': json.dumps('Hello from Lambda!')
    }
```

And another one named `ResultHandler`:

```python
import json

def lambda_handler(event, context):
    function_result = event['your_event_input']
    return {
        'body': json.dumps(function_result + ' from Kestra')
    }
```

You can then create a custom version or alias for your function, if needed.","id: aws_lambda
namespace: blueprint

tasks:
  - id: parallel
    type: io.kestra.core.tasks.flows.Parallel
    tasks:
      - id: lambda
        type: io.kestra.plugin.aws.lambda.Invoke
        functionArn: arn:aws:lambda:eu-central-1:123456789:function:demo

      - id: lambdaVersion
        type: io.kestra.plugin.aws.lambda.Invoke
        functionArn: arn:aws:lambda:eu-central-1:123456789:function:ResultHandler:1
        functionPayload:
          your_event_input: hello

      - id: lambdaAlias
        type: io.kestra.plugin.aws.lambda.Invoke
        functionArn: arn:aws:lambda:eu-central-1:123456789:function:ResultHandler:kestra
        functionPayload:
          your_event_input: hey there

  - id: lambdaResult
    type: io.kestra.plugin.scripts.shell.Commands
    runner: PROCESS
    commands:
      - cat {{outputs.lambda.uri}} | jq -r '.body'",2023-09-12 09:53:25.605,"Serverless,AWS"
194,"Use Debezium to trigger a flow whenever new entries hit a Postgres database, then send notification to Slack and process data in Python","[""io.kestra.plugin.notifications.slack.SlackIncomingWebhook"", ""io.kestra.plugin.serdes.json.JsonWriter"", ""io.kestra.plugin.scripts.python.Script"", ""io.kestra.plugin.debezium.postgres.Trigger""]","This blueprint showcases how you can trigger a Flow based on Debezium data-capture pattern.
Every time new entries reach the database, the flow is triggered. It sends a notification through Slack with the number of rows ingested and then execute a Python script that read the corresponding data in json.


This blueprint can be reproduced with the following `docker-compose.yml` setup

```
services:

  db:
    image: debezium/postgres:latest
    restart: always
    environment:
      POSTGRES_PASSWORD: example
    ports:
      - 5433:5432

  adminer:
    image: adminer
    restart: always
    ports:
      - 8082:8080
 ```
 
 You can access localhost:8082 to create and edit databases or tables via the adminer interface. The database is accessible on `5433` port.
 
 Note that depending of your database installation, you might need to change the `pluginName` property of the debezium plugin. Other options can be seen in corresponding documentation.","id: listen-debezium
namespace: blueprint
tasks:

  - id: slack_notificaiton
    type: io.kestra.plugin.notifications.slack.SlackIncomingWebhook
    url: ""{{ secret('SLACK_WEBHOOK') }}""
    payload: |
      {
        ""channel"": ""U052JMPLBM3"",
        ""text"": ""{{ trigger.size }} new rows have been added to the database""
      }

  - id: json
    type: io.kestra.plugin.serdes.json.JsonWriter
    from: ""{{ trigger.uris['postgres.order'] }}""

  - id: python
    type: io.kestra.plugin.scripts.python.Script
    script: |
      import json

      with open(""{{ outputs.json.uri }}"", ""r"") as fopen:
        data = json.load(fopen)

      print(data)

triggers:
  - id: listen-debezium
    type: io.kestra.plugin.debezium.postgres.Trigger
    hostname: host.docker.internal
    port: ""5433""
    username: postgres
    password: example
    database: postgres
    pluginName: PGOUTPUT
    snapshotMode: INITIAL
    format: INLINE
    interval: PT3S
",2023-11-06 15:09:52.893,"Trigger,Postgres"
99,Run a SQL query with DuckDB on MotherDuck and get the result as a CSV file,"[""io.kestra.plugin.jdbc.duckdb.Query"", ""io.kestra.plugin.serdes.csv.CsvWriter""]","This flow demonstrates how you can query data stored on [MotherDuck](https://motherduck.com/) using DuckDB. The `CsvWriter` returns the output as a CSV file which you can download from the Execution page.
","id: motherduck
namespace: dev
description: |
  This flow demonstrates how you can query data using MotherDuck. The `CsvWriter` returns the output as a CSV file which you can download from the Execution page. 
tasks:
  - id: query
    type: io.kestra.plugin.jdbc.duckdb.Query
    sql: |
      SELECT by, COUNT(*) as nr_comments 
      FROM sample_data.hn.hacker_news
      GROUP BY by
      ORDER BY nr_comments DESC;
    store: true

  - id: csv
    type: io.kestra.plugin.serdes.csv.CsvWriter
    from: ""{{ outputs.query.uri }}""

taskDefaults:
  - type: io.kestra.plugin.jdbc.duckdb.Query
    values:
      url: ""jdbc:duckdb:md:my_db?motherduck_token={{ secret('MOTHERDUCK_TOKEN') }}""
      timeZoneId: Europe/Berlin
",2023-07-26 00:12:30.316,"DuckDB,SQL"
142,Install custom Node packages from package.json before running a Node.js script,"[""io.kestra.core.tasks.flows.WorkingDirectory"", ""io.kestra.core.tasks.storages.LocalFiles"", ""io.kestra.plugin.scripts.node.Script""]","This blueprint shows how to run a Node.js script with a custom package.json. If you don't want to write the file inline, you can add it from the UI editor or use the `git.Clone` task to retrieve the package.json file from a git repository.","id: node_custom_package
namespace: blueprint
tasks:

  - id: wd
    type: io.kestra.core.tasks.flows.WorkingDirectory
    tasks:
      - id: lc
        type: io.kestra.core.tasks.storages.LocalFiles
        inputs:
          package.json: |
            {
              ""name"": ""your_project_name"",
              ""version"": ""1.0.0"",
              ""type"": ""module"",
              ""dependencies"": {
                ""colors"": ""^1.4.0""
              }
            }

      - id: ""script""
        type: ""io.kestra.plugin.scripts.node.Script""
        beforeCommands:
          - npm install
        script: |
          import colors from 'colors';
          console.log(colors.red(""Hello""));",2023-10-12 23:29:32.455,Docker
152,Extract data from HackerNews and store it in a local Parquet file using CloudQuery CLI,"[""io.kestra.core.tasks.flows.WorkingDirectory"", ""io.kestra.core.tasks.storages.LocalFiles"", ""io.kestra.plugin.cloudquery.CloudQueryCLI""]","This flow shows how to extract data from HackerNews using CloudQuery and store it in a local Parquet file. 

You can use the same format to ingest data from any other source and load it to any destination supported by CloudQuery.

To avoid parsing CloudQuery TABLE and UUID variables as Kestra's native Pebble expressions, we use the `{% raw %}{{TABLE}}{% endraw %}` syntax.","id: cloudquery_sync_hn_to_parquet
namespace: blueprint

tasks:
  - id: wdir
    type: io.kestra.core.tasks.flows.WorkingDirectory
    tasks:
      - id: config
        type: io.kestra.core.tasks.storages.LocalFiles
        inputs:
          config.yml: |
            kind: source
            spec:
              name: hackernews
              path: cloudquery/hackernews
              version: v3.0.13
              tables: [""*""]
              destinations:
                - file
              spec:
                item_concurrency: 100
                start_time: ""{{ execution.startDate | dateAdd(-1, 'DAYS') }}""
            ---
            kind: destination
            spec:
              name: file
              path: cloudquery/file
              version: v3.4.8
              spec:
                path: ""{% raw %}{{TABLE}}/{{UUID}}.{{FORMAT}}{% endraw %}""
                format: parquet

      - id: hn_to_parquet
        type: io.kestra.plugin.cloudquery.CloudQueryCLI
        commands:
          - sync config.yml --log-console --log-level=warn",2023-10-12 18:53:22.208,"CLI,Ingest"
41,"Process partitions in parallel in Python, report outputs and metrics about number of rows and processing time for all partitioned files","[""io.kestra.plugin.scripts.python.Script"", ""io.kestra.core.tasks.flows.EachParallel""]","This flow extracts a list of partitions and then processes each partition in parallel in isolated Python scripts, each running in a separate Docker container. The flow will then track the number of rows and the processing time for each partition, which you can inspect in the Metrics tab.","id: pythonPartitionsMetrics
namespace: blueprint
description: Process partitions in parallel

tasks:
  - id: getPartitions
    type: io.kestra.plugin.scripts.python.Script
    runner: DOCKER
    docker:
      image: ghcr.io/kestra-io/pydata:latest
    script: |
      from kestra import Kestra
      partitions = [f""file_{nr}.parquet"" for nr in range(1, 10)]
      Kestra.outputs({'partitions': partitions})

  - id: processPartitions
    type: io.kestra.core.tasks.flows.EachParallel
    value: '{{outputs.getPartitions.vars.partitions}}'
    tasks:
      - id: partition
        type: io.kestra.plugin.scripts.python.Script
        runner: DOCKER
        docker:
          image: ghcr.io/kestra-io/pydata:latest
        script: |
          import random
          import time
          from kestra import Kestra

          filename = '{{ taskrun.value }}'
          print(f""Reading and processing partition {filename}"")
          nr_rows = random.randint(1, 1000)
          processing_time = random.randint(1, 20)
          time.sleep(processing_time)
          Kestra.counter('nr_rows', nr_rows, {'partition': filename})
          Kestra.timer('processing_time', processing_time, {'partition': filename})
",2023-07-25 22:49:16.223,"Python,Metrics,Outputs,Parallel"
46,Failure notifications to Slack to monitor the health of production workflows,"[""io.kestra.plugin.notifications.slack.SlackExecution"", ""io.kestra.core.models.triggers.types.Flow""]","This flow sends a Slack alert any time a flow from the `prod` namespace finishes with errors or warnings. Thanks to the `executionId` variable, the alert includes a link to the failed flow's execution page.

Given that this flow runs on a Flow trigger, there is no need for boilerplate code to define alert logic in each flow separately. Instead, the Flow trigger allows you to define that logic only once. The trigger will listen to the execution state of any flow in the `prod` namespace, including all child namespaces, and will automatically send Slack messages on failure.

This flow assumes that you stored the Slack webhook URL as a secret.","id: slackFailureAlert
namespace: prod.monitoring

tasks:
  - id: send
    type: io.kestra.plugin.notifications.slack.SlackExecution
    url: ""{{ secret('SLACK_WEBHOOK') }}""
    channel: ""#general""
    executionId: ""{{trigger.executionId}}""

triggers:
  - id: listen
    type: io.kestra.core.models.triggers.types.Flow
    conditions:
      - type: io.kestra.core.models.conditions.types.ExecutionStatusCondition
        in:
          - FAILED
          - WARNING
      - type: io.kestra.core.models.conditions.types.ExecutionNamespaceCondition
        namespace: prod
        prefix: true
",2023-07-21 08:40:19.858,"Trigger,Notifications"
32,Publish a message to an SQS queue,"[""io.kestra.plugin.aws.sqs.Publish""]","This flow publishes a message to an SQS queue. The queue URL points to an already existing queue.

This flow assumes AWS credentials stored as secrets `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY` and `AWS_DEFAULT_REGION`.","id: sqsPublishMessage
namespace: blueprint

inputs:
  - name: message
    type: STRING
    defaults: ""Hi from SQS!""

tasks:
  - id: publishMessage
    type: io.kestra.plugin.aws.sqs.Publish
    accessKeyId: ""{{ secret('AWS_ACCESS_KEY_ID') }}""
    secretKeyId: ""{{ secret('AWS_SECRET_ACCESS_KEY') }}""
    region: ""{{ secret('AWS_DEFAULT_REGION') }}""
    queueUrl: https://sqs.eu-central-1.amazonaws.com/123456789/kestra
    from:
      data: ""{{inputs.message}}""
",2023-10-12 15:44:29.768,"Inputs,AWS,Queue"
53,Git workflow for dbt with Amazon Redshift,"[""io.kestra.core.tasks.flows.WorkingDirectory"", ""io.kestra.plugin.git.Clone"", ""io.kestra.plugin.dbt.cli.Build""]","This flow:
- clones a dbt Git repository from GitHub
- pulls a public container image with the required dependencies
- runs dbt CLI commands within a container.

This flow assumes that your Redshift credentials are stored as secrets.

To run a flow from this blueprint, you can simply copy-paste the contents of your `~/.dbt/profiles.yml` into the `profiles` section and you're good to go!","id: dbtGitDockerRedshift
namespace: blueprint

tasks:
  - id: dbt
    type: io.kestra.core.tasks.flows.WorkingDirectory
    tasks:
    - id: cloneRepository
      type: io.kestra.plugin.git.Clone
      url: https://github.com/kestra-io/dbt-demo
      branch: main

    - id: dbtCore
      type: io.kestra.plugin.dbt.cli.DbtCLI
      docker:
        image: ghcr.io/kestra-io/dbt-redshift:latest
      profiles: |
        jaffle_shop:
          outputs:
            dev:
              type: redshift
              host: myhostname.us-east-1.redshift.amazonaws.com
              user: ""{{ secret('REDSHIFT_USER') }}""
              password: ""{{ secret('REDSHIFT_PASSWORD') }}""
              port: 5439
              dbname: analytics
              schema: dbt
              autocommit: true # autocommit after each statement
              threads: 8
              connect_timeout: 10
          target: dev
      commands:
        - dbt deps
        - dbt build
",2023-08-11 15:06:59.193,"dbt,AWS,Git"
7,Run two sequences in parallel,"[""io.kestra.core.tasks.flows.Parallel"", ""io.kestra.core.tasks.flows.Sequential"", ""io.kestra.core.tasks.debugs.Return""]",This blueprint shows how to run two independent task sequences in parallel.,"id: parallel_sequences
namespace: blueprint

tasks:
  - id: parallel
    type: io.kestra.core.tasks.flows.Parallel
    tasks:
      
      - id: sequence1
        type: io.kestra.core.tasks.flows.Sequential
        tasks:
          - id: task1
            type: io.kestra.core.tasks.debugs.Return
            format: ""{{ task.id }}""

          - id: task2
            type: io.kestra.core.tasks.debugs.Return
            format: ""{{ task.id }}""

      - id: sequence2
        type: io.kestra.core.tasks.flows.Sequential
        tasks:
          - id: task3
            type: io.kestra.core.tasks.debugs.Return
            format: ""{{ task.id }}""

          - id: task4
            type: io.kestra.core.tasks.debugs.Return
            format: ""{{ task.id }}""",2023-07-26 00:50:52.711,Parallel
100,"Extract data from an API and load it to Postgres using Python, Git and Docker (passing custom environment variables to the container)","[""io.kestra.core.tasks.flows.WorkingDirectory"", ""io.kestra.plugin.git.Clone"", ""io.kestra.plugin.scripts.python.Commands""]","This flow clones a Git repository with two Python scripts:
1. The first one extracts data from an API and stores the results into a file `users.json`
2. The second Python script reads that extracted raw data file and loads it to Postgres using Python and Pandas.

**The benefits of this approach:**
- your **orchestration logic** (YAML) is decoupled from your **business logic** (here: Python) -- different tasks can be maintained by different team members without stepping on each other's toes
- you can update your business logic without having to touch any of your orchestration code (_unless you want to, e.g. to modify the Docker image or pip package versions_)
- each task can run in a **separate Docker container**, making dependency management easier; for instance the second task needs additional libraries for Postgres and Pandas, and those can be installed at runtime using the `beforeCommands` property.","id: postgresS3PythonGit
namespace: blueprint

tasks:
  - id: wdir
    type: io.kestra.core.tasks.flows.WorkingDirectory
    tasks:
      - id: cloneRepository
        type: io.kestra.plugin.git.Clone
        url: https://github.com/kestra-io/scripts
        branch: main

      - id: getUsers
        type: io.kestra.plugin.scripts.python.Commands
        docker:
          image: ghcr.io/kestra-io/pydata:latest
        warningOnStdErr: false
        commands:
          - python etl/get_users_from_api.py

      - id: saveUsersPg
        type: io.kestra.plugin.scripts.python.Commands
        beforeCommands:
          - pip install pandas psycopg2 sqlalchemy > /dev/null
        warningOnStdErr: false
        commands:
          - python etl/save_users_pg.py
        env:
          DB_USERNAME: ""postgres""
          DB_PASSWORD: ""{{ secret('DB_PASSWORD') }}""
          DB_HOST: ""host.docker.internal""
          DB_PORT: ""5432""",2023-07-25 23:53:48.874,"pip,Python,Postgres,Git"
136,Query SurrealDB and store the result as a downloadable artifact,"[""io.kestra.plugin.surrealdb.Query""]","This flow will query SurrealDB and store the result in Kestra internal storage. The result of the query can be used by other tasks in the same flow using the syntax `{{ outputs.query.uri }}`.

To install and run SurrealDB, follow the instructions in the [SurrealDB documentation](https://surrealdb.com/docs/introduction/start).","id: surreal_db
namespace: blueprints

tasks:
  - id: article
    type: io.kestra.plugin.surrealdb.Query
    query: |
      CREATE article SET
        created_at = time::now(),
        author = author:john,
        title = 'Lorem ipsum dolor',
        text = 'Donec eleifend, nunc vitae commodo accumsan, mauris est fringilla.',
        account = (SELECT VALUE id FROM account WHERE name = 'ACME Inc' LIMIT 1)[0]
      ;

  - id: account
    type: io.kestra.plugin.surrealdb.Query
    query: |
      CREATE account SET
      name = 'ACME Inc',
      created_at = time::now()
      ;

  - id: query
    type: io.kestra.plugin.surrealdb.Query
    query: SELECT * FROM article, account;
    fetchType: STORE

  - id: query_condition
    type: io.kestra.plugin.surrealdb.Query
    query: SELECT * FROM article WHERE author.age < 30 FETCH author, account;
    fetchType: STORE

taskDefaults:
  - type: io.kestra.plugin.surrealdb.Query
    values:
      host: localhost
      database: test
      namespace: test
      username: root
      password: root
",2023-09-22 15:54:21.952,
38,Query a CSV file with DuckDB and send results via Slack every Monday,"[""io.kestra.plugin.fs.http.Download"", ""io.kestra.plugin.jdbc.duckdb.Query"", ""io.kestra.plugin.notifications.slack.SlackIncomingWebhook""]","This flow will read a CSV file, analyze it with DuckDB, and send the results via Slack.
This flow assumes `SLACK_WEBHOOK` [secret](https://kestra.io/docs/developer-guide/secrets). 

If you use [MotherDuck](https://motherduck.com/), use Kestra [secret](https://kestra.io/docs/developer-guide/secrets) to store the [MotherDuck service token](https://motherduck.com/docs/authenticating-to-motherduck). Then, add the `url` property to point the task to your MotherDuck database.

```yaml
  - id: transform
    type: io.kestra.plugin.jdbc.duckdb.Query
    url: ""jdbc:duckdb:md:my_db?motherduck_token={{ secret('MOTHERDUCK_TOKEN') }}""
```","id: csvDuckDBSlack
namespace: blueprint

tasks:
  - id: analyzeSales
    type: io.kestra.plugin.jdbc.duckdb.Query
    sql: |
      INSTALL httpfs;
      LOAD httpfs;
      SELECT sum(total) as total, avg(quantity) as avg_quantity
      FROM read_csv_auto('https://raw.githubusercontent.com/kestra-io/datasets/main/csv/orders.csv', header=True);
    fetch: true

  - id: slack
    type: io.kestra.plugin.notifications.slack.SlackIncomingWebhook
    url: ""{{ secret('SLACK_WEBHOOK') }}""
    payload: |
      {""channel"":""#reporting"",""text"":""Current Sales numbers: total sales is `${{outputs.analyzeSales.rows[0].total}}` and average sales quantity is `{{outputs.analyzeSales.rows[0].avg_quantity}}`""}

triggers:
  - id: everyMonday
    type: io.kestra.core.models.triggers.types.Schedule
    cron: 0 9 * * MON",2023-07-26 18:53:39.193,"Schedule,Notifications,SQL,DuckDB,Outputs"
150,Schedule a CloudQuery data ingestion sync with kestra,"[""io.kestra.plugin.cloudquery.Sync"", ""io.kestra.core.models.triggers.types.Schedule""]","This flow will start a batch job to sync data from HackerNews to DuckDB with CloudQuery. The sync process relies on CloudQuery source and destination plugins. The source plugin will extract data from HackerNews and the destination plugin will load it to DuckDB. 

This flow uses Kestra templating to dynamically configure the start time of the sync process to be 1 day before the scheduled date, allowing you to also backfill data for past intervals in small batches. 

Alternatively, you can toggle the `incremental` flag to `true` to enable incremental sync. During incremental syncs, Kestra will store the CloudQuery cursor in the Kestra backend, and will use it to always start the sync process from the last cursor position.

To get started configuring CloudQuery sources and destinations, go to [CloudQuery Integrations](https://www.cloudquery.io/integrations) page. From here, you can select your desired source and destination, and you'll see the YAML configuration that you can copy and paste into your own file, or into Kestra flow. This page will also provide more detailed instructions and references about tables available in each plugin.","id: cloudquery_sync
namespace: dev

tasks:
  - id: hn_to_duckdb
    type: io.kestra.plugin.cloudquery.Sync
    incremental: false
    configs:
      - kind: source
        spec:
          name: hackernews
          path: cloudquery/hackernews
          version: v3.0.13
          tables: [""*""]
          destinations: [""duckdb""]
          spec:
            item_concurrency: 100
            start_time: ""{{ trigger.date ?? execution.startDate | dateAdd(-1, 'DAYS') }}""
      - kind: destination
        spec:
          name: duckdb
          path: cloudquery/duckdb
          version: v4.2.10
          write_mode: overwrite-delete-stale
          spec:
            connection_string: hn.db

triggers:
  - id: schedule
    type: io.kestra.core.models.triggers.types.Schedule
    cron: ""@daily""
    timezone: US/Eastern
",2023-10-12 13:33:35.004,"CLI,Ingest"
139,CRUD operations in SurrealQL: run multiple SurrealDB queries and send the query results via Slack,"[""io.kestra.plugin.surrealdb.Query"", ""io.kestra.core.tasks.log.Log"", ""io.kestra.plugin.notifications.slack.SlackIncomingWebhook""]","This flow demonstrates how to use SurrealDB to run queries, create tables, as well as insert, update and delete data. The flow parses the final query result and sends it in a Slack message.

This flow assumes that the Slack Incoming Webhook URL is stored as a secret named `SLACK_WEBHOOK`.","id: surreal_db
namespace: blueprints

tasks:
  - id: company
    type: io.kestra.plugin.surrealdb.Query
    query: |
      CREATE company SET
      name = 'Kestra',
      created_at = time::now()

  - id: delete_anna
    type: io.kestra.plugin.surrealdb.Query
    query: DELETE author:anna;

  - id: add_author_tbl
    type: io.kestra.plugin.surrealdb.Query
    disabled: true
    query: |
      CREATE author:anna SET
      name.first = 'Anna',
      name.last = 'Geller',
      name.full = string::join(' ', name.first, name.last),
      admin = true

  - id: fix_admin_permission
    type: io.kestra.plugin.surrealdb.Query
    query: UPDATE author:anna SET admin = false WHERE name.last = 'Geller';

  - id: create_article_tbl
    type: io.kestra.plugin.surrealdb.Query
    query: |
      CREATE article SET
      created_at = time::now(),
      author = author:anna,
      title = 'Kestra 0.12 simplifies building modular, event-driven and containerized workflows',
      company = (SELECT VALUE id FROM company WHERE name = 'Kestra' LIMIT 1)[0]

  - id: query
    type: io.kestra.plugin.surrealdb.Query
    query: SELECT title FROM article;
    fetchType: FETCH_ONE

  - id: log_query_results
    type: io.kestra.core.tasks.log.Log
    message: ""{{ outputs.query.row }}""

  - id: slack
    type: io.kestra.plugin.notifications.slack.SlackIncomingWebhook
    url: ""{{ secret('SLACK_WEBHOOK') }}""
    payload: |
      {
        ""channel"": ""#general"",
        ""text"": ""{{ outputs.query.row.title }}""
      }

taskDefaults:
  - type: io.kestra.plugin.surrealdb.Query
    values:
      host: localhost
      database: test
      namespace: test
      username: root
      password: root",2023-09-28 09:27:35.922,
140,Deploy a Lambda function using the Serverless Framework,"[""io.kestra.core.tasks.flows.WorkingDirectory"", ""io.kestra.core.tasks.storages.LocalFiles"", ""io.kestra.plugin.scripts.node.Commands""]","This flow will deploy a Lambda function using the [Serverless Framework](https://www.serverless.com/). 

The `WorkingDirectory` task will create a temporary directory with the `serverless.yml` and `handler.py` files in it. 

The `Commands` task will install the Serverless Framework and deploy the Lambda function. Then, it will invoke the Lambda function and print its output to the logs. Optionally, the flow can also remove the Lambda function after the execution.
","id: lambda
namespace: blueprint

tasks:
  - id: working_dir
    type: io.kestra.core.tasks.flows.WorkingDirectory
    tasks:
    - id: deploy_lambda
      type: io.kestra.core.tasks.storages.LocalFiles
      inputs:
        serverless.yml: |
          service: lambda
          frameworkVersion: '3'

          provider:
            name: aws
            runtime: python3.9
            region: eu-central-1
            memorySize: 512 # optional, in MB, default is 1024; can be 128, 256, 512, 1024, 2048, 4096, 5120, ...
            timeout: 10 # optional, in seconds, default is 6

          functions:
            etl:
              handler: handler.run

        handler.py: |
          import platform
          import sys


          def extract() -> int:
              print(""Extracting data..."")
              return 21


          def transform(x: int) -> int:
              print(""Transforming data..."")
              return x * 2


          def load(x: int) -> None:
              print(f""Loading {x} into destination..."")


          def run(event=None, context=None):
              raw_data = extract()
              transformed = transform(raw_data)
              load(transformed)
              print(""Hello from Kestra 🚀"")
              print(f""Host's network name = {platform.node()}"")
              print(f""Python version = {platform.python_version()}"")
              print(f""Platform information (instance type) = {platform.platform()}"")
              print(f""OS/Arch = {sys.platform}/{platform.machine()}"")

    - id: sls_commands
      type: io.kestra.plugin.scripts.node.Commands
      description: npm install -g serverless
      runner: PROCESS
      warningOnStdErr: false
      commands:
        - sls deploy 
        - sls invoke -f etl --log
        # - sls remove
",2023-10-04 09:42:30.691,"Serverless,AWS"
193,Extract a CSV file via HTTP API and upload it to S3 by using scheduled date as a filename,"[""io.kestra.plugin.fs.http.Download"", ""io.kestra.plugin.compress.ArchiveDecompress"", ""io.kestra.plugin.aws.s3.Upload"", ""io.kestra.core.models.triggers.types.Schedule""]","This flow shows how to extract a CSV file from an HTTP API and upload it to S3. The flow is triggered every hour, and the uploaded file is named after the trigger date. The flow uses the [Global Power Plant Database](https://datasets.wri.org/dataset/globalpowerplantdatabase) as an example, but you can replace the URL with any other file accessible via an HTTP API call.
","id: upload_to_s3
namespace: blueprint

inputs:
  - name: bucket
    type: STRING
    defaults: declarative-data-orchestration

tasks:
  - id: get_zip_file
    type: io.kestra.plugin.fs.http.Download
    uri: https://wri-dataportal-prod.s3.amazonaws.com/manual/global_power_plant_database_v_1_3.zip

  - id: unzip
    type: io.kestra.plugin.compress.ArchiveDecompress
    algorithm: ZIP
    from: ""{{outputs.get_zip_file.uri}}""

  - id: csv_upload
    type: io.kestra.plugin.aws.s3.Upload
    from: ""{{ outputs.unzip.files['global_power_plant_database.csv'] }}""
    bucket: ""{{ inputs.bucket }}""
    key: ""powerplant/{{ trigger.date ?? execution.startDate | date('yyyy_MM_dd__HH_mm_ss') }}.csv""

triggers:
  - id: hourly
    type: io.kestra.core.models.triggers.types.Schedule
    cron: ""@hourly""",2023-11-01 09:35:10.785,"Variables,Trigger,S3,Ingest"
12,Task outputs,"[""io.kestra.core.tasks.debugs.Return"", ""io.kestra.core.tasks.log.Log""]",This blueprint shows how to use outputs from one task to another.,"id: task_outputs
namespace: blueprint

tasks:

  - id: task1
    type: io.kestra.core.tasks.debugs.Return
    format: ""Hello""

  - id: task2
    type: io.kestra.core.tasks.log.Log
    message: ""{{outputs.task1.value}} World!""
",2023-07-26 18:59:18.414,Outputs
137,Send multiple records to AWS Kinesis Data Streams in a simple list of maps or using a JSON API payload,"[""io.kestra.plugin.aws.kinesis.PutRecords"", ""io.kestra.plugin.fs.http.Download"", ""io.kestra.plugin.serdes.json.JsonReader""]","This flow will send multiple records to AWS Kinesis Data Streams.  The `PutRecords` task accepts either a simple list of maps (*i.e., a list of dictionaries*) or a URL of an internal storage file in ION format. When extracting data from an HTTP API, you must convert the JSON payload to ION format using the `JsonReader` task. Then, you can pass it to the `PutRecords` task.

The Outputs tab will display all records sent to AWS Kinesis Data Streams, including the shard ID and sequence number. You can use these values to retrieve the records from the stream using the `GetRecords` AWS API call.

Note that when sending the `data` payload, this must be a string value. If you want to send a JSON object to your Kinesis Data Stream, wrap it into a string, as shown in the [following Gist](https://gist.githubusercontent.com/anna-geller/0b8ca0df9f4af137fc92346b360be4a7/raw/f5e7c3e1831e3d9e83d7854d133740216d82ab8d/user_events2.json). In contrast, [this payload](https://gist.githubusercontent.com/anna-geller/1db78ffc652f26a9904959a0040ad73c/raw/d2e630127c634c20932835e50359e933ea10493f/user_events.json) will not work.
","id: aws_kinesis_json
namespace: blueprints

tasks:
  - id: put_records_simple_map
    type: io.kestra.plugin.aws.kinesis.PutRecords
    streamName: kestra
    records:
      - data: sign-in
        partitionKey: user1
      - data: sign-out
        partitionKey: user1

  - id: extract
    type: io.kestra.plugin.fs.http.Download
    uri: https://huggingface.co/datasets/kestra/datasets/resolve/main/json/user_events.json

  - id: json_to_ion
    type: io.kestra.plugin.serdes.json.JsonReader
    from: ""{{ outputs.extract.uri }}""
    newLine: false

  - id: put_records
    type: io.kestra.plugin.aws.kinesis.PutRecords
    streamName: kestra
    records: ""{{ outputs.json_to_ion.uri }}""
",2023-09-26 09:22:08.43,"API,AWS"
78,Azure Blob Storage file detection event triggers upload to BigQuery and dbt Cloud job,"[""io.kestra.core.tasks.flows.EachSequential"", ""io.kestra.plugin.gcp.bigquery.Load"", ""io.kestra.plugin.dbt.cloud.TriggerRun"", ""io.kestra.plugin.azure.storage.blob.Trigger""]","When a new file arrives in Azure Blob Storage, this flow processes that file and uploads it to BigQuery. The flow is configured to process multiple files in parallel if multiple files are uploaded to Azure BLOB container.

Then, the flow runs a dbt Cloud job and waits for its completion. Kestra retrieves dbt Cloud job's artifacts and dynamically parses them within the Gantt view. This way, you can inspect how long it took to run each dbt model and dbt test. 

This flow assumes that sensitive information such as Azure Connection string, Google Service Account, and dbt Cloud API token are stored as secrets. For testing, you can copy-paste those directly into the flow. ","id: azureBlobToBigQuery
namespace: blueprint

tasks:
  - id: each
    type: io.kestra.core.tasks.flows.EachParallel
    value: ""{{ trigger.blobs | jq('.[].uri') }}""
    tasks:
      - id: uploadFromFile
        type: io.kestra.plugin.gcp.bigquery.Load
        destinationTable: gcpProject.dataset.table
        from: ""{{taskrun.value}}""
        writeDisposition: ""WRITE_APPEND""
        projectId: yourGcpProject
        serviceAccount: ""{{ secret('GCP_CREDS') }}""
        ignoreUnknownValues: true
        autodetect: true
        format: CSV
        csvOptions:
          allowJaggedRows: true
          encoding: UTF-8
          fieldDelimiter: "",""
  
  - id: dbtCloudJob
    type: io.kestra.plugin.dbt.cloud.TriggerRun
    accountId: ""{{ secret('DBT_CLOUD_ACCOUNT_ID') }}""
    token: ""{{ secret('DBT_CLOUD_API_TOKEN') }}""
    jobId: ""366381""
    wait: true

triggers:
  - id: watch
    type: io.kestra.plugin.azure.storage.blob.Trigger
    interval: PT1S
    endpoint: ""https://kestra.blob.core.windows.net""
    connectionString: ""{{ secret('AZURE_CONNECTION_STRING') }}""
    container: ""stage""
    prefix: ""marketplace/""
    action: MOVE
    moveTo: 
      container: stage
      name: archive/marketplace/
",2023-07-25 23:47:21.869,"dbt,BigQuery,Ingest,Azure,Trigger"
24,"Download a zip file, unzip it and upload all files in parallel to AWS S3 - using taskDefaults to avoid boilerplate code","[""io.kestra.plugin.fs.http.Download"", ""io.kestra.plugin.compress.ArchiveDecompress"", ""io.kestra.core.tasks.flows.Parallel"", ""io.kestra.plugin.aws.s3.Upload""]","This flow downloads a zip file, unzips it, and uploads all files to S3 in parallel.

This flow assumes AWS credentials stored as secrets `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY` and `AWS_DEFAULT_REGION` configured using `taskDefaults` to avoid boilerplate configuration.","id: s3parallelUploads
namespace: blueprint

inputs:
  - name: bucket
    type: STRING
    defaults: declarative-data-orchestration

tasks:
  - id: getZipFile
    type: io.kestra.plugin.fs.http.Download
    uri: https://wri-dataportal-prod.s3.amazonaws.com/manual/global_power_plant_database_v_1_3.zip

  - id: unzip
    type: io.kestra.plugin.compress.ArchiveDecompress
    algorithm: ZIP
    from: ""{{outputs.getZipFile.uri}}""

  - id: parallelUploadToS3
    type: io.kestra.core.tasks.flows.Parallel
    tasks:
      - id: csv
        type: io.kestra.plugin.aws.s3.Upload
        from: ""{{outputs.unzip.files['global_power_plant_database.csv']}}""
        key: powerplant/global_power_plant_database.csv
      
      - id: pdf
        type: io.kestra.plugin.aws.s3.Upload
        from: ""{{outputs.unzip.files['Estimating_Power_Plant_Generation_in_the_Global_Power_Plant_Database.pdf']}}""
        key: powerplant/Estimating_Power_Plant_Generation_in_the_Global_Power_Plant_Database.pdf
      
      - id: txt
        type: io.kestra.plugin.aws.s3.Upload
        from: ""{{outputs.unzip.files['RELEASE_NOTES.txt']}}""
        key: powerplant/RELEASE_NOTES.txt

taskDefaults:
  - type: io.kestra.plugin.aws.s3.Upload
    values:
      accessKeyId: ""{{ secret('AWS_ACCESS_KEY_ID') }}""
      secretKeyId: ""{{ secret('AWS_SECRET_ACCESS_KEY') }}""
      region: ""{{ secret('AWS_DEFAULT_REGION') }}""
      bucket: ""{{inputs.bucket}}""",2023-07-26 20:18:44.027,"Parallel,S3,Inputs,Outputs"
191,Stream kestra audit logs from a Kafka topic to BigQuery for analytics and troubleshooting,"[""io.kestra.plugin.kafka.Consume"", ""io.kestra.plugin.scripts.nashorn.FileTransform"", ""io.kestra.plugin.serdes.avro.AvroWriter"", ""io.kestra.plugin.gcp.bigquery.Load"", ""io.kestra.core.models.triggers.types.Schedule""]","This flow shows how to stream Kestra audit logs to BigQuery. The audit logs are stored in the `kestra_auditlogs` Kafka topic. The flow consumes the audit logs from the Kafka topic, transforms the data, and loads it into BigQuery. 

The flow is triggered every day at 10 AM UTC. You can customize the trigger by changing the cron expression, timezone and more. For more information about cron expressions, visit the [following documentation](https://kestra.io/docs/developer-guide/triggers/schedule).","id: auditlogs_to_bigquery
namespace: blueprint

tasks:
  - id: consume
    type: io.kestra.plugin.kafka.Consume
    properties:
      auto.offset.reset: earliest
      bootstrap.servers: local:9092
    topic: kestra_auditlogs
    valueDeserializer: JSON
    maxRecords: 500

  - id: transform
    type: io.kestra.plugin.scripts.nashorn.FileTransform
    from: ""{{ outputs.consume.uri }}""
    script: |
      var jacksonMapper = Java.type('io.kestra.core.serializers.JacksonMapper');
      delete row['headers'];

      var value = row['value']

      row['id'] = value['id']
      row['type'] = value['type']
      row['detail'] = value['detail']
      row['date'] = value['date']
      row['deleted'] = value['deleted']
      row['value'] = jacksonMapper.ofJson().writeValueAsString(value)
      row['detail_type'] = value['detail']['type']
      row['detail_cls'] = value['detail']['cls']
      row['detail_permission'] = value['detail']['permission']
      row['detail_id'] = value['detail']['id']
      row['detail_namespace'] = value['detail']['namespace']
      row['detail_flowId'] = value['detail']['flowId']
      row['detail_executionId'] = value['detail']['executionId']

  - id: avroWriter
    type: io.kestra.plugin.serdes.avro.AvroWriter
    from: ""{{ outputs.transform.uri }}""
    description: convert the file from Kestra internal storage to avro.
    schema: |
      {
        ""type"": ""record"",
        ""name"": ""Root"",
        ""fields"":
          [
            { ""name"": ""id"", ""type"": [""null"", ""string""] },
            { ""name"": ""type"", ""type"": [""null"", ""string""] },
            { ""name"": ""detail"", ""type"": [""null"", ""string""] },
            { ""name"": ""date"", ""type"": [""null"", ""string""] },
            { ""name"": ""deleted"", ""type"": [""null"", ""string""] },
            { ""name"": ""value"", ""type"": [""null"", ""string""] },
            { ""name"": ""detail_type"", ""type"": [""null"", ""string""] },
            { ""name"": ""detail_cls"", ""type"": [""null"", ""string""] },
            { ""name"": ""detail_permission"", ""type"": [""null"", ""string""] },
            { ""name"": ""detail_id"", ""type"": [""null"", ""string""] },
            { ""name"": ""detail_namespace"", ""type"": [""null"", ""string""] },
            { ""name"": ""detail_flowId"", ""type"": [""null"", ""string""] },
            { ""name"": ""detail_executionId"", ""type"": [""null"", ""string""] }
          ]
      }

  - id: load
    type: io.kestra.plugin.gcp.bigquery.Load
    avroOptions:
      useAvroLogicalTypes: true
    destinationTable: your_gcp_project.dwh.autditlogs
    format: AVRO
    from: ""{{outputs.avroWriter.uri }}""
    writeDisposition: WRITE_TRUNCATE
    serviceAccount: ""{{ secret('GCP_CREDS') }}""
    projectId: your_gcp_project

triggers:
  - id: schedule
    type: io.kestra.core.models.triggers.types.Schedule
    cron: ""0 10 * * *""
",2023-10-26 17:52:28.177,"Outputs,Trigger,BigQuery,Ingest"
25,List objects in an S3 bucket and process them in parallel,"[""io.kestra.plugin.aws.s3.List"", ""io.kestra.core.tasks.log.Log"", ""io.kestra.core.tasks.flows.EachParallel""]","This flow lists objects with a specific prefix in an S3 bucket and then processes each object in parallel. 

This flow assumes AWS credentials stored as secrets `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY` and `AWS_DEFAULT_REGION`.
","id: s3MapOverObjects
namespace: blueprint

inputs:
  - name: bucket
    type: STRING
    defaults: declarative-data-orchestration

tasks:
  - id: listObjects
    type: io.kestra.plugin.aws.s3.List
    bucket: ""{{inputs.bucket}}""
    prefix: powerplant/
    accessKeyId: ""{{ secret('AWS_ACCESS_KEY_ID') }}""
    secretKeyId: ""{{ secret('AWS_SECRET_ACCESS_KEY') }}""
    region: ""{{ secret('AWS_DEFAULT_REGION') }}""

  - id: printObjects
    type: io.kestra.core.tasks.log.Log
    message: ""found objects {{outputs.listObjects.objects}}""

  - id: mapOverS3Objects
    type: io.kestra.core.tasks.flows.EachParallel
    value: ""{{outputs.listObjects.objects}}""
    tasks: # all tasks listed here will run in parallel
    - id: filename
      type: io.kestra.core.tasks.log.Log
      message: ""filename {{json(taskrun.value).key}} with size {{json(taskrun.value).size}}""
",2023-07-26 20:22:14.725,"Parallel,S3,Outputs,Inputs,Variables"
141,Deploy Terraform resources defined in a GitHub repository using S3 remote backend,"[""io.kestra.core.tasks.flows.WorkingDirectory"", ""io.kestra.plugin.git.Clone"", ""io.kestra.core.tasks.storages.LocalFiles"", ""io.kestra.plugin.scripts.shell.Commands""]","This flow will clone a Git repository and run Terraform commands to deploy the infrastructure resources defined in code. 

The repository already specifies a remote S3 backend, so the state will be stored in S3. Check the [main.tf](https://github.com/anna-geller/kestra-ci-cd/blob/main/main.tf) file for more details.

You can run that flow on schedule to follow GitOps principles:

```yaml
triggers:
  - id: schedule
    type: io.kestra.core.models.triggers.types.Schedule
    cron: ""*/15 * * * *""
```

Or you can add a [GitHub webhook trigger](https://levelup.gitconnected.com/when-github-actions-get-painful-to-troubleshoot-try-this-instead-9a134c9e9baf) to run this flow anytime a Pull Request is merged to your repository, effectively implementing CI/CD in Kestra:

```yaml
triggers:
  - id: github
    type: io.kestra.core.models.triggers.types.Webhook
    key: ""{{ secret('WEBHOOK_KEY') }}""
```

Note that for a private GitHub repository, you should add a `username` and `password`. Also, if your terraform configuration is stored in a different directory within the Git repository, you can add the global `-chdir` flag to all terraform commands e.g.:

```
terraform init -chdir=environment/production
terraform apply -auto-approve -chdir=environment/production
```","id: git_terraform
namespace: blueprint

tasks:
  - id: git
    type: io.kestra.core.tasks.flows.WorkingDirectory
    tasks:
      - id: clone_repository
        type: io.kestra.plugin.git.Clone
        url: https://github.com/anna-geller/kestra-ci-cd
        branch: main

      - id: variables
        type: io.kestra.core.tasks.storages.LocalFiles
        inputs:
          terraform.tfvars: |
            username            = ""cicd""
            password            = ""{{ secret('CI_CD_PASSWORD') }}""
            hostname            = ""https://demo.kestra.io""

      - id: terraform
        type: io.kestra.plugin.scripts.shell.Commands
        docker:
          image: hashicorp/terraform
          entryPoint: [""""]
        beforeCommands:
          - terraform init
        commands:
          - terraform apply -auto-approve
        env:
          AWS_ACCESS_KEY_ID: ""{{ secret('AWS_ACCESS_KEY_ID') }}""
          AWS_SECRET_ACCESS_KEY: ""{{ secret('AWS_SECRET_ACCESS_KEY') }}""
          AWS_DEFAULT_REGION: ""{{ secret('AWS_DEFAULT_REGION') }}""
",2023-10-05 11:45:44.756,"Trigger,Docker,Git"
122,Schedule a Python data ingestion job to extract data from an API and load it to DuckDB using dltHub (data load tool),"[""io.kestra.plugin.scripts.python.Script"", ""io.kestra.core.models.triggers.types.Schedule""]",This flow loads data from the Chess.com API into DuckDB destination. The flow is scheduled to run daily at 9 AM.,"id: dlt
namespace: blueprint

tasks:
  - id: chessAPI-to-DuckDB
    type: io.kestra.plugin.scripts.python.Script
    docker:
      image: python:slim
    beforeCommands:
      - pip install dlt[duckdb]
    warningOnStdErr: false
    script: |
      import dlt
      import requests

      pipeline = dlt.pipeline(
          pipeline_name='chess_pipeline',
          destination='duckdb',
          dataset_name='player_data'
      )
      data = []
      for player in ['magnuscarlsen', 'rpragchess']:
          response = requests.get(f'https://api.chess.com/pub/player/{player}')
          response.raise_for_status()
          data.append(response.json())
      # Extract, normalize, and load the data
      pipeline.run(data, table_name='player')

triggers:
  - id: daily
    type: io.kestra.core.models.triggers.types.Schedule
    disabled: true # enable schedule if needed
    cron: ""0 9 * * *""",2023-08-23 10:27:18.497,"Schedule,Ingest"
94,Create a Spark job on a Databricks cluster and wait for its completion,"[""io.kestra.plugin.databricks.job.CreateJob"", ""io.kestra.core.tasks.log.Log""]","This flow will start a job on an existing Databricks cluster. The Python script referenced on `pythonFile` property is stored in the Databricks workspace. The flow will wait up to five hours for the task to complete before running the next task (here, simply printing a log message to the console).","id: startJobOnExistingCluster
namespace: blueprint

tasks:
  - id: createJob
    type: io.kestra.plugin.databricks.job.CreateJob
    authentication:
      token: ""{{ secret('DATABRICKS_TOKEN') }}""
    host: ""{{ secret('DATABRICKS_HOST') }}""
    jobTasks:
      - existingClusterId: abcdefg12345678
        taskKey: yourArbitraryTaskKey
        sparkPythonTask:
          pythonFile: /Shared/hello.py
          sparkPythonTaskSource: WORKSPACE
    waitForCompletion: PT5H

  - id: logStatus
    type: io.kestra.core.tasks.log.Log
    message: The job finished, all done!",2023-07-20 12:19:58.717,Databricks
154,Scrape StackOverflow using AutoScraper in Python,"[""io.kestra.plugin.scripts.python.Script"", ""io.kestra.core.tasks.debugs.Return""]","This flow shows how to scrape a web page using AutoScraper in Python. It uses the [AutoScraper](https://github.com/alirezamika/autoscraper) library to extract data from StackOverflow, and the Kestra Python SDK to send the output from a Python script to Kestra. This way, you can pass data between Python scripts and other Kestra tasks.","id: autoscraper
namespace: blueprint

tasks:
  - id: scrape
    type: io.kestra.plugin.scripts.python.Script
    beforeCommands:
      - pip install autoscraper kestra
    warningOnStdErr: false
    script: |
      from autoscraper import AutoScraper
      from kestra import Kestra
      
      url = ""https://stackoverflow.com/questions/2081586/web-scraping-with-python""
      
      # You can also put urls here to retrieve urls.
      wanted_list = [""What are metaclasses in Python?""]
      
      scraper = AutoScraper()
      result = scraper.build(url, wanted_list)

      # get related topics of any stackoverflow page:
      related = scraper.get_result_similar(
          ""https://stackoverflow.com/questions/606191/convert-bytes-to-a-string""
      )
      Kestra.outputs({""data"": result, ""related"": related})


  - id: use_output_data
    type: io.kestra.core.tasks.debugs.Return
    format: ""{{ outputs.scrape.vars.data }}""

  - id: use_output_related
    type: io.kestra.core.tasks.debugs.Return
    format: ""{{ outputs.scrape.vars.related }}""",2023-10-13 10:36:18.28,Python
49,Git workflow for dbt with Google BigQuery,"[""io.kestra.core.tasks.flows.WorkingDirectory"", ""io.kestra.plugin.git.Clone"", ""io.kestra.plugin.dbt.cli.Build""]","This flow:
- clones a dbt Git repository from GitHub, 
- pulls a public container image with the required dependencies
- runs dbt CLI commands within a container.

This flow assumes that your Google Cloud Service Account JSON key file's content is stored as a secret named `GCP_CREDS`. 

Before you run this blueprint, simply copy-paste the contents of your `~/.dbt/profiles.yml` into the `profiles` section and you're good to go!","id: dbtGitDockerBigQuery
namespace: blueprint

tasks:
  - id: dbt
    type: io.kestra.core.tasks.flows.WorkingDirectory
    tasks:
    - id: cloneRepository
      type: io.kestra.plugin.git.Clone
      url: https://github.com/kestra-io/dbt-demo
      branch: main

    - id: serviceAccount
      type: io.kestra.core.tasks.storages.LocalFiles
      inputs:
        sa.json: ""{{ secret('GCP_CREDS') }}""

    - id: dbtCore
      type: io.kestra.plugin.dbt.cli.DbtCLI
      docker:
        image: ghcr.io/kestra-io/dbt-bigquery:latest
      profiles: |
        jaffle_shop:
          outputs:
            dev:
              type: bigquery
              dataset: your_big_query_dataset_name
              project: your_big_query_project
              keyfile: sa.json
              location: EU
              method: service-account
              priority: interactive
              threads: 16
              timeout_seconds: 300
              fixed_retries: 1
          target: dev
      commands:
        - dbt deps
        - dbt build
",2023-08-15 18:54:13.445,"Git,BigQuery,dbt"
47,Extract data via an HTTP API call and upload it as a file to Azure Blog Storage,"[""io.kestra.plugin.fs.http.Download"", ""io.kestra.plugin.azure.storage.blob.Upload""]",This flow downloads a file via an HTTPS request and then upload it to Azure Blob Storage.,"id: azureBlobUpload
namespace: blueprint

tasks:
  - id: extract
    type: io.kestra.plugin.fs.http.Download
    uri: https://raw.githubusercontent.com/kestra-io/datasets/main/csv/salaries.csv

  - id: load
    type: io.kestra.plugin.azure.storage.blob.Upload
    endpoint: ""https://kestra.blob.core.windows.net""
    connectionString: ""{{ secret('AZURE_CONNECTION_STRING') }}""
    container: kestra
    from: ""{{ outputs.download_data.uri }}""
    name: data.csv",2023-07-27 00:13:47.765,"Azure,Outputs"
33,React to an SQS trigger,"[""io.kestra.plugin.scripts.shell.Commands"", ""io.kestra.plugin.aws.sqs.Trigger""]","This flow reacts to an SQS trigger. Any time there is a new message in the queue, the flow is triggered.

The queue URL points to an already existing queue. The `{{trigger.uri}}` points to a file in Kestra's internal storage containing the content of the SQS message. You can read the contents of that file in any task.

This flow assumes AWS credentials stored as secrets `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY` and `AWS_DEFAULT_REGION`.","id: reactToSqsTrigger
namespace: blueprint

tasks:
  - id: printMessage
    type: io.kestra.plugin.scripts.shell.Commands
    runner: PROCESS
    commands:
      - cat ""{{trigger.uri}}""

triggers:
  - id: sqs
    type: io.kestra.plugin.aws.sqs.Trigger
    accessKeyId: ""{{ secret('AWS_ACCESS_KEY_ID') }}""
    secretKeyId: ""{{ secret('AWS_SECRET_ACCESS_KEY') }}""
    region: ""{{ secret('AWS_DEFAULT_REGION') }}""
    queueUrl: https://sqs.eu-central-1.amazonaws.com/338306982838/kestra
    maxRecords: 1
",2023-10-12 15:44:40.162,"Variables,Trigger,AWS,Queue"
1,Run a flow on the first Monday of each month at 9 AM and backfill past intervals,"[""io.kestra.core.tasks.log.Log"", ""io.kestra.core.models.triggers.types.Schedule"", ""io.kestra.core.models.conditions.types.DayWeekInMonthCondition""]","This flow will run only on the first Monday of each month at 9 AM. Before starting the regularly scheduled runs, Kestra will first execute runs for the past intervals from the `start` date until the current date. ","id: backfill_past_mondays
namespace: blueprint
    
tasks:
  - id: log_trigger_or_execution_date
    type: io.kestra.core.tasks.log.Log
    message: ""{{ trigger.date ?? execution.startDate }}""

triggers:
  - id: firstMondayOfTheMonth
    type: io.kestra.core.models.triggers.types.Schedule
    timezone: Europe/Berlin
    backfill:
      start: 2023-04-01T00:00:00Z
    cron: ""0 11 * * MON"" # at 11 on every Monday
    scheduleConditions: # only first Monday of the month
      - type: io.kestra.core.models.conditions.types.DayWeekInMonthCondition
        date: ""{{ trigger.date }}""
        dayOfWeek: ""MONDAY""
        dayInMonth: ""FIRST""
",2023-10-12 13:23:26.688,"Backfills,Schedule,Variables"
76,Consume data from a Kafka topic and write it to a JSON file,"[""io.kestra.plugin.kafka.Consume"", ""io.kestra.plugin.serdes.json.JsonWriter""]",This flow consumes data from a Kafka topic and writes it to a JSON file.,"id: consumeKafkaMessages
namespace: blueprint

tasks:
  - id: consume
    type: io.kestra.plugin.kafka.Consume
    topic: topic_test
    properties:
      bootstrap.servers: 'localhost:9093'
      auto.offset.reset: earliest
    pollDuration: PT20S
    maxRecords: 50
    keyDeserializer: STRING
    valueDeserializer: JSON

  - id: writeJson
    type: io.kestra.plugin.serdes.json.JsonWriter
    newLine: true
    from: ""{{ outputs.consume.uri }}""",2023-10-12 15:43:36.507,"Outputs,Queue"
106,Run dbt CLI commands in one container: dbt deps & dbt build,"[""io.kestra.core.tasks.flows.WorkingDirectory"", ""io.kestra.plugin.git.Clone"", ""io.kestra.core.tasks.storages.LocalFiles"", ""io.kestra.plugin.scripts.shell.Commands""]","This flow runs multiple dbt commands in a single container. 

This approach can be useful if you want to run several dbt commands and you want to run them all in one container to minimize latency.","id: dbtGitDockerDuckDB
namespace: blueprint

tasks:
  - id: dbt
    type: io.kestra.core.tasks.flows.WorkingDirectory
    tasks:
    - id: cloneRepository
      type: io.kestra.plugin.git.Clone
      url: https://github.com/kestra-io/dbt-demo
      branch: main

    - id: packagesConfig
      type: io.kestra.core.tasks.storages.LocalFiles
      inputs:
        packages.yml: |
          packages:
            - package: brooklyn-data/dbt_artifacts
              version: 2.4.2  

    - id: dbtCore
      type: io.kestra.plugin.dbt.cli.DbtCLI
      runner: DOCKER
      docker:
        image: ghcr.io/kestra-io/dbt-duckdb:latest
      profiles: |
        jaffle_shop:
          outputs:
            dev:
              type: duckdb
              path: "":memory:""
              extensions: 
                - parquet
              fixed_retries: 1
              threads: 16
              timeout_seconds: 300
          target: dev
      commands:
        - dbt deps
        - dbt build
",2023-08-11 15:14:08.019,"Local files,CLI,Git,dbt,DuckDB"
18,Trigger multiple Airbyte syncs in parallel,"[""io.kestra.core.tasks.flows.Parallel"", ""io.kestra.plugin.airbyte.connections.Sync""]","This flow syncs data from multiple sources in parallel using Airbyte.

The Airbyte server credentials, referenced in the `taskDefaults`, are stored as secrets. The `taskDefaults` flow property helps remove boilerplate code, allowing to define common values such as `url`, `username` and `password` in one place.
  
This flow executes three Airbyte connections in parallel. To control how many tasks run in parallel, use the `concurrent` property of the `io.kestra.core.tasks.flows.Parallel` task.
","id: airbyteSyncParallel
namespace: blueprint

tasks:
  - id: data-ingestion
    type: io.kestra.core.tasks.flows.Parallel
    tasks:
      - id: salesforce
        type: io.kestra.plugin.airbyte.connections.Sync
        connectionId: e3b1ce92-547c-436f-b1e8-23b6936c12ab
      - id: google-analytics
        type: io.kestra.plugin.airbyte.connections.Sync
        connectionId: e3b1ce92-547c-436f-b1e8-23b6936c12cd
      - id: facebook-ads
        type: io.kestra.plugin.airbyte.connections.Sync
        connectionId: e3b1ce92-547c-436f-b1e8-23b6936c12ef

taskDefaults:
  - type: io.kestra.plugin.airbyte.connections.Sync
    values:
      url: http://host.docker.internal:8000/
      username: ""{{ secret('AIRBYTE_USERNAME') }}""
      password: ""{{ secret('AIRBYTE_PASSWORD') }}""",2023-07-26 00:52:06.599,"Ingest,Parallel"
64,"Trigger multiple Airbyte Cloud syncs in parallel, then run a dbt job","[""io.kestra.core.tasks.flows.Parallel"", ""io.kestra.plugin.airbyte.cloud.jobs.Sync"", ""io.kestra.core.tasks.flows.WorkingDirectory"", ""io.kestra.plugin.git.Clone"", ""io.kestra.plugin.dbt.cli.Build""]","This flow runs Airbyte Cloud syncs in parallel and then runs dbt Core's CLI commands. 

It's recommended to configure the GCP service account and [Airbyte API token](https://portal.airbyte.com/apiKeys), referenced in the `taskDefaults`, as a secret.","id: airbyteCloudDbt
namespace: blueprint

tasks:
  - id: data-ingestion
    type: io.kestra.core.tasks.flows.Parallel
    tasks:
      - id: salesforce
        type: io.kestra.plugin.airbyte.cloud.jobs.Sync
        connectionId: e3b1ce92-547c-436f-b1e8-23b6936c12ab
      - id: google-analytics
        type: io.kestra.plugin.airbyte.cloud.jobs.Sync
        connectionId: e3b1ce92-547c-436f-b1e8-23b6936c12cd
      - id: facebook-ads
        type: io.kestra.plugin.airbyte.cloud.jobs.Sync
        connectionId: e3b1ce92-547c-436f-b1e8-23b6936c12ef
  
  - id: dbt
    type: io.kestra.core.tasks.flows.WorkingDirectory
    tasks:
      - id: cloneRepository
        type: io.kestra.plugin.git.Clone
        url: https://github.com/kestra-io/dbt-demo
        branch: main
      
      - id: dbt-build
        type: io.kestra.plugin.dbt.cli.Build
        runner: DOCKER
        dbtPath: /usr/local/bin/dbt
        dockerOptions:
          image: ghcr.io/kestra-io/dbt-bigquery:latest
        inputFiles:
          .profile/profiles.yml: |
            jaffle_shop:
              outputs:
                dev:
                  type: bigquery
                  dataset: your_big_query_dataset_name
                  project: your_big_query_project
                  fixed_retries: 1
                  keyfile: sa.json
                  location: EU
                  method: service-account
                  priority: interactive
                  threads: 8
                  timeout_seconds: 300
              target: dev
          sa.json: ""{{ secret('GCP_CREDS') }}""

taskDefaults:
  - type: io.kestra.plugin.airbyte.cloud.jobs.Sync
    values:
      token: ""{{ secret('AIRBYTE_CLOUD_API_TOKEN') }}""
",2023-07-26 03:01:22.145,"Ingest,BigQuery,dbt,Git,Parallel"
110,"Anomaly detection using DuckDB SQL query and S3 file event trigger, sending a CSV file attachment via email if anomalies are detected","[""io.kestra.plugin.jdbc.duckdb.Query"", ""io.kestra.plugin.serdes.csv.CsvWriter"", ""io.kestra.core.tasks.flows.If"", ""io.kestra.plugin.notifications.mail.MailSend"", ""io.kestra.plugin.aws.s3.Trigger""]","This flow will be triggered any time a new file arrives in a given S3 `bucket` and `source_prefix` folder. 

The flow will check for anomalies in the data from that file using a DuckDB query, and will move the file to the same S3 bucket below the `destination_prefix` folder.

If anomalies are detected, the flow will send an email to the recipients specified on the `to` property, and will send anomalous rows as a CSV file attachment in the same email.

If you use [MotherDuck](https://motherduck.com/), use Kestra [secret](https://kestra.io/docs/developer-guide/secrets) to store the [MotherDuck service token](https://motherduck.com/docs/authenticating-to-motherduck). Then, modify the `query` task as follows to point the task to your MotherDuck database:

```yaml
  - id: query
    type: io.kestra.plugin.jdbc.duckdb.Query
    description: Validate new file for anomalies
    sql: |
      SELECT *
      FROM read_csv_auto('s3://{{vars.bucket}}/{{vars.destination_prefix}}/{{ trigger.objects | jq('.[].key') | first }}')
      WHERE price * quantity != total;
    store: true
    url: ""jdbc:duckdb:md:my_db?motherduck_token={{ secret('MOTHERDUCK_TOKEN') }}""
```

","id: s3TriggerDuckDB
namespace: blueprint

variables:
  bucket: kestraio
  source_prefix: monthly_orders
  destination_prefix: stage_orders

tasks:
  - id: query
    type: io.kestra.plugin.jdbc.duckdb.Query
    description: Validate new file for anomalies
    sql: |
      INSTALL httpfs;
      LOAD httpfs;
      SET s3_region='{{ secret('AWS_DEFAULT_REGION') }}';
      SET s3_access_key_id='{{ secret('AWS_ACCESS_KEY_ID') }}';
      SET s3_secret_access_key='{{ secret('AWS_SECRET_ACCESS_KEY') }}';
      SELECT *
      FROM read_csv_auto('s3://{{vars.bucket}}/{{vars.destination_prefix}}/{{ trigger.objects | jq('.[].key') | first }}')
      WHERE price * quantity != total;
    store: true

  - id: csv
    type: io.kestra.plugin.serdes.csv.CsvWriter
    description: Create CSV file from query results
    from: ""{{ outputs.query.uri }}""

  - id: if-anomalies-detected
    type: io.kestra.core.tasks.flows.If
    condition: ""{{outputs.query.size}}""
    description: Send an email if outliers detected
    then:
      - id: sendEmailAlert
        type: io.kestra.plugin.notifications.mail.MailSend
        subject: Anomalies in data detected
        from: anna@kestra.io
        to: anna@kestra.io
        username: anna@kestra.io
        host: mail.privateemail.com
        port: 465
        password: ""{{ secret('EMAIL_PASSWORD') }}"" # or when using environment variables ""{{envs.email_password}}""
        sessionTimeout: 6000
        attachments:
          - name: anomalies_in_orders.csv
            uri: ""{{ outputs.csv.uri }}""
        htmlTextContent: |
          Detected anomalies in sales data in file <b>s3://{{vars.bucket}}/{{vars.destination_prefix}}/{{ trigger.objects | jq('.[].key') | first }}'</b>. <br />
          Anomalous rows are attached in a CSV file.<br /><br />
          Best regards,<br />
          Data Team

triggers:
  - id: pollForNewS3files
    type: io.kestra.plugin.aws.s3.Trigger
    bucket: ""{{vars.bucket}}""
    prefix: ""{{vars.source_prefix}}"" 
    maxKeys: 1 # 1 file = 1 execution
    interval: PT1S # every second
    filter: FILES
    action: MOVE
    moveTo:
      key: ""{{vars.destination_prefix}}/{{vars.source_prefix}}""
    region: ""{{ secret('AWS_DEFAULT_REGION') }}""
    accessKeyId: ""{{ secret('AWS_ACCESS_KEY_ID') }}""
    secretKeyId: ""{{ secret('AWS_SECRET_ACCESS_KEY') }}""
",2023-07-24 15:34:58.286,"Notifications,DuckDB,Trigger,S3"
145,Ingest Google Analytics data into a DuckDB database using dlt,"[""io.kestra.plugin.scripts.python.Script""]","This flow demonstrates how to extract data from Google Analytics and load it into a DuckDB database using dlt. The entire workflow logic is contained in a single Python script that uses the dlt Python library to ingest the data into a DuckDB database. 

The credentials to access the Google Analytics API are stored using [Kestra secrets](https://kestra.io/docs/developer-guide/secrets).
","id: dlt_google_analytics_to_duckdb
namespace: blueprint

tasks:
  - id: dlt_pipeline
    type: io.kestra.plugin.scripts.python.Script
    docker:
      image: python:3.11
    beforeCommands:
      - pip install dlt[duckdb]
      - dlt --non-interactive init google_analytics duckdb
    warningOnStdErr: false
    env:
      SOURCES__GOOGLE_ANALYTICS__CREDENTIALS__PROJECT_ID: ""{{ secret('GOOGLE_ANALYTICS_PROJECT_ID') }}""
      SOURCES__GOOGLE_ANALYTICS__CREDENTIALS__CLIENT_EMAIL: ""{{ secret('GOOGLE_ANALYTICS_CLIENT_EMAIL') }}""
      SOURCES__GOOGLE_ANALYTICS__CREDENTIALS__PRIVATE_KEY: ""{{ secret('GOOGLE_ANALYTICS_PRIVATE_KEY') }}""
      SOURCES__GOOGLE_ANALYTICS__PROPERTY_ID: ""{{ secret('GOOGLE_ANALYTICS_PROPERTY_ID') }}""
    script: |
      import dlt
      from google_analytics import google_analytics

      QUERIES = [
          {
              ""resource_name"": ""sample_analytics_data1"",
              ""dimensions"": [""browser"", ""city""],
              ""metrics"": [""totalUsers"", ""transactions""],
          },
          {
              ""resource_name"": ""sample_analytics_data2"",
              ""dimensions"": [""browser"", ""city"", ""dateHour""],
              ""metrics"": [""totalUsers""],
          },
      ]

      pipeline = dlt.pipeline(
          pipeline_name=""google_analytics_pipeline"",
          destination=""duckdb"",
          dataset_name=""google_analytics"",
      )

      load_info = pipeline.run(google_analytics(queries=QUERIES))
",2023-10-10 12:14:58.278,"GCP,Ingest"
148,Ingest Zendesk data into Weaviate using dlt,"[""io.kestra.plugin.scripts.python.Script""]","This flow demonstrates how to extract data from Zendesk into Weaviate using dlt. The entire workflow logic is contained in a single Python script that uses the dlt Python library to ingest the data into Weaviate. The credentials to access the Zendesk API and Weaviate are stored using [Kestra secrets](https://kestra.io/docs/developer-guide/secrets).
","id: dlt_zendesk_to_weaviate
namespace: blueprint

tasks:
  - id: dlt_pipeline
    type: io.kestra.plugin.scripts.python.Script
    docker:
      image: python:3.11
    beforeCommands:
      - pip install dlt[weaviate]
      - dlt --non-interactive init zendesk weaviate
    warningOnStdErr: false
    env:
      SOURCES__ZENDESK__ZENDESK_SUPPORT__CREDENTIALS__PASSWORD: ""{{ secret('ZENDESK_PASSWORD') }}""
      SOURCES__ZENDESK__ZENDESK_SUPPORT__CREDENTIALS__SUBDOMAIN: ""{{ secret('ZENDESK_SUBDOMAIN') }}""
      SOURCES__ZENDESK__ZENDESK_SUPPORT__CREDENTIALS__EMAIL: ""{{ secret('ZENDESK_EMAIL') }}""
      DESTINATION__WEAVIATE__CREDENTIALS__URL: ""{{ secret('WEAVIATE_URL') }}""
      DESTINATION__WEAVIATE__CREDENTIALS__API_KEY: ""{{ secret('WEAVIATE_API_KEY') }}""
    script: |
      import dlt
      from zendesk import zendesk_support

      pipeline = dlt.pipeline(
          pipeline_name=""zendesk_pipeline"",
          destination=""weaviate"",
          dataset_name=""zendesk"",
      )

      zendesk_source = zendesk_support(load_all=False)
      tickets = zendesk_source.tickets

      load_info = pipeline.run(
          weaviate_adapter(
              tickets,
              vectorize=[""subject"", ""description""],
          )
      )",2023-10-10 12:26:20.569,Ingest
130,"Generate PDF files from HTML, Markdown, Word or Excel using Gotenberg","[""io.kestra.plugin.scripts.shell.Commands""]","You can start your Gotenberg API server using the command: `docker run --rm -p 3000:3000 gotenberg/gotenberg:7`. Replace the `server` variable below to point to your server. For development, you can test your workflow using the demo API: https://demo.gotenberg.dev. 
  
For more information, see the [Gotenberg documentation](https://gotenberg.dev/docs/getting-started/installation).","id: generate_pdf_with_gotenberg
namespace: blueprint

variables:
  server: https://demo.gotenberg.dev # http://localhost:3000
  template: https://sparksuite.github.io/simple-html-invoice-template/

tasks:
  - id: pdf
    type: io.kestra.plugin.scripts.shell.Commands
    runner: PROCESS
    warningOnStdErr: false
    commands:
      - curl --request POST '{{vars.server}}/forms/chromium/convert/url' --form 'url=""{{vars.template}}""' -o {{ outputDir }}/myfile.pdf
",2023-09-20 23:36:40.84,"CLI,API"
129,Download a PDF file and extract text from it using Apache Tika,"[""io.kestra.plugin.fs.http.Download"", ""io.kestra.plugin.tika.Parse"", ""io.kestra.core.tasks.log.Log""]","This flow downloads a PDF file using the HTTP Download task. 
Then, it extracts text from the PDF file using Apache Tika.
Finally, it logs the extracted text using the Log task.","id: parse_pdf
namespace: blueprint

tasks:
  - id: download_pdf
    type: io.kestra.plugin.fs.http.Download
    uri: https://huggingface.co/datasets/kestra/datasets/resolve/main/pdf/app_store.pdf

  - id: parse_text
    type: io.kestra.plugin.tika.Parse
    from: '{{ outputs.download_pdf.uri }}'
    contentType: TEXT
    store: false

  - id: log_extracted_text
    type: io.kestra.core.tasks.log.Log
    message: ""{{ outputs.parse_text.result.content }}""",2023-10-09 17:21:49.101,"Outputs,Ingest,Local files"
27,Send an SMS message using AWS SNS based on a runtime-specific input,"[""io.kestra.plugin.aws.sns.Publish""]","This flow sends an SMS message to a phone number using AWS SNS. The phone number must be registered when creating an AWS SNS topic. You can override the SMS message text at runtime by leveraging the input argument `smsText`.

This flow assumes AWS credentials stored as secrets `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY` and `AWS_DEFAULT_REGION`.","id: sendSMS
namespace: blueprint

inputs:
  - name: smsText
    type: STRING
    defaults: ""Hello from Kestra and AWS SNS!""

tasks:
  - id: sendSMS
    type: io.kestra.plugin.aws.sns.Publish
    region: ""{{ secret('AWS_DEFAULT_REGION') }}""
    accessKeyId: ""{{ secret('AWS_ACCESS_KEY_ID') }}""
    secretKeyId: ""{{ secret('AWS_SECRET_ACCESS_KEY') }}""
    topicArn: arn:aws:sns:eu-central-1:338306982838:kestra
    from:
      data: |
        {{inputs.smsText}}",2023-07-26 20:24:49.546,"AWS,Inputs"
143,CI/CD with a GitHub webhook trigger in production and a manually-supplied JSON-type input for local testing,"[""io.kestra.core.tasks.debugs.Return"", ""io.kestra.plugin.scripts.python.Script"", ""io.kestra.core.models.triggers.types.Webhook""]","This flow can be used as a template to implement a CI/CD pipeline that processes event payload from a GitHub webhook trigger. In this example, any time you open a new Pull Request, the flow will be triggered and the flow will use the payload information to implement custom actions. Here, we simly add a comment to the pull request using the Pull Request's comment URL from the event payload, but your use case might require retrieving information such as branch name, etc.  

To facilitate local testing, the expression `'{{ trigger.body.pull_request.comments_url ?? inputs.payload.pull_request.comments_url }}'` will make sure that the flow uses a value from the webhook if the flow is triggered via a webhook, and will otherwise use the value from a manually provided input e.g. [a JSON payload](https://gist.github.com/anna-geller/54c999b3b2ba3647223618f8c657910f). This way, you can test the workflow end-to-end using a mock data provided as a JSON-type input before testing it with live data from a webhook.

For more information about this usage pattern, check [the following blog post](https://levelup.gitconnected.com/when-github-actions-get-painful-to-troubleshoot-try-this-instead-9a134c9e9baf).","id: cicd
namespace: blueprints

inputs:
  - name: payload
    type: JSON

tasks:
  - id: return
    type: io.kestra.core.tasks.debugs.Return
    format: '{{ trigger.body.pull_request.comments_url ?? inputs.payload.pull_request.comments_url }}'

  - id: python_action
    type: io.kestra.plugin.scripts.python.Script
    docker:
        image: ghcr.io/kestra-io/pydata:latest
    script: |
        import requests
        import json

        url = ""{{ outputs.return.value }}""

        headers = {
            'Authorization': 'token {{ secret('GITHUB_ACCESS_TOKEN') }}',
            'Accept': 'application/vnd.github.v3+json'
        }
        payload = {'body': 'hello from `{{execution.id}}` in `{{flow.id}}`'}

        response = requests.post(url, headers=headers, json=payload)

        if response.status_code == 201:
            print(""Comment successfully created."")
        else:
            print(f""Failed to create comment: {response.text}"")

triggers:
  - id: github
    type: io.kestra.core.models.triggers.types.Webhook
    key: ""{{ secret('WEBHOOK_KEY') }}""
    conditions:
      - type: io.kestra.core.models.conditions.types.VariableCondition
        expression: ""{{ trigger.body.pull_request.state == 'open' and trigger.body.pull_request.comments_url }}""",2023-10-17 12:39:40.874,"API,Git"
23,Create an S3 Bucket,"[""io.kestra.plugin.aws.s3.CreateBucket""]","This flow takes a **bucket name** as input and creates a new S3 bucket with that name. 

This flow assumes AWS credentials stored as secrets `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`.","id: createBucket
namespace: blueprint

inputs:
  - name: bucket
    type: STRING
    defaults: declarative-orchestration-with-kestra
    required: true

tasks:
  - id: createBucket
    type: io.kestra.plugin.aws.s3.CreateBucket
    accessKeyId: ""{{ secret('AWS_ACCESS_KEY_ID') }}""
    secretKeyId: ""{{ secret('AWS_SECRET_ACCESS_KEY') }}""
    region: ""{{ secret('AWS_DEFAULT_REGION') }}""
    bucket: ""{{inputs.bucket}}""
",2023-07-26 20:06:56.953,S3
58,Load a CSV file to a Postgres table,"[""io.kestra.plugin.fs.http.Download"", ""io.kestra.plugin.jdbc.postgresql.Query"", ""io.kestra.plugin.jdbc.postgresql.CopyIn""]","This blueprints shows how to copy a CSV file into a Postgres table.

","id: copyin_postgres
namespace: blueprint

tasks:
  - id: download
    type: io.kestra.plugin.fs.http.Download
    uri: https://raw.githubusercontent.com/lukes/ISO-3166-Countries-with-Regional-Codes/master/all/all.csv


  - id: create_table
    type: io.kestra.plugin.jdbc.postgresql.Query
    url: jdbc:postgresql://sample_postgres:5433/world
    username: postgres
    password: postgres
    sql: |
      CREATE TABLE IF NOT EXISTS country_referential(
        name VARCHAR,
        ""alpha-2"" VARCHAR,
        ""alpha-3"" VARCHAR,
        ""country-code"" VARCHAR,
        ""iso_3166-2"" VARCHAR,
        region VARCHAR,
        ""sub-region"" VARCHAR,
        ""intermediate-region"" VARCHAR,
        ""region-code"" VARCHAR,
        ""sub-region-code"" VARCHAR,
        ""intermediate-region-code"" VARCHAR
      );

  - id: copyin
    type: io.kestra.plugin.jdbc.postgresql.CopyIn
    url: jdbc:postgresql://sample_postgres:5433/world
    username: postgres
    password: postgres
    format: CSV
    from: '{{ outputs.download.uri }}'
    table: country_referential
    header: true

  - id: read
    type: io.kestra.plugin.jdbc.postgresql.Query
    url: jdbc:postgresql://sample_postgres:5433/world
    username: postgres
    password: postgres
    sql: SELECT * FROM country_referential LIMIT 10
    fetch: true",2023-07-27 00:32:04.092,"Outputs,SQL,Postgres"
8,Run two tasks in parallel,"[""io.kestra.core.tasks.flows.Parallel"", ""io.kestra.core.tasks.debugs.Return""]",This blueprints show how to run two tasks in parallel.,"id: parallel_tasks
namespace: blueprint

tasks:
  - id: parallel
    type: io.kestra.core.tasks.flows.Parallel
    tasks:

      - id: task1
        type: io.kestra.core.tasks.debugs.Return
        format: ""{{ task.id }}""

      - id: task2
        type: io.kestra.core.tasks.debugs.Return
        format: ""{{ task.id }}""
",2023-07-26 00:51:06.671,Parallel
