id: azureBlobToBigQuery
namespace: blueprint

tasks:
  - id: each
    type: io.kestra.core.tasks.flows.EachParallel
    value: "{{ trigger.blobs | jq('.[].uri') }}"
    tasks:
      - id: uploadFromFile
        type: io.kestra.plugin.gcp.bigquery.Load
        destinationTable: gcpProject.dataset.table
        from: "{{taskrun.value}}"
        writeDisposition: "WRITE_APPEND"
        projectId: yourGcpProject
        serviceAccount: "{{ secret('GCP_CREDS') }}"
        ignoreUnknownValues: true
        autodetect: true
        format: CSV
        csvOptions:
          allowJaggedRows: true
          encoding: UTF-8
          fieldDelimiter: ","
  - id: dbtCloudJob
    type: io.kestra.plugin.dbt.cloud.TriggerRun
    accountId: "{{ secret('DBT_CLOUD_ACCOUNT_ID') }}"
    token: "{{ secret('DBT_CLOUD_API_TOKEN') }}"
    jobId: "366381"
    wait: true

triggers:
  - id: watch
    type: io.kestra.plugin.azure.storage.blob.Trigger
    interval: PT1S
    endpoint: "https://kestra.blob.core.windows.net"
    connectionString: "{{ secret('AZURE_CONNECTION_STRING') }}"
    container: "stage"
    prefix: "marketplace/"
    action: MOVE
    moveTo:
      container: stage
      name: archive/marketplace/
